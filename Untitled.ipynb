{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7be1e4-1f8b-4ff4-bd0a-2af043acb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Timestamp: 2024-10-17 22:34:05\n",
      "Running MMK_raw_date_scraping.py\n",
      "https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format\n",
      "Page: 1 one star\n",
      "No review\n",
      "Page: 1 two star\n",
      "No review\n",
      "Page: 1 four star\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STARK\\AppData\\Local\\Temp\\ipykernel_12048\\3234024334.py:231: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, text='Vine Customer Review of Free Product')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped 3 reviews\n",
      "Page: 2 four star\n",
      "No review\n",
      "Page: 1 five star\n",
      "Page 1 scraped 3 reviews\n",
      "Page: 2 five star\n",
      "No review\n",
      "https://www.amazon.com/product-reviews/B0CFM7BTW8/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 one star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 one star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 one star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 one star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 one star\n",
      "Page 6 scraped 10 reviews\n",
      "Page: 7 one star\n",
      "Page 7 scraped 10 reviews\n",
      "Page: 8 one star\n",
      "Page 8 scraped 10 reviews\n",
      "Page: 9 one star\n",
      "Page 9 scraped 10 reviews\n",
      "Page: 10 one star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 two star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 two star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 two star\n",
      "Page 4 scraped 8 reviews\n",
      "No more pages left\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 four star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 four star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 four star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 four star\n",
      "Page 5 scraped 9 reviews\n",
      "No more pages left\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 five star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 five star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 five star\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 278\u001b[0m\n\u001b[0;32m    276\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&filterByStar=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_star&pageNumber=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&sortBy=recent\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage:\u001b[39m\u001b[38;5;124m'\u001b[39m,page, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m star\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m soup \u001b[38;5;241m=\u001b[39m get_soup_amazon(url)  \u001b[38;5;66;03m# Get the soup object from the URL\u001b[39;00m\n\u001b[0;32m    279\u001b[0m extracted_reviews \u001b[38;5;241m=\u001b[39m amazon_review(soup, url)  \u001b[38;5;66;03m# Extract reviews from the soup\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-section a-spacing-top-large a-text-center no-reviews-section\u001b[39m\u001b[38;5;124m'\u001b[39m}):  \n",
      "Cell \u001b[1;32mIn[5], line 150\u001b[0m, in \u001b[0;36mget_soup_amazon\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     cookies \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat-main\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtza|IwEBILCkGKez-bDPkyK0rOA-46d-88vlSz7iC8zSWdEiSLs_r5NlyuNiCo4NoYo8pOtMLyWrOq-4gOs7rl-gHuz8xNhr2ce2isTgfDLm10yWkv3Bb8wnAkCDO8B9otdWa6lc-4p95eJOVtueLtz8Vw_XWyZ5hJXZzphOM5UAoZZ5x3EueI56ClWXr04WmGHygiFwaekYbNqau7gLyZQBFeg4fXsN2bUsumMCc8K25AmBv3wg0A\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msess-at-main\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mVtSfvkuHHDzklDLJNIJWnTv1vX0k8K0IlItvYEb0GLo=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession-id-time\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2082787201l\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m }\n\u001b[1;32m--> 150\u001b[0m req \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, cookies\u001b[38;5;241m=\u001b[39mcookies)\n\u001b[0;32m    151\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(req\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m soup\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\urllib3\\response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\site-packages\\urllib3\\response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "import urllib.parse\n",
    "import urllib3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable urllib3 warnings\n",
    "urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the timestamp\n",
    "print(\"Current Timestamp:\", timestamp)\n",
    "\n",
    "print('Running MMK_raw_date_scraping.py')\n",
    "\n",
    "# %%\n",
    "excel_file_path = r\"C:\\Users\\tayu430\\anaconda3_remote\\envs\\webscrapper\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheet_name = \"data_new\"\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "df_amazon\n",
    "\n",
    "# %%\n",
    "path = r\"C:\\Users\\tayu430\\anaconda3_remote\\envs\\webscrapper\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(path, sheet_name = sheets, engine='openpyxl')\n",
    "review_template\n",
    "\n",
    "# %% [markdown]\n",
    "# # Amazon\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Function\n",
    "\n",
    "# %%\n",
    "### Not use docker\n",
    "\n",
    "# header = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "#         'Accept-Encoding': 'gzip, deflate, br',\n",
    "#         'Accept-Language': 'en-US,en;q=0.9',\n",
    "#         'Cache-Control': 'max-age=0', \n",
    "#         'Downlink': '10',\n",
    "#         'Dpr': '1',\n",
    "#         'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "#         'Sec-Ch-Ua-Mobile': '?0',\n",
    "#         'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "#         'Sec-Fetch-Dest': 'document',\n",
    "#         'Sec-Fetch-Mode': 'navigate',\n",
    "#         'Sec-Fetch-Site': 'same-origin',\n",
    "#         'Sec-Fetch-User': '?1',\n",
    "#         'Upgrade-Insecure-Requests': '1'\n",
    "#     }\n",
    "\n",
    "# url ='https://www.amazon.com/HP-DeskJet-2755e-Wireless-Printer/product-reviews/B08XYP6BJV/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "# response = requests.get(url, headers=header)\n",
    "# response.raise_for_status()\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "# %%\n",
    "from datetime import datetime\n",
    "\n",
    "#docker section\n",
    "# def get_soup(url):   \n",
    "#     r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})  \n",
    "#     soup = BeautifulSoup(r.text, 'html.parser')  \n",
    "#     return soup  \n",
    "\n",
    "def get_soup_amazon(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "    headers = {\n",
    "        \"Host\": host,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    if host == 'www.amazon.co.uk':\n",
    "        cookies = {\n",
    "            \"id_pkel\": \"n0\",\n",
    "            \"session-id\": \"260-4298792-7658758\",\n",
    "            \"ubid-acbuk\": \"258-6971084-0540716\",\n",
    "            \"at-acbuk\": \"Atza|IwEBIP_rct34sEzoIRoVA3yCl9iNVj1vAkz4UScATDbZbC-hmPsIBEWMVfxD5DK4JdS4e0q-B3snMwPGDAZiSNULMr19y8lh0JspbVm_IlbF3ansTJ7ocxWUxcMeBXFZeg80bpT8FJalwoGBUfEN4RTParyxnZFPtfYCI7qk53UkgAfXJYqlWpxVVYNjihc38BEOBWWYwU2Jqqw4LMoWayecXYwZxragFHKmjCApjny15SChsA\",\n",
    "            \"sess-at-acbuk\": \"\\\"U0kkcZUX2VEJfWcYvHDR+3X7nAkcpnfTMO6xUCoGZPw=\\\"\",\n",
    "            \"sst-acbuk\": \"Sst1|PQF7uChzfDOz9iDl5ODdyi0jCceK_CQiKZcZ3Wbz-GiUtWitVPEKjUjMFSmItS6pnSHcV8mraZvRzfSgJnkzhno8Ae1vwqF-cUJR_967Ldn2X42v7IqdxlPpCQT7kZVe4AYaLP7_rmpojkWo0ro13PqK8R0k8Lmps_HZuiLc2y60SnVSE9jhrGd6GYZrNzgXlr7-IVZGn9xw-0s_U3RNJkagUPYraLKUty7Ij8vUc4YTjy6v9Pqunw-jxK3n4_FLehumir8j0ZDyH1mbjlMk3pRb0ROqDgbZT8sO3My5P3QxvMo\",\n",
    "            \"session-id-time\": \"2082787201l\",\n",
    "            \"x-acbuk\": \"\\\"7tyUa0SXz2iP93WG9IBc5lnA2RSDxZS3GfL2ZeriqVieDKMLCztGBUgquMV0axF@\\\"\",\n",
    "            \"i18n-prefs\": \"GBP\",\n",
    "            \"lc-acbuk\": \"en_GB\",\n",
    "            \"sp-cdn\": \"\\\"L5Z9:IN\\\"\",\n",
    "            \"session-token\": \"/l9KYAtgAfZIZDTr0iecZc/IpcgnmXUKW6DzLGsEoaXqRucpzG9ZNA4HmwO2ZhVWusplDqSCMtsshKKUMQk3USzit3k3s283i1UDZk4PQMpF2UzdTDWyMGmcpgy5lR0yemgFEYAO8ODnMEmqYGOKwCxTKwRDzbk4auO/MkyaPvJAcJrC17YgER44Q141xQKSepf7Wr6s2S9DnZXiPMujW2iZcquia0U7HRGhg29r18/+Cz5Zng6R5lf7XEJZDoaMtbKKps+XbgwS+YD8uRB9OJP42eVWxx0Mh5MWHw6pQBt3bj77U4/RmeNbdnTgMx+/a2SmfHXsS8EH6j3WrX34DusILMs/15fkMLJ9NQVv0UzKYg2A2BcM9pzS3G/+x6gu\"\n",
    "        }\n",
    "    elif(host == 'www.amazon.es'):\n",
    "        cookies = {\n",
    "        \"id_pkel\": \"n0\",\n",
    "        \"session-id\": \"259-2556844-3879143\",\n",
    "        \"ubid-acbes\": \"259-0682713-2972732\",\n",
    "        \"session-token\": \"\\\"j0uY88Xhi3bqvARsrwoCvUwDXSsk54Z6KXEC0p3nqqn+oWm93ZVq8aM6svPCzGU5a02BCCyHxQmkYjQXaJCbeBkjPoXQ9UOq3QvR7uNQGyqfW5cmNxwMTJzcbvd/WeJOF88+K4tFofxgJwuGZfWOpL//bgQlgMWokVW26WZ9Z3Bv6eIG7rdB8VwDqpWOqKSZ5FWmcHm5LjxNDPE3L7mXRUtSkQWgwdjLhl3aeH44THDWO6+2XuwB2qMJwYbHS1fR1PhQ+0GTzDxM2ZlW0WaOA6T5r9UiUekPaEigH3mynOAz3RdiS29woM/TeoF64DQi/NONY2BApK9QNPHyEAip+bI2MqPqBpcXrpOlg2RuZOoEapaEO9OR2g==\\\"\",\n",
    "        \"x-acbes\": \"\\\"JyDSBpmaQLvMnoNd@xnPdLsgiVJliu7cBhz0K9xA9dNJhky09MN?MqaZllflC?Cm\\\"\",\n",
    "        \"at-acbes\": \"Atza|IwEBILVWijl6PCPAFk_wimanCI5HB5wcswC7umy_12IO9b1h9YN2rTAvxrruf0B7sBgniKfalFE3TDPNRUN9XaNqXRZd7I2k47QkgPs7LP3bsIr9-7CtvbhzH7wKkNVa7pPxzryb4Clz-sgH2F9KIqdF-Vwu2BNxCYhsUZmP8Al40bIggb897UHf9CoSy3VKqeRDSn6gUWCSCgICPEJUuyOVi19qRMdXVbpSlsEVunTUxuobVQ\",\n",
    "        \"sess-at-acbes\": \"\\\"8hl/o7uIRofZyD1wGq5JYBUvlMaEn1e1oPpL+PEz6ZU=\\\"\",\n",
    "        \"sst-acbes\": \"Sst1|PQHaKUE6V8mvs7u4faCwHDpzCTMgNC6CLHInD6RsB9MmRoPjEZSWOdl7sQEK7hZ6gOA88j-M_nfWejx60k9FFKLC6m3Z06mzfzoIR3w4tXMJ69SWudOqIEzmCavvPsYj6RvDW8bqP3gphKuh4GIXWFyOfZ0Nz_XXyCCzOH5RI3RRuL_tCps4AkwuzHRAGIpD4ZJFWf5fF2zJNhAMdhQ8DGyLvoLDqa6cVU_9qSKwyFXREPzPtyNLu89-bQJAOX_qJXOZPI1t5qG2bmf7cogZnyeFtPqgokvvitYPswOuv_l-PqI\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"i18n-prefs\": \"EUR\",\n",
    "        \"lc-acbes\": \"es_ES\"\n",
    "    }\n",
    "    else:\n",
    "        cookies = {\n",
    "        \"at-main\": \"Atza|IwEBILCkGKez-bDPkyK0rOA-46d-88vlSz7iC8zSWdEiSLs_r5NlyuNiCo4NoYo8pOtMLyWrOq-4gOs7rl-gHuz8xNhr2ce2isTgfDLm10yWkv3Bb8wnAkCDO8B9otdWa6lc-4p95eJOVtueLtz8Vw_XWyZ5hJXZzphOM5UAoZZ5x3EueI56ClWXr04WmGHygiFwaekYbNqau7gLyZQBFeg4fXsN2bUsumMCc8K25AmBv3wg0A\",\n",
    "        \"sess-at-main\": \"\\\"VtSfvkuHHDzklDLJNIJWnTv1vX0k8K0IlItvYEb0GLo=\\\"\",\n",
    "        \"sst-main\": \"Sst1|PQFkpgtkYvuh30UDcQDVUmdRCbB-NbcfJ3Et1IJ7zvhMFDgbqZbrmApcoR4Zy3DEA4yEwiCduu26P9aZNYkAY8be_yT3HMRe6flKd499jfNQ5DVTWWi9BcRY6nACHh-qDtc74kvKfmHa6qWfafwhRuYUfbT316Z6qHKFSgpS1Kz7Ta3PDbe1YWaWdEwc-AIKPu2497ympmZcZn89OU4BdFX71KDDhCENXZNRzYw6xdASs40P7iUL3Y69QFI-mr6wN_E5ZybkNsf9iHMpBqYJt9kukTRWg1lXrn0uaMqZPP1hDZg\",\n",
    "        \"x-main\": \"\\\"Fa?@jbVer5IDB1p5XKuF7HGYTxWyoJuSGsOMWOuZd4Pcq0TBbYAMXT7OXi2zhmCt\\\"\",\n",
    "        \"i18n-prefs\": \"USD\",\n",
    "        \"lc-main\": \"en_US\",\n",
    "        \"session-token\": \"\\\"RRt0+TjE+kscTjhb80ckzUFtCrD34JY8El2Fg84cTIa4CFWwXrhNH1EoCr/2fAnUSGb5n2G07+yQaaq3URgY7Q8HNkLsNgw4BDvPTHnSoipearc8BXsIRlH+YbLz1wYDbduxJKasPwJ6+sKJZ2Gzuxrfj9b0R86ygc6kdz/7ZRJPexHbZ6V+HTwsMhxhIqGyhINbg06EJlHq8rXtWnqwR854bBimHoPbbtzTErY47sEnvYyPjBWwPQqt7umXkHNkAp0BbcWQOA8NghFsAvvxLWjmxC8zn8KTT8zqHe1slAWGfv95XgKhzOxrxQnBw3t2iE/qEB3lzS58dQIEujFfnsxqyESp66vLwfDbQKiY7bFfcJAKxsO3pQ==\\\"\",\n",
    "        \"ubid-main\": \"132-4010737-0455008\",\n",
    "        \"session-id\": \"139-2733986-5232362\",\n",
    "        \"session-id-time\": \"2082787201l\"\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def amazon_review(soup, url):    \n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "\n",
    "    # NPI lanched in 2024-01-15\n",
    "    date_string = \"2024-01-15\"\n",
    "    min_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "\n",
    "    for item in reviews:    \n",
    "        review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "        parsed_url = urlparse(url)\n",
    "        host = parsed_url.netloc\n",
    "        \n",
    "        if(host == 'www.amazon.co.uk'):\n",
    "            review_date = datetime.strptime(review_date_string, \"%d %B %Y\")\n",
    "        elif(host == 'www.amazon.es'):\n",
    "            review_date = datetime.strptime(review_date_string, \"%d %B %Y\")\n",
    "        else:\n",
    "            review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "        if review_date < min_date:\n",
    "            print('Review date is less than 2024-01-15')\n",
    "            break\n",
    "    \n",
    "        review = {    \n",
    "            'Model': model,    \n",
    "            'Review date': review_date,     \n",
    "            \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        \n",
    "  \n",
    "        try:    \n",
    "            review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "        except AttributeError:    \n",
    "            review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "  \n",
    "        try:    \n",
    "            review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "  \n",
    "        try:    \n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        try:      \n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "        except AttributeError:        \n",
    "            review[\"Review name\"] = None  \n",
    "  \n",
    "        try:    \n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"People_find_helpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            if seeding:\n",
    "               review['Seeding or not'] = seeding\n",
    "            else:\n",
    "                raise AttributeError\n",
    "        except AttributeError:  \n",
    "            try: \n",
    "                review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, text='Vine Customer Review of Free Product')\n",
    "\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "\n",
    "        try:\n",
    "            review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "             review['Aggregation'] = None\n",
    "    \n",
    "  \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "  \n",
    "    return extracted_reviews\n",
    "\n",
    "# %% [markdown]\n",
    "# ## HP Review\n",
    "\n",
    "# %%\n",
    "urls = ['https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format', \n",
    "        'https://www.amazon.com/product-reviews/B0CFM7BTW8/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        'https://www.amazon.com/product-reviews/B0CFM82NS2/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        'https://www.amazon.com/product-reviews/B0CFM7VJNK/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        'https://www.amazon.com/product-reviews/B0CFM8KL9G/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        'https://www.amazon.com/product-reviews/B0CFM94G5H/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        'https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format']\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['one', 'two', 'four','five'] \n",
    "max_retry_attempts = 2\n",
    "all_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for page in range(1, 11):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url = f'{link}&filterByStar={y}_star&pageNumber={page}&sortBy=recent'  \n",
    "                    print('Page:',page, f'{y} star')\n",
    "                    soup = get_soup_amazon(url)  # Get the soup object from the URL\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                            print('No review')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                    \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews)\n",
    "                        print(f\"Page {page} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if page >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {page} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {page} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None)\n",
    "amazon2= pd.DataFrame(all_reviews)\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "# amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "amazon_hp_combine\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Convert to dataframe\n",
    "\n",
    "# %%\n",
    "amazon_final = amazon_hp_combine \n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'US'\n",
    "amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Aggregation':'Aggregation_Flag',\n",
    "    'Country': 'Country'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "amazon_final_df.head()\n",
    "\n",
    "# amazon_final_df.to_csv(r'amazon_review.csv',index = False)\n",
    "\n",
    "# %%\n",
    "final_review = pd.concat([review_template, amazon_final_df])\n",
    "final_review.head()\n",
    "\n",
    "# %%\n",
    "# # Query previous amazon review\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# server = 'SQL-Cluster01.ijp.sgp.rd.hpicorp.net'\n",
    "# database = 'STAR_Rating'\n",
    "# schema = 'dbo'\n",
    "# driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# # dataframe = amazon_final_df\n",
    "# table = \"Ink_web_reviews\"\n",
    "\n",
    "# engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    "\n",
    "\n",
    "# existing_rows_query = f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM {schema}.{table}\n",
    "#     WHERE Retailer in ('Amazon')\n",
    "# \"\"\"\n",
    "# result_df = pd.read_sql_query(existing_rows_query, engine)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# result_df['Review_Date'] = result_df['Review_Date'].dt.date\n",
    "\n",
    "# non_duplicated_df = amazon_final_df[(~amazon_final_df['Review_Date'].isin(result_df['Review_Date']))&\n",
    "#                                    (~amazon_final_df['Review_Content'].isin(result_df['Review_Content']))].drop_duplicates()\n",
    "# non_duplicated_df\n",
    "\n",
    "# %%\n",
    "# from sqlalchemy import create_engine, text\n",
    " \n",
    "# server = 'SQL-Cluster01.ijp.sgp.rd.hpicorp.net'\n",
    "# database = 'STAR_Rating'\n",
    "# schema = 'dbo'\n",
    "# driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# dataframe = non_duplicated_df\n",
    "# table = \"Ink_web_reviews\"\n",
    "\n",
    "# engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "# chunk_size = 10000\n",
    "# total_rows = len(dataframe)\n",
    "# num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# for i in range(num_chunk):\n",
    "#     start_index = i * chunk_size\n",
    "#     end_index = (i + 1) * chunk_size\n",
    "#     chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "#     chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "#     print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")\n",
    "\n",
    "# %%\n",
    "# ## Change column setting (Rating to one decimal place)\n",
    "# update_query = f'''\n",
    "#  ALTER TABLE dbo.Ink_web_reviews\n",
    "#  ALTER COLUMN Review_Date DATE\n",
    "#  '''\n",
    "# with engine.connect() as connection:\n",
    "#         connection.execute(update_query)\n",
    "\n",
    "# print(\"Precision updated in SQL table.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Bestbuy\n",
    "\n",
    "# %%\n",
    "from datetime import datetime\n",
    "def get_review_bestbuy(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.61',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "def bestbuy_review(soup, url):    \n",
    "    bestbuy = {}\n",
    "    bestbuy_reviews = []  \n",
    "    Model = soup.find(\"h1\", {'class':\"heading-5 v-fw-regular\"})\n",
    "    if not Model:\n",
    "        Model = soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"})\n",
    "    Model = Model.text if Model else None\n",
    "    \n",
    "        \n",
    "    npi = soup.find('span',{'class':'c-reviews order-2'} ).text\n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "    if review_session:\n",
    "        for item in review_session:    \n",
    "            bestbuy = {    \n",
    "                 'Model':Model,\n",
    "                # 'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "                'URL':url \n",
    "            }\n",
    "            try:    \n",
    "                bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review title']  = None\n",
    "\n",
    "            try:\n",
    "                bestbuy['Review_Name']  = item.find(\"div\", {\"class\": \"ugc-author v-fw-medium body-copy-lg\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review_Name']  = None\n",
    "                \n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review rating']  = None\n",
    "\n",
    "            review_date_element = item.find(\"time\", {\"class\": \"submission-date\"})\n",
    "            if review_date_element:\n",
    "                review_date_string = review_date_element['title']\n",
    "                review_date_datetime = datetime.strptime(review_date_string, '%b %d, %Y %I:%M %p')\n",
    "                formatted_review_date = review_date_datetime.strftime('%Y-%m-%d')\n",
    "                bestbuy['Review_Date'] = formatted_review_date\n",
    "            else:\n",
    "                bestbuy['Review_Date'] = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review promotion']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review aggregation']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Content']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Recommendation']  = None\n",
    "\n",
    "            try:    \n",
    "                network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "                if network_badge:\n",
    "                    bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "                else:\n",
    "                    bestbuy['Seeding or not']  = \"\"\n",
    "            except AttributeError:    \n",
    "                bestbuy['Seeding or not']  = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_helpful']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_unhelpful']  = None\n",
    "\n",
    "\n",
    "            bestbuy_reviews.append(bestbuy)    \n",
    "        \n",
    "    \n",
    "  \n",
    "    return npi, bestbuy_reviews \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Best buy hp\n",
    "\n",
    "# %%\n",
    "# ### Novellie\n",
    "# urls = [\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-envy-inspire-7255e-wireless-all-in-one-inkjet-photo-printer-with-3-months-of-instant-ink-included-with-hp-white-sandstone/6492187?variant=A&sort=MOST_RECENT',\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-envy-inspire-7955e-wireless-all-in-one-inkjet-photo-printer-with-3-months-of-instant-ink-included-with-hp-white-sandstone/6478251?variant=A&sort=MOST_RECENT'\n",
    "\n",
    "# ]\n",
    "\n",
    "# %%\n",
    "urls = ['https://www.bestbuy.com/site/reviews/hp-officejet-pro-8135e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565476?variant=A'\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9125e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565475?variant=A',\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9135e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565473?variant=A',\n",
    "#         'https://www.bestbuy.com/site/reviews/hp-officejet-pro-8139e-wireless-all-in-one-inkjet-printer-with-12-months-of-instant-ink-included-with-hp-white/6565474?variant=A',\n",
    "#         'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9730e-wireless-all-in-one-wide-format-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6578444?variant=A'\n",
    "]\n",
    "\n",
    "# %%\n",
    "max_attempts = 5\n",
    "bestbuy_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    should_continue = True\n",
    "    attempt_count = 0  # Counter for attempts\n",
    "    for x in range(1, 100):\n",
    "        if not should_continue:\n",
    "            break\n",
    "        while True:\n",
    "            url = f'{link}&page={x}'\n",
    "            try:\n",
    "                soup = get_review_bestbuy(url)\n",
    "                npi, reviews = bestbuy_review(soup, url)\n",
    "                if npi == 'Be the first to write a review':\n",
    "                    should_continue = False\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')\n",
    "                bestbuy_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-disabled\": \"true\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                if x > 1 and next_page_link and next_page_link.get(\"aria-disabled\") == \"true\":\n",
    "                    should_continue = False\n",
    "                    print('No more pages left')\n",
    "                    break\n",
    "\n",
    "                if len(reviews) < 20:\n",
    "                    should_continue = False\n",
    "                    print('Only 1 page')\n",
    "                    break\n",
    "                else:\n",
    "                    break \n",
    "            except Exception as e:\n",
    "                attempt_count += 1\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds... (Attempt {attempt_count}/{max_attempts})\")\n",
    "                if attempt_count >= max_attempts:\n",
    "                    print(\"Maximum number of attempts reached. Exiting loop.\")\n",
    "                    should_continue = False\n",
    "                    break\n",
    "                time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "review = pd.DataFrame(bestbuy_reviews)\n",
    "review['Retailer']=\"Best Buy\"\n",
    "review['scraping_date'] = pd.to_datetime(date.today())\n",
    "\n",
    "review['HP Model Number'] = review['Model'].str.extract(r'(\\d+e*)')\n",
    "\n",
    "hp_combine = pd.merge(review, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "\n",
    "hp_combine['Review Model'] = hp_combine['HP Model'] \n",
    "hp_combine['People_find_helpful'] = hp_combine['People_find_helpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "hp_combine['People_find_unhelpful'] = hp_combine['People_find_unhelpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "\n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "hp_combine_bestbuy = hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "hp_combine_bestbuy = hp_combine_bestbuy.drop_duplicates()\n",
    "\n",
    "hp_combine_bestbuy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "bestbuy_final = hp_combine_bestbuy\n",
    "bestbuy_final.drop_duplicates(inplace = True)\n",
    "\n",
    "bestbuy_final = bestbuy_final.sort_values(by = ['Review Model', 'Review title', 'Review Content', 'scraping_date'])\n",
    "\n",
    "bestbuy_final['Competitor_Flag'] = bestbuy_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "bestbuy_final['Country'] = 'US'\n",
    "\n",
    "bestbuy_final_version= bestbuy_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review_Date': 'Review_Date',\n",
    "    # 'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Review promotion': 'Promotion_Flag',\n",
    "    'Review aggregation': 'Aggregation_Flag'\n",
    "})\n",
    "\n",
    "bestbuy_final_version.drop(columns = ['Review Recommendation',  'People_find_unhelpful'],inplace = True)\n",
    "# bestbuy_final_version.to_csv('Bestbuy_NPI_review.csv',index = False)\n",
    "\n",
    "\n",
    "# %%\n",
    "Final_review = pd.concat([final_review, bestbuy_final_version], ignore_index = True)\n",
    "# Final_review = pd.concat([review_template, bestbuy_final_version], ignore_index = True)\n",
    "Final_review\n",
    "\n",
    "# %%\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "Final_review.info()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Staple\n",
    "\n",
    "def max_pages(sku, url):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "\n",
    "    # Create payload as a dictionary\n",
    "    payload = {\n",
    "        'tenantType': 'StaplesDotCom',\n",
    "        'sku': sku,\n",
    "        'offset': 0,\n",
    "        'limit': 20,\n",
    "        'includeRelated': 'false',\n",
    "        'filterByRating': 0,\n",
    "        'relatedOnly': 'false',\n",
    "        'includeRatingOnlyReviews': 'true',\n",
    "        'filterByPhotos': 'false',\n",
    "        'sortBy': 'date',\n",
    "        'sortOrder': 'desc'\n",
    "    }\n",
    "\n",
    "    # Convert the payload dictionary to a query string\n",
    "    query_string = urllib.parse.urlencode(payload)\n",
    "\n",
    "    # Construct the full URL with the query string\n",
    "    full_url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "    # Set headers for the GET request\n",
    "    headers = {\n",
    "        'Referer': url\n",
    "        # Add any other necessary headers here\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(full_url, headers=headers, verify=False)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content\n",
    "        data = response.json()\n",
    "        review_count = data['reviewList']['total']\n",
    "        max_pages = math.ceil(review_count / 20)\n",
    "        return max_pages\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def staple_review(url, sku, max_pages):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "\n",
    "    headers = {\n",
    "        'Referer': url\n",
    "        # Add any other necessary headers here\n",
    "    }\n",
    "\n",
    "    all_data = []  # List to store data from all pages\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        payload = {\n",
    "            'tenantType': 'StaplesDotCom',\n",
    "            'sku': sku,\n",
    "            'offset': (page - 1) * 20,\n",
    "            'limit': 20,\n",
    "            'includeRelated': 'false',\n",
    "            'filterByRating': 0,\n",
    "            'relatedOnly': 'false',\n",
    "            'includeRatingOnlyReviews': 'true',\n",
    "            'filterByPhotos': 'false',\n",
    "            'sortBy': 'date',\n",
    "            'sortOrder': 'desc'\n",
    "        }\n",
    "\n",
    "        # Convert the payload dictionary to a query string\n",
    "        query_string = urllib.parse.urlencode(payload)\n",
    "\n",
    "        # Construct the full URL with the query string\n",
    "        full_url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "        # Make the GET request with SSL verification disabled\n",
    "        response = requests.get(full_url, headers=headers, verify=False)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON content\n",
    "            data = response.json()\n",
    "            reviews = data['reviewList']['reviews']\n",
    "            df = pd.DataFrame(reviews)\n",
    "            all_data.append(df)\n",
    "            # print('Reviews count on page', page, ':', len(df))\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code} on page {page}\")\n",
    "\n",
    "    # print('Total Reviews count:', sum(len(df) for df in all_data))\n",
    "\n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    result_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# %%\n",
    "def extract_sku_from_url(url):\n",
    "    # Use regular expression to extract numeric value from the end of the URL\n",
    "    match = re.search(r'/(\\d+)$', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# ### Novellie\n",
    "# urls = ['https://www.staples.com/ptd/review/24514342',\n",
    "# 'https://www.staples.com/ptd/review/24503811']\n",
    "\n",
    "urls = ['https://www.staples.com/ptd/review/24583383',\n",
    "'https://www.staples.com/ptd/review/24583386',\n",
    "'https://www.staples.com/ptd/review/24583387',\n",
    "       'https://www.staples.com/ptd/review/24583384'\n",
    "       'https://www.staples.com/ptd/review/24583385']\n",
    "\n",
    "### 3M \n",
    "      # 'https://www.staples.com/ptd/review/24532269',\n",
    "      #   'https://www.staples.com/ptd/review/24455371',\n",
    "      #  'https://www.staples.com/ptd/review/24455373',\n",
    "      #  'https://www.staples.com/ptd/review/24455374' ]\n",
    "\n",
    "# %%\n",
    "staples_df_hp = pd.DataFrame()\n",
    "\n",
    "for url in urls:\n",
    "    sku = extract_sku_from_url(url)\n",
    "    print('Get reviews from', url)\n",
    "    print('Total pages',max_pages(sku,url))\n",
    "    page = max_pages(sku,url)\n",
    "    data = staple_review(url, sku, page)\n",
    "    print('Total Reviews scraped:', len(data))\n",
    "    if data is not None:\n",
    "        staples_df_hp = pd.concat([staples_df_hp, data], axis=0)\n",
    "        \n",
    "\n",
    "# %%\n",
    "def extract_source_name(user_dict):\n",
    "    if isinstance(user_dict, dict):\n",
    "        return user_dict.get('sourceName', '')\n",
    "    else:\n",
    "        return ''\n",
    "if 'syndication' in staples_df_hp.columns:    \n",
    "    staples_df_hp['syndication'] = staples_df_hp['syndication'].apply(extract_source_name)\n",
    "staple_final = staples_df_hp[staples_df_hp['published'] == True]\n",
    "staple_final['Model'] = staple_final['catalogItems'].apply(lambda x: x[0]['title'] if x else None)\n",
    "if 'syndication' in staple_final.columns:\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating',  'user', 'syndication', 'incentivized', 'Model']]\n",
    "else:\n",
    "    # If the condition is not met, exclude the 'syndication' column\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating', 'user', 'incentivized', 'Model']]\n",
    "\n",
    "def tidy_up_user(user):\n",
    "    if isinstance(user, dict) and 'nickName' in user:\n",
    "        return user['nickName']\n",
    "    else:\n",
    "        return 'blank'\n",
    "    \n",
    "\n",
    "    \n",
    "staple_final['user'] = staple_final['user'].apply(tidy_up_user)\n",
    "\n",
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "staple_final['Retailer']=\"Staples\"\n",
    "staple_final['scraping_date'] =pd.to_datetime(date.today())\n",
    "staple_final['HP Model Number'] = staple_final['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "# staple_final['Review date'] = pd.to_datetime(staple['Review date'])\n",
    "\n",
    "staple_hp_combine = pd.merge(staple_final, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "staple_hp_combine['Review Model'] = staple_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model','id']  \n",
    "  \n",
    "staple_hp_combine.drop(columns_to_drop, axis = 1,inplace = True) \n",
    "\n",
    "\n",
    "staple_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'dateCreated': 'Review_Date',\n",
    "    'text': 'Review_Content',\n",
    "    # 'URL': 'URL',\n",
    "    'title': 'Review_Title',\n",
    "    # 'Response name': 'Response_Name',\n",
    "    # 'Response text': 'Response_Text',\n",
    "    # 'Response date': 'Response_Date',\n",
    "    'incentivized': 'Seeding_Flag',\n",
    "    'user': 'Review_Name',\n",
    "    # 'People_find_helpful': 'People_Find_Helpful',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'rating': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    # 'Segment': 'Segment',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model'\n",
    "    # 'reviewedDate': 'Review_Date'\n",
    "    # 'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "staple_final = staple_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "staple_final['Competitor_Flag'] = staple_final['Review_Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "# staple_final.drop('id', axis = 1) \n",
    "staple_final.drop_duplicates(inplace = True)\n",
    "staple_final['Review_Date'] = pd.to_datetime(staple_final['Review_Date']).dt.date\n",
    "\n",
    "# %%\n",
    "Final_review_all = pd.concat([Final_review, staple_final], ignore_index=True)\n",
    "pattern = r'(\\w+ \\d{1,2}, \\d{4})'\n",
    "Final_review_all['Response_Date'] = Final_review_all['Response_Date'].fillna('').str.extract(pattern)\n",
    "Final_review_all['Response_Date'] = pd.to_datetime(Final_review_all['Response_Date'],errors = 'coerce').dt.date.fillna('')\n",
    "Final_review_all['Review_Date'] = pd.to_datetime(Final_review_all['Review_Date']).dt.date\n",
    "Final_review_all['Review_Rating'] = Final_review_all['Review_Rating'].astype('int64')\n",
    "Final_review_all['People_Find_Helpful'] = Final_review_all['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review_all['Scraping_Date'] =  pd.to_datetime(Final_review_all['Scraping_Date']).dt.date\n",
    "Final_review_all.sort_values(by = ['Review_Date'], ascending=False)\n",
    "Final_review_filter = Final_review_all\n",
    "Final_review_filter ['Review_Date'] = pd.to_datetime(Final_review_filter['Review_Date']).dt.date\n",
    "Final_review_filter ['Review_Rating'] = Final_review_filter['Review_Rating'].astype(int)\n",
    "Final_review_filter['Review_Rating_Label'] = Final_review_filter['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "Final_review_filter\n",
    "\n",
    "# %%\n",
    "previous = pd.read_csv(r\"C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\MMK\\MMK_web_review_raw data.csv\")\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date']).dt.date\n",
    "previous['People_Find_Helpful'] = previous['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "previous['Scraping_Date'] =  pd.to_datetime(previous['Scraping_Date']).dt.date\n",
    "previous ['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "def extract_first_ten_words(row):\n",
    "    words = row.split()\n",
    "    return ''.join(words[:10])\n",
    "\n",
    "# Apply the function to create a new column\n",
    "# previous['FirstTenWords'] = previous['Review_Content'].fillna(\"\").apply(extract_first_ten_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned_text)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "Final_review_filter['FirstTenWords'] = Final_review_filter['Review_Content'].fillna(0).apply(clean_text)\n",
    "df_concat = pd.concat([previous, Final_review_filter],ignore_index=True)\n",
    "df_concat.sort_values(by = ['Review_Date','Review_Model','Retailer','FirstTenWords','Scraping_Date'],inplace = True)\n",
    "df_concat_final = df_concat.drop_duplicates(subset=['Review_Model', 'Retailer',  'Review_Date','Review_Rating', 'FirstTenWords'], keep='first')\n",
    "df_concat_final.sort_values(by = ['Scraping_Date','Review_Date'], ascending=False)\n",
    "df_concat_final['Scraping_Date'] = pd.to_datetime(df_concat_final['Scraping_Date']).dt.date\n",
    "# df_concat_final.loc[:,'Scraping_Date'] = pd.to_datetime(df_concat_final['Scraping_Date']).dt.date\n",
    "df_concat_final.drop(columns = 'FirstTenWords',inplace = True)\n",
    "df_concat_final.head()\n",
    "\n",
    "# %%\n",
    "df_concat_final.to_csv(r\"C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\MMK\\MMK_web_review_raw data.csv\", index = False)\n",
    "\n",
    "print('MMK_raw_data_scraping completed. MMK_raw file saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5454551-e8e3-491e-9dbc-a932546b69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "def update_time_left():\n",
    "    now = datetime.now()\n",
    "    next_year = now.year + 1\n",
    "    new_year = datetime(next_year, 1, 1, 0, 0, 0)\n",
    "    time_left = new_year - now\n",
    "    \n",
    "    hours_left, remainder = divmod(time_left.seconds, 3600)\n",
    "    minutes_left, seconds_left = divmod(remainder, 60)\n",
    "    time_left_label.config(\n",
    "        text=f\"{time_left.days} days, {hours_left} hrs, {minutes_left} mins, {seconds_left} secs\"\n",
    "    )\n",
    "    \n",
    "    current_time_label.config(\n",
    "        text=f\"Current Time: {now.strftime('%H:%M:%S')}\"\n",
    "    )\n",
    "    root.after(1000, update_time_left)  # Update every second\n",
    "\n",
    "def start_countdown():\n",
    "    for i in range(10, -1, -1):  # Countdown from 10 to 0\n",
    "        countdown_label.config(text=str(i), fg=colors[i % len(colors)])\n",
    "        root.update()\n",
    "        time.sleep(1)\n",
    "    display_new_year_message()\n",
    "\n",
    "def display_new_year_message():\n",
    "    countdown_label.config(\n",
    "        text=\" HAPPY NEW YEAR! \", \n",
    "        fg=\"gold\", \n",
    "        font=(\"Comic Sans MS\", 48, \"bold\"),\n",
    "    )\n",
    "\n",
    "# Set up the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"New Year Countdown Timer\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "# Load and set the festive background image\n",
    "bg_image_path = \"festive_bg.jpg\"  # Replace with your festive image file path\n",
    "bg_image = Image.open(bg_image_path).resize((800, 600), Image.Resampling.LANCZOS)\n",
    "bg_photo = ImageTk.PhotoImage(bg_image)\n",
    "\n",
    "# Set background color if image fails\n",
    "root.config(bg='black')\n",
    "\n",
    "# Background label to show image\n",
    "bg_label = tk.Label(root, image=bg_photo)\n",
    "bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Ensure the image stays referenced so Tkinter doesn't garbage collect it\n",
    "root.bg_photo = bg_photo\n",
    "\n",
    "# Colors for the countdown\n",
    "colors = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"]\n",
    "\n",
    "# Current Time Label\n",
    "current_time_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 20), fg=\"white\", bg=\"black\"\n",
    ")\n",
    "current_time_label.pack(pady=20)  # Top area\n",
    "\n",
    "# Time Left Label\n",
    "time_left_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 24, \"bold\"), fg=\"cyan\", bg=\"black\"\n",
    ")\n",
    "time_left_label.pack(pady=20)  # Below current time\n",
    "\n",
    "# Countdown Label\n",
    "countdown_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 72, \"bold\"), fg=\"white\", bg=\"black\"\n",
    ")\n",
    "countdown_label.pack(pady=100)  # Center area\n",
    "\n",
    "# Start Button\n",
    "start_button = tk.Button(\n",
    "    root, text=\"Start Countdown\", command=start_countdown, \n",
    "    font=(\"Comic Sans MS\", 20, \"bold\"), bg=\"dark blue\", fg=\"white\"\n",
    ")\n",
    "start_button.pack(pady=30)  # Bottom area\n",
    "\n",
    "# Initialize the time display\n",
    "update_time_left()\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62aa5240-3c51-4a75-9c8e-80d3132854c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "image \"pyimage5\" doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m bg_image \u001b[38;5;241m=\u001b[39m bg_image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m600\u001b[39m), Image\u001b[38;5;241m.\u001b[39mResampling\u001b[38;5;241m.\u001b[39mLANCZOS)\n\u001b[0;32m     45\u001b[0m bg_photo \u001b[38;5;241m=\u001b[39m ImageTk\u001b[38;5;241m.\u001b[39mPhotoImage(bg_image)\n\u001b[1;32m---> 47\u001b[0m bg_label \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mLabel(root, image\u001b[38;5;241m=\u001b[39mbg_photo)\n\u001b[0;32m     48\u001b[0m bg_label\u001b[38;5;241m.\u001b[39mplace(relwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, relheight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Colors for the countdown\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\tkinter\\__init__.py:3236\u001b[0m, in \u001b[0;36mLabel.__init__\u001b[1;34m(self, master, cnf, **kw)\u001b[0m\n\u001b[0;32m   3218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, master\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cnf\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m   3219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a label widget with the parent MASTER.\u001b[39;00m\n\u001b[0;32m   3220\u001b[0m \n\u001b[0;32m   3221\u001b[0m \u001b[38;5;124;03m    STANDARD OPTIONS\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3234\u001b[0m \n\u001b[0;32m   3235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3236\u001b[0m     Widget\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, master, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, cnf, kw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Andy\\Lib\\tkinter\\__init__.py:2647\u001b[0m, in \u001b[0;36mBaseWidget.__init__\u001b[1;34m(self, master, widgetName, cnf, kw, extra)\u001b[0m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m   2646\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cnf[k]\n\u001b[1;32m-> 2647\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mcall(\n\u001b[0;32m   2648\u001b[0m     (widgetName, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w) \u001b[38;5;241m+\u001b[39m extra \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options(cnf))\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m   2650\u001b[0m     k\u001b[38;5;241m.\u001b[39mconfigure(\u001b[38;5;28mself\u001b[39m, v)\n",
      "\u001b[1;31mTclError\u001b[0m: image \"pyimage5\" doesn't exist"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "def update_time_left():\n",
    "    now = datetime.now()\n",
    "    next_year = now.year + 1\n",
    "    new_year = datetime(next_year, 1, 1, 0, 0, 0)\n",
    "    time_left = new_year - now\n",
    "    \n",
    "    hours_left, remainder = divmod(time_left.seconds, 3600)\n",
    "    minutes_left, seconds_left = divmod(remainder, 60)\n",
    "    time_left_label.config(\n",
    "        text=f\"{time_left.days} days, {hours_left} hrs, {minutes_left} mins, {seconds_left} secs\"\n",
    "    )\n",
    "    \n",
    "    current_time_label.config(\n",
    "        text=f\"Current Time: {now.strftime('%H:%M:%S')}\"\n",
    "    )\n",
    "    root.after(1000, update_time_left)  # Update every second\n",
    "\n",
    "def start_countdown():\n",
    "    for i in range(10, -1, -1):  # Countdown from 10 to 0\n",
    "        countdown_label.config(text=str(i), fg=colors[i % len(colors)])\n",
    "        root.update()\n",
    "        time.sleep(1)\n",
    "    display_new_year_message()\n",
    "\n",
    "def display_new_year_message():\n",
    "    countdown_label.config(\n",
    "        text=\" HAPPY NEW YEAR! \", \n",
    "        fg=\"gold\", \n",
    "        font=(\"Comic Sans MS\", 48, \"bold\"),\n",
    "    )\n",
    "\n",
    "# Set up the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"New Year Countdown Timer\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "# Load and set the festive background image\n",
    "bg_image = Image.open(\"C:\\\\Users\\\\STARK\\\\Downloads\\\\festive_bg.jpg\")  # Replace with your festive image file path\n",
    "bg_image = bg_image.resize((800, 600), Image.Resampling.LANCZOS)\n",
    "bg_photo = ImageTk.PhotoImage(bg_image)\n",
    "\n",
    "bg_label = tk.Label(root, image=bg_photo)\n",
    "bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Colors for the countdown\n",
    "colors = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"]\n",
    "\n",
    "# Current Time Label\n",
    "current_time_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 20), fg=\"white\", bg=\"black\"\n",
    ")\n",
    "current_time_label.pack(pady=20)\n",
    "\n",
    "# Time Left Label\n",
    "time_left_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 24, \"bold\"), fg=\"cyan\", bg=\"black\"\n",
    ")\n",
    "time_left_label.pack(pady=20)\n",
    "\n",
    "# Countdown Label\n",
    "countdown_label = tk.Label(\n",
    "    root, text=\"\", font=(\"Comic Sans MS\", 72, \"bold\"), fg=\"white\", bg=\"black\"\n",
    ")\n",
    "countdown_label.pack(expand=True)\n",
    "\n",
    "# Start Button\n",
    "start_button = tk.Button(\n",
    "    root, text=\"Start Countdown\", command=start_countdown, \n",
    "    font=(\"Comic Sans MS\", 20, \"bold\"), bg=\"dark blue\", fg=\"white\"\n",
    ")\n",
    "start_button.pack(pady=30)\n",
    "\n",
    "# Initialize the time display\n",
    "update_time_left()\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da20f17-f15a-4010-b52d-44634175db5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
