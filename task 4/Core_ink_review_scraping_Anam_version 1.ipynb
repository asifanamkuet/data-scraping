{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for maching HP model and HP Class\n",
    "# path = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "# path = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'data_new'\n",
    "df_amazon = pd.read_excel(path, sheet_name = sheets)\n",
    "df_amazon['Comp Model number'] =  df_amazon['Comp Model number'].astype(str)\n",
    "df_amazon['HP Model Number'] =  df_amazon['HP Model Number'].astype(str)\n",
    "# df_amazon\n",
    "df_amazon.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbac7e9-dbdd-4378-814e-6e54a32ea432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store all scraping results\n",
    "# path = rpath = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "# path = rpath = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "path = rpath = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(path, sheet_name = sheets)\n",
    "review_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977c139",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367beb0-92f5-4fc1-a0f8-7c868f6d225e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function (New Soup with Cookies--ANAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list extracted_reviews has all the dictionaries of review\n",
    "\n",
    "def get_soup(url):\n",
    "    headers = {\n",
    "        \"Host\": \"www.amazon.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    \n",
    "    cookies = {\n",
    "        \"id_pkel\": \"n0\",\n",
    "        \"session-token\": \"\\\"JCjKSSKTw7jVnrHceNZ1Fu9RrQvAIa+s2p8SaNUNOmiDV++3Zu+IzXgcDxVF1R7afT+dELYyTqw1tjXikJRvLOqF/fB796ptA6w2imoOQW3RXiNVLk1+/wQ3Gyv7CO9mTHo6K5nznt80rWbD8JPTrkrA4cfYIM/7bOVpd5LrmWEetgku5048aKmWOHN1mLV81SEzBdOeRFF+bn2FFhiDhuX694izkkgPtGfiJl8ZhsVl/bDHuPhwXZW3mGLo0NHuCRCiV3y04371l+4ezeInm8odlirZdgtdCeUgQX3BaHgUu0mdD/NTYNReC11kVYyjQy2PxiQ+H5J3yJ1mJKA2xP0si7I3yClRfE7Yox2LDLkmITohV7/0SQ==\\\"\",\n",
    "        \"x-main\": \"\\\"DqP4xE7LeosRnDhmsoZxlQWlOSq0S@H3kfs6w5KT@IJvSqyW@FP5YDzCVNF2?Or@\\\"\",\n",
    "        \"at-main\": \"Atza|IwEBILqIzWhp33lFOzPr-LefQfnaGe7DjvswGMC91WCpK7fv7j_HKjl4Nzwb3NssJedEoFPlU0o3FpNwpz28DesoYtVg7TZVLJt0vKqqOyZf0QAqx-IdUEezH4MAk_hI0Mvc2NUQavOYlPjRHjsD4u0ScUEL5eyOz9Phfczu3nQbebANxVYLZLmJh5vG_Q6QBV6q_dA6MoN8ZO57k-K4N3KVn0RvL6JMfsZT6nlChD-aTpxDBg\",\n",
    "        \"sess-at-main\": \"\\\"gb/7B4csGrwlKApsj93tsowvv4Be5sxIMfJBWCOAl0s=\\\"\",\n",
    "        \"sst-main\": \"Sst1|PQEUieAFX8NNHVzhl1NKjGddCbufYeBVV0Uhty2ygjXgz6IpJdCudqKsA5hI3uA9mtpAJ8CYGj0G7xLU7zSG8uzF9LPjKaRrEqJ4DKYJunvj5gG_35tyPPC5gsUElqjjrq82W078uJs491QbzjBJ_HVygvByC60QKjF5NU1gMf43JYgnJm2e1I6S01sLrkP0MlJV4aRznUDYJ7J0ArUh7ytFY1TlUrlLk4NhfgmX6DrthT7orjIxRvc1LD-TXFRkHKZOGSdP7D-7Ub2qULE_YibbRbuvjj0YOe8Uh_oFsmbtvp0\",\n",
    "        \"i18n-prefs\": \"USD\",\n",
    "        \"lc-main\": \"en_US\",\n",
    "        \"sp-cdn\": \"\\\"L5Z9:IN\\\"\",\n",
    "        \"ubid-main\": \"132-2235232-6889017\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"session-id\": \"131-3748265-2522507\",\n",
    "        \"csm-hit\": \"tb:ZK52AJH8KQJXFXD7BSAA+s-7N9Q8F0T2KMBKYCFNT10|1724917542317&t:1724917542317&adb:adblk_no\"\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    return soup\n",
    " \n",
    "  \n",
    "def amazon_review(soup, url):    \n",
    "    review = {} #create a dictionary\n",
    "    extracted_reviews = []  #create an list\n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    \n",
    "    for item in reviews:    \n",
    "        review = {    \n",
    "            'Model': model,    \n",
    "            'Review date': item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1],     \n",
    "            \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        \n",
    "  \n",
    "        try:    \n",
    "            review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "        except AttributeError:    \n",
    "            review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "  \n",
    "        try:    \n",
    "            review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "  \n",
    "        try:    \n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        try:      \n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "        except AttributeError:        \n",
    "            review[\"Review name\"] = None  \n",
    "  \n",
    "        try:    \n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"People_find_helpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            if seeding:\n",
    "               review['Seeding or not'] = seeding\n",
    "            else:\n",
    "                raise AttributeError\n",
    "        except AttributeError:  \n",
    "            try: \n",
    "                review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, text='Vine Customer Review of Free Product')\n",
    "\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "\n",
    "\n",
    "        try:\n",
    "            review['Aggregation_Flag'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "             review['Aggregation_Flag'] = None\n",
    "\n",
    "            \n",
    "    \n",
    "        #extract one page of review \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "  \n",
    "    return extracted_reviews \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27318c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HP Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f67b2b-a51e-4dfb-8c04-7a81d1d9b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run for all CPE, URL from excel\n",
    "# path =  r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "# sheets = 'Amazon'\n",
    "# amazon_url = pd.read_excel(path, sheet_name = sheets)\n",
    "\n",
    "# all_list = amazon_url['HP URL'].to_list()\n",
    "\n",
    "# urls = []\n",
    "# for value in all_list:\n",
    "#     if value is not None and value not in urls:\n",
    "#         urls.append(value)\n",
    "# print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c9034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ad hoc run for CPE\n",
    "\n",
    "urls = [\n",
    "    ## Malbec\n",
    "'https://www.amazon.com/product-reviews/B08WCDLKFK/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "# 'https://www.amazon.com/product-reviews/B0B4PMYCWC/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "\n",
    "# ## Manhantten\n",
    "# 'https://www.amazon.com/product-reviews/B08QR6P8KV/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format',\n",
    "# 'https://www.amazon.com/product-reviews/B08WC8VD8G/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format'\n",
    "\n",
    "## CISS\n",
    "    # 'https://www.amazon.com/product-reviews/B0BL466Y41/ref=cm_cr_arp_d_viewopt_sr?',\n",
    "    # 'https://www.amazon.com/product-reviews/B09TPZ3HLZ/ref=cm_cr_arp_d_viewopt_srt?'\n",
    "\n",
    "## Novellie\n",
    "# 'https://www.amazon.com/product-reviews/B09NS5HDWY'\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['one','two','three','four','five'] \n",
    "max_retry_attempts = 10\n",
    "all_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for page in range(1, 10):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url = f'{link}/ref=cm_cr_arp_d_viewopt_fmt?filterByStar={y}_star&pageNumber={page}&sortBy=recent&formatType=current_format'  \n",
    "                    print(url)\n",
    "                    print('Page:',page, f'{y} star')\n",
    "                    try: #added\n",
    "                        soup = get_soup(url)  # Get the soup object from the URL\n",
    "                    except Exception as e:\n",
    "                        print(\"Error in soup: \",e) #added\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                            print('No review')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                    \n",
    "                    #extend is to add a list of items \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews) #extend is for appending a list \n",
    "                        print(f\"Page {page} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if page >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {page} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {page} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a5543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(all_reviews[0]))\n",
    "print(len(all_reviews))\n",
    "all_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None) #not doing anything\n",
    "amazon2= pd.DataFrame(all_reviews) #convert list to df\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)') #this is regex\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "\n",
    "amazon_hp_combine = pd.merge(amazon2, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine = amazon_hp_combine.drop_duplicates()\n",
    "amazon_hp_combine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a90049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save HP Amazon review if needed\n",
    "# import datetime \n",
    "# date = datetime.date.today().strftime('%Y%m%d')\n",
    "# amazon_hp_combine.to_csv(f'amazon_hp_{date}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acca421",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comp review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769e37d-9533-4e8c-9262-0276f3967536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read url file\n",
    "# path =  r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "# path =  r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "path =  r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'Amazon'\n",
    "amazon_url = pd.read_excel(path, sheet_name = sheets)\n",
    "urls = amazon_url['Competitor URL'].to_list()\n",
    "\n",
    "\n",
    "link_list = []\n",
    "for value in urls:\n",
    "    if value is not None and value not in link_list:\n",
    "        link_list.append(value)\n",
    "print(len(link_list)) #create a list of comp urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da73d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_list_comp = []  \n",
    "\n",
    "star = ['one','two','three','four','five'] \n",
    "max_retry_attempts = 20\n",
    "\n",
    "for link in urls[0:1]:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for page in range(1, 10):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    print(\"===============\")\n",
    "                    url = f'{link}/ref=cm_cr_arp_d_viewopt_fmt?filterByStar={y}_star&pageNumber={page}&sortBy=recent&formatType=current_format'  \n",
    "                    print(url)\n",
    "                    print('Page:',page, f'{y} star')\n",
    "                    soup = get_soup(url)  # Get the soup object from the URL\n",
    "                    #print(soup)\n",
    "                    #exit()\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                            print('No review')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                    \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        review_list_comp.extend(extracted_reviews)\n",
    "                        print(f\"Page {page} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if page >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {page} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {page} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e23cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import date \n",
    "amazon_comp= pd.DataFrame(review_list_comp)\n",
    "#debug: print amazon_comp to check before the Review date \n",
    "print(amazon_comp)\n",
    "amazon_comp['Retailer']=\"Amazon\"\n",
    "amazon_comp['scraping_date'] =  pd.to_datetime(date.today())\n",
    "amazon_comp['Review date'] = pd.to_datetime(amazon_comp['Review date']) #because there is no data\n",
    "amazon_comp['Review title'] = amazon_comp['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon_comp['Comp Model number'] = amazon_comp['Model'].str.extract(r'(\\d+)')\n",
    "amazon_comp['People_find_helpful'] = amazon_comp['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_comp = amazon_comp.drop_duplicates()\n",
    "\n",
    "amazon_comp_combine = pd.merge(amazon_comp, df_amazon, on = 'Comp Model number', how = \"left\" )\n",
    "amazon_comp_combine['Review Model'] = amazon_comp_combine['Comp Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "\n",
    "amazon_comp_combine = amazon_comp_combine.drop(columns_to_drop, axis = 1) \n",
    "amazon_comp_combine['Comp Model'] = ''\n",
    "amazon_comp_combine['Segment'] = ''\n",
    "amazon_comp_combine['HP Class'] = ''\n",
    "amazon_comp_combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae98d5-c5bd-450c-af85-08a967dabf7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combine comp and HP, concat with previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd79e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add competitors below\n",
    "# amazon_final = pd.concat([amazon_hp_combine, amazon_comp_combine], axis=0, ignore_index=True)  \n",
    "amazon_final = amazon_hp_combine\n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "# amazon_final['Competitor_flag'] = amazon_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'US'\n",
    "amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Country': 'Country'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8e5da-1589-4f1a-b81a-c16b454c3095",
   "metadata": {},
   "source": [
    "## Read from SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use encryption and pyodbc\n",
    "import pyodbc\n",
    "\n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating' \n",
    "\n",
    "\n",
    "# Establish connection using pyodbc\n",
    "conn = pyodbc.connect(r'Driver={ODBC Driver 17 for SQL Server};Server='+server+';Database='+database+';Trusted_Connection=yes;Encrypt=yes;TrustServerCertificate=yes')\n",
    "\n",
    "#Change model number accordingly\n",
    "existing_rows_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM STAR_Rating.dbo.Ink_web_reviews\n",
    "    WHERE Retailer IN ('bestbuy')\n",
    "    AND (Review_Model LIKE ('%2755e%')\n",
    "    OR Review_Model LIKE ('%4155e%'))\n",
    "\"\"\"\n",
    "\n",
    "result_df = pd.read_sql_query(existing_rows_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f395fd",
   "metadata": {},
   "source": [
    "## Read from stackline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For taccola\n",
    "amazon2 = pd.read_csv(r\"C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\My Scripts\\segment_Taccola_reviews_reviewTrends_Amazon_US_201609_202409.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1278d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon2 = amazon2.rename(columns={\n",
    "    'Title':'Model',\n",
    "     'Reviewer': 'Review name',\n",
    "    'Review Stars': 'Review rating',\n",
    "    'Review Text': 'Review Content',\n",
    "    'Review Title': 'Review title',\n",
    "    'Is Verified': 'Verified Purchase or not',\n",
    "    'Reviews Url': 'URL',\n",
    "     'Has Response':'Response_Text'\n",
    "})\n",
    "\n",
    "amazon2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2[ 'Review Date'] = pd.to_datetime(amazon2[ 'Review Date'])\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "\n",
    "amazon_hp_combine = pd.merge(amazon2, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine = amazon_hp_combine.drop_duplicates()\n",
    "amazon_hp_combine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeffb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_final = amazon_hp_combine\n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'US'\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review Date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Country': 'Country',\n",
    "    'Review ID':'Review_ID'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "amazon_final_df.columns \n",
    "amazon_final_df.drop(columns=['Retailer ID', 'Retailer Name', 'Retailer SKU', 'UPC', 'Model Number',\n",
    "       'Brand', 'Category', 'Subcategory', 'Week ID', 'Week Ending'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ef213",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = amazon_final_df \n",
    "previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d8409-2e5d-4406-8a09-cc07786a4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df['Review_Date'] = pd.to_datetime(result_df['Review_Date'].dt.date)\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned_text)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "# previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "result_df['FirstTenWords'] = result_df['Review_Content'].fillna(0).apply(clean_text)\n",
    "amazon_final_df['FirstTenWords'] = amazon_final_df['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "non_duplicated_df = amazon_final_df[(~amazon_final_df['Review_Date'].isin(result_df['Review_Date']))&\n",
    "                                   (~amazon_final_df['FirstTenWords'].isin(result_df['FirstTenWords']))].drop_duplicates()\n",
    "non_duplicated_df.drop(columns = ['FirstTenWords'], inplace = True)\n",
    "non_duplicated_df.rename(columns = {'Aggregation':'Aggregation_Flag'}, inplace = True)\n",
    "non_duplicated_df.sort_values(by = ['Review_Date'],ascending = True)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "non_duplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_df.drop(columns = 'Segment Name', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to SQL\n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating' \n",
    "\n",
    "# Establish connection using pyodbc\n",
    "conn = pyodbc.connect(r'Driver={ODBC Driver 17 for SQL Server};Server='+server+';Database='+database+';Trusted_Connection=yes;Encrypt=yes;TrustServerCertificate=yes')\n",
    "\n",
    "dataframe = non_duplicated_df\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "chunk_size = 10000\n",
    "total_rows = len(dataframe)\n",
    "num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Iterate over chunks and insert into the database\n",
    "for i in range(num_chunk):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = min((i + 1) * chunk_size, total_rows)  # Ensure end index doesn't exceed total rows\n",
    "    chunk = dataframe.iloc[start_index:end_index]\n",
    "\n",
    "    # Generate SQL INSERT statement\n",
    "    sql_insert = f\"INSERT INTO {table} ([{'], ['.join(chunk.columns)}]) VALUES ({', '.join(['?' for _ in chunk.columns])})\"\n",
    "\n",
    "    # Convert chunk DataFrame to list of tuples\n",
    "    records = [tuple(row) for row in chunk.values]\n",
    "\n",
    "    # Execute SQL INSERT statement\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(sql_insert, records)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")\n",
    "    \n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66853c",
   "metadata": {},
   "source": [
    "# Bestbuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def get_review_bestbuy(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.61',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "def bestbuy_review(soup, url):    \n",
    "    bestbuy = {}\n",
    "    bestbuy_reviews = []  \n",
    "    Model = soup.find(\"h1\", {'class':\"heading-5 v-fw-regular\"})\n",
    "    if not Model:\n",
    "        Model = soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"})\n",
    "    Model = Model.text if Model else None\n",
    "    \n",
    "        \n",
    "    npi = soup.find('span',{'class':'c-reviews order-2'} ).text\n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "    if review_session:\n",
    "        for item in review_session:    \n",
    "            bestbuy = {    \n",
    "                 'Model':Model,\n",
    "                # 'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "                'URL':url \n",
    "            }\n",
    "            try:    \n",
    "                bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review title']  = None\n",
    "\n",
    "            try:\n",
    "                bestbuy['Review_Name']  = item.find(\"div\", {\"class\": \"ugc-author v-fw-medium body-copy-lg\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review_Name']  = None\n",
    "                \n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review rating']  = None\n",
    "\n",
    "            review_date_element = item.find(\"time\", {\"class\": \"submission-date\"})\n",
    "            if review_date_element:\n",
    "                review_date_string = review_date_element['title']\n",
    "                review_date_datetime = datetime.strptime(review_date_string, '%b %d, %Y %I:%M %p')\n",
    "                formatted_review_date = review_date_datetime.strftime('%Y-%m-%d')\n",
    "                bestbuy['Review_Date'] = formatted_review_date\n",
    "            else:\n",
    "                bestbuy['Review_Date'] = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review promotion']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review aggregation']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Content']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Recommendation']  = None\n",
    "\n",
    "            try:    \n",
    "                network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "                if network_badge:\n",
    "                    bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "                else:\n",
    "                    bestbuy['Seeding or not']  = \"\"\n",
    "            except AttributeError:    \n",
    "                bestbuy['Seeding or not']  = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_helpful']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_unhelpful']  = None\n",
    "\n",
    "\n",
    "            bestbuy_reviews.append(bestbuy)    \n",
    "        \n",
    "    \n",
    "  \n",
    "    return npi, bestbuy_reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8750-55bd-4732-bedf-a324c3aac49d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best buy hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ba74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "\n",
    "## Manhatten\n",
    "#     'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9015e-wireless-all-in-one-inkjet-printer-with-6-months-of-instant-ink-included-with-hp-white/6450667?variant=A',\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9025e-wireless-all-in-one-inkjet-printer-with-6-months-of-instant-ink-included-with-hp-white/6450665?variant=A'\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-smart-tank-5101-wireless-all-in-one-supertank-inkjet-printer-with-up-to-2-years-of-ink-included-white/6527565?variant=A'\n",
    "\n",
    "\n",
    "## Taccola\n",
    "    'https://www.bestbuy.com/site/reviews/hp-deskjet-2755e-wireless-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6454282?variant=A',\n",
    "   'https://www.bestbuy.com/site/reviews/hp-deskjet-4155e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6454283?variant=A'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestbuy_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    should_continue = True\n",
    "    for x in range(1, 25):\n",
    "        if not should_continue:\n",
    "            break\n",
    "        while True:\n",
    "            url = f'{link}&page={x}&sort=MOST_RECENT'\n",
    "            try:\n",
    "                soup = get_review_bestbuy(url)\n",
    "                npi, reviews = bestbuy_review(soup, url)\n",
    "                if npi == 'Be the first to write a review':\n",
    "                    should_continue = False\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')\n",
    "                bestbuy_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-disabled\": \"true\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                if x > 1 and next_page_link and next_page_link.get(\"aria-disabled\") == \"true\":\n",
    "                    should_continue = False\n",
    "                    print('No more pages left')\n",
    "                    break\n",
    "\n",
    "                if len(reviews) < 20:\n",
    "                    should_continue = False\n",
    "                    print('Only 1 page')\n",
    "                    break\n",
    "                else:\n",
    "                    break \n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "review = pd.DataFrame(bestbuy_reviews)\n",
    "review['Retailer']=\"bestbuy\"\n",
    "review['scraping_date'] = pd.to_datetime(date.today())\n",
    "\n",
    "review['HP Model Number'] = review['Model'].str.extract(r'(\\d+e*)')\n",
    "\n",
    "hp_combine = pd.merge(review, df_amazon, on = \"HP Model Number\", how = \"left\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9403bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_combine['Review Model'] = hp_combine['HP Model'] \n",
    "# hp_combine['People_find_helpful'] = hp_combine['People_find_helpful'].fillna(\"\").str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "\n",
    "\n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "hp_combine_bestbuy = hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "hp_combine_bestbuy = hp_combine_bestbuy.drop_duplicates()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b7713-9566-4113-8ef7-e27981f9297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestbuy_final = pd.concat([hp_combine_bestbuy, comp_combine_bestbuy], axis=0, ignore_index=True)  \n",
    "bestbuy_final = hp_combine_bestbuy\n",
    "bestbuy_final.drop_duplicates(inplace = True)\n",
    "\n",
    "bestbuy_final = bestbuy_final.sort_values(by = ['Review Model', 'Review title', 'Review Content', 'scraping_date'])\n",
    "\n",
    "bestbuy_final['Competitor_Flag'] = bestbuy_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "bestbuy_final['Country'] = 'US'\n",
    "\n",
    "bestbuy_final_version= bestbuy_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review_Date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Review promotion': 'Promotion_Flag',\n",
    "    'Review aggregation': 'Aggregation_Flag'\n",
    "})\n",
    "\n",
    "bestbuy_final_version.drop(columns = ['Review Recommendation',  'People_find_unhelpful'],inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3906f45-7fce-4bce-ac9f-d629dda58277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_review = pd.concat([review_template, bestbuy_final_version], ignore_index = True)\n",
    "len(Final_review.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebd866-5c2d-46ef-a720-345b3606b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].str.extract(r'\\((\\d+)\\)')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "Final_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31859f-bb34-4b6b-9e85-144af85ab257",
   "metadata": {},
   "source": [
    "## Save to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac75a3-adf8-4a07-93f1-53bd56855631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use encryption and pyodbc\n",
    "import pyodbc\n",
    "\n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating' \n",
    "\n",
    "# Establish connection using pyodbc\n",
    "conn = pyodbc.connect(r'Driver={ODBC Driver 17 for SQL Server};Server='+server+';Database='+database+';Trusted_Connection=yes;Encrypt=yes;TrustServerCertificate=yes')\n",
    "\n",
    "#Change retailer and model number accordingly\n",
    "existing_rows_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM STAR_Rating.dbo.Ink_web_reviews\n",
    "    WHERE Retailer IN ('bestbuy')\n",
    "    AND (Review_Model LIKE ('%2755e%')\n",
    "    OR Review_Model LIKE ('%4155e%'))\n",
    "\"\"\"\n",
    "\n",
    "result_df = pd.read_sql_query(existing_rows_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc55cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216106b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df['Review_Date'] = pd.to_datetime(result_df['Review_Date'].dt.date)\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned_text)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "Final_review['FirstTenWords'] = Final_review['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "result_df['FirstTenWords'] = result_df['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "\n",
    "non_duplicated_df = Final_review[(~Final_review['Review_Date'].isin(result_df['Review_Date']))&\n",
    "                                   (~Final_review['FirstTenWords'].isin(result_df['FirstTenWords']))].drop_duplicates()\n",
    "non_duplicated_df.drop(columns = ['FirstTenWords'], inplace = True)\n",
    "non_duplicated_df.rename(columns = {'Aggregation':'Aggregation_Flag'}, inplace = True)\n",
    "non_duplicated_df.sort_values(by = ['Review_Date'],ascending = True)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "non_duplicated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_df['People_Find_Helpful'] = non_duplicated_df['People_Find_Helpful'].fillna(0).astype(int)\n",
    "non_duplicated_df.drop(columns = 'Syndicated_Source', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eab7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    " \n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "dataframe = non_duplicated_df\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "chunk_size = 10000\n",
    "total_rows = len(dataframe)\n",
    "num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(num_chunk):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = (i + 1) * chunk_size\n",
    "    chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "    chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "    print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3b7ce-e0a5-43ee-9a10-59a8d7964c3a",
   "metadata": {},
   "source": [
    "# Staple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee74d7-9c8d-416b-afc3-db8d32d52854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "def staple_review(url, sku, max_pages):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Referer': url\n",
    "    }\n",
    "\n",
    "    all_data = []  # List to store data from all pages\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'tenantType': 'StaplesDotCom',\n",
    "            'sku': sku,\n",
    "            'offset': (page - 1) * 20,\n",
    "            'limit': 20,\n",
    "            'includeRelated': 'false',\n",
    "            'relatedOnly': 'false',\n",
    "            'includeRatingOnlyReviews': 'false',\n",
    "            'sortBy': 'date',\n",
    "            'sortOrder': 'desc'\n",
    "        }\n",
    "\n",
    "        # Make the GET request\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON content\n",
    "            data = response.json()\n",
    "            reviews = data.get('reviewList', {}).get('reviews', [])\n",
    "            if reviews:\n",
    "                df = pd.DataFrame(reviews)\n",
    "                all_data.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page}. Status code: {response.status_code}\")\n",
    "\n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    if all_data:\n",
    "        result_df = pd.concat(all_data, ignore_index=True)\n",
    "        return result_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def max_pages(sku, url):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Referer': url\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'tenantType': 'StaplesDotCom',\n",
    "        'sku': sku,\n",
    "        'offset': 0,\n",
    "        'limit': 20,\n",
    "        'includeRelated': 'false',\n",
    "        'relatedOnly': 'false',\n",
    "        'includeRatingOnlyReviews': 'false',\n",
    "        'sortBy': 'date',\n",
    "        'sortOrder': 'desc'\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        total_reviews = data.get('reviewList', {}).get('total', 0)\n",
    "        max_pages = math.ceil(total_reviews / 20)\n",
    "        return max_pages\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return 0\n",
    "\n",
    "def extract_sku_from_url(url):\n",
    "    # Use regular expression to extract numeric value from the end of the URL\n",
    "    match = re.search(r'/(\\d+)$', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Main scraping process\n",
    "staples_df_hp = pd.DataFrame()\n",
    "\n",
    "# Define URLs\n",
    "urls = ['https://www.staples.com/ptd/review/24455371']\n",
    "\n",
    "for url in urls:\n",
    "    sku = extract_sku_from_url(url)\n",
    "    print('Get reviews from', url)\n",
    "    total_pages = max_pages(sku, url)\n",
    "    print('Total pages:', total_pages)\n",
    "    \n",
    "    if total_pages > 0:\n",
    "        data = staple_review(url, sku, total_pages)\n",
    "        print('Total Reviews scraped:', len(data))\n",
    "        if not data.empty:\n",
    "            staples_df_hp = pd.concat([staples_df_hp, data], axis=0)\n",
    "\n",
    "# Post-processing and data cleanup\n",
    "def extract_source_name(user_dict):\n",
    "    if isinstance(user_dict, dict):\n",
    "        return user_dict.get('sourceName', '')\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "if 'syndication' in staples_df_hp.columns:    \n",
    "    staples_df_hp['syndication'] = staples_df_hp['syndication'].apply(extract_source_name)\n",
    "\n",
    "staple_final = staples_df_hp[staples_df_hp['published'] == True]\n",
    "staple_final['Model'] = staple_final['catalogItems'].apply(lambda x: x[0]['title'] if x else None)\n",
    "\n",
    "if 'syndication' in staple_final.columns:\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating', 'user', 'syndication', 'incentivized', 'Model']]\n",
    "else:\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating', 'user', 'incentivized', 'Model']]\n",
    "\n",
    "def tidy_up_user(user):\n",
    "    if isinstance(user, dict) and 'nickName' in user:\n",
    "        return user['nickName']\n",
    "    else:\n",
    "        return 'blank'\n",
    "\n",
    "staple_final['user'] = staple_final['user'].apply(tidy_up_user)\n",
    "\n",
    "# Additional data processing\n",
    "pd.set_option('display.max_columns', None)\n",
    "staple_final['Retailer'] = \"Staples\"\n",
    "staple_final['scraping_date'] = pd.to_datetime(date.today())\n",
    "staple_final['HP Model Number'] = staple_final['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "# Merge and data clean-up\n",
    "staple_hp_combine = pd.merge(staple_final, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "staple_hp_combine['Review Model'] = staple_hp_combine['HP Model'] \n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model', 'id']\n",
    "staple_hp_combine.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "staple_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'HP Class': 'HP_Class',\n",
    "    'dateCreated': 'Review_Date',\n",
    "    'text': 'Review_Content',\n",
    "    'title': 'Review_Title',\n",
    "    'incentivized': 'Seeding_Flag',\n",
    "    'user': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'rating': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Review Model': 'Review_Model'\n",
    "}\n",
    "\n",
    "# 'HP Class': 'HP_Class',\n",
    "#     'Review Model': 'Review_Model',\n",
    "#     'Retailer': 'Retailer',\n",
    "#     'Comp Model': 'Comp_Model',\n",
    "#     'Review_Date': 'Review_Date',\n",
    "#     'Review name': 'Review_Name',\n",
    "#     'Review rating': 'Review_Rating',\n",
    "#     'Review title': 'Review_Title',\n",
    "#     'Review Content': 'Review_Content',\n",
    "#     'Seeding or not': 'Seeding_Flag',\n",
    "#     'People_find_helpful': 'People_Find_Helpful',\n",
    "#     'URL': 'URL',\n",
    "#     'scraping_date': 'Scraping_Date',\n",
    "#     'Review promotion': 'Promotion_Flag',\n",
    "#     'Review aggregation': 'Aggregation_Flag'\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "staple_final = staple_hp_combine.rename(columns=column_mapping)\n",
    "staple_final['Competitor_Flag'] = staple_final['Review_Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "staple_final.drop_duplicates(inplace=True)\n",
    "staple_final['Review_Date'] = pd.to_datetime(staple_final['Review_Date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_review = staple_final \n",
    "Final_review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec95a4-c4e3-4ead-ba63-6e991599b95e",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06eefe-4562-466d-a203-9d0e7b58e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(\\w+ \\d{1,2}, \\d{4})'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "Final_review.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5babd9-8a71-498e-ae2e-e7ef04c6f939",
   "metadata": {},
   "source": [
    "## Save to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query previous review\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# dataframe = amazon_final_df\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    "\n",
    "\n",
    "existing_rows_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {schema}.{table}\n",
    "    WHERE Retailer in ('staples')\n",
    "    AND Review_Model in ('HP OfficeJet Pro 9015e',\n",
    "'HP OfficeJet Pro 9025e'\n",
    ")\n",
    "\"\"\"\n",
    "result_df = pd.read_sql_query(existing_rows_query, engine)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned_text)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "result_df['FirstTenWords'] = result_df['Review_Content'].fillna(0).apply(clean_text)\n",
    "Final_review['FirstTenWords'] = Final_review['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "\n",
    "non_duplicated_df = Final_review[\n",
    "                                   (~Final_review['FirstTenWords'].isin(result_df['FirstTenWords']))].drop_duplicates()\n",
    "# non_duplicated_df.drop(columns = ['FirstTenWords'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61546f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6294f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "filter_date = datetime.date(2023, 12, 14)\n",
    "non_duplicated_df['Review_Date'] = pd.to_datetime(non_duplicated_df['Review_Date']).dt.date\n",
    "\n",
    "# Filter the DataFrame to keep only rows after '2023-12-14'\n",
    "non_duplicated_df = non_duplicated_df[non_duplicated_df['Review_Date'] > filter_date]\n",
    "\n",
    "non_duplicated_df = non_duplicated_df.iloc[:, :-1] ##New Line\n",
    "\n",
    "non_duplicated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd7daa-440d-402a-a45a-25aeb16254ee",
   "metadata": {},
   "source": [
    "## check for error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a39d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    " \n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "dataframe = non_duplicated_df\n",
    "table = \"Ink_web_reviews\"\n",
    "print(dataframe)\n",
    "\n",
    "# dataframe.drop(columns = ['FirstTenWords'], inplace = True)\n",
    "\n",
    "# print(\"new dataframe:\", dataframe)\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "chunk_size = 10000\n",
    "total_rows = len(dataframe)\n",
    "num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(num_chunk):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = (i + 1) * chunk_size\n",
    "    chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "    chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "    print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b969fd-3386-4467-81d3-ed9a7af11055",
   "metadata": {},
   "source": [
    "# Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2245567-7a73-4701-a90d-97634f239715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "User_Agent = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "   ,'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/109.0',\n",
    "    'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-G973U) AppleWebKit/537.36 (KHTML, like Gecko)',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; U; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.5399.183 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/113.0'\n",
    "]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2990a4-c042-4bd0-b7e6-9f82c7c8a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = [\n",
    "    'ACID=e743918f-9c01-4185-9889-01b383f39a46; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMDI5Mjg0MjMwNCwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAwMjkyODQyMzA0LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDAyOTI4NDIzMDMsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAwMjkyODQyMzA0LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6ZTc0MzkxOGYtOWMwMS00MTg1LTk4ODktMDFiMzgzZjM5YTQ2In0%3D; vtc=TPvkWCN79GksgVjpd8lQp4; btc=TPvkWCN79GksgVjpd8lQp4; bsc=XjMXQ-W4dVqYGkP1JSNWEs; _pxvid=d8404a5f-85e4-11ee-a839-f5e2c99825fc; pxcts=dd47a175-85e4-11ee-b2aa-79f32486f419; _tap_path=/rum.gif; _tap-criteo=1700292852331:1700292852725:1; _tap-Ted=1700292852724:1700292852725:1; _tap-lrV=1700292862974:1700292862975:1; _tap-lrB=1700292878531:1700292878532:1; _tap-appnexus=1700292879419:1700292879718:1; _gcl_au=1.1.258469864.1700294686; bstc=XgTuQhQdHEhnMqLxEbRYgw; mobileweb=0; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=; ak_bmsc=928D3BE7B227834B8EF4992840D6AFA1~000000000000000000000000000000~YAAQT0Dfb2Hq79qLAQAAWF0E7xW+cNY2y9+fVR2/0QrvmtyiqK/i/uEwVaXReQYp0Z0y6FV2m0EQCZomlg6PikSMdPE5Yx+Al074WilmM5mGxgz2kYLAXQy0aPKfmDgo9ohHooJNwq7VcMShtmCmJ1ARCqwfhpgMsbNFrR7LgHL3FRZJhrg9alhKuKLZRfToimKXZ0sKxSm5rLcO6L57KGoWlZX3NmA0ddatFrENafTH0mQVlKWpVPKHtgOZZh4EsGHOO90Z1fy0P2hIJNmpHpoDLk1w+0tBu9Upo+8L5KPS0K1LcuVDuaXEx4+n6o2fMg+HrkC1AF5T7cKLiLDZlbL/C5kSesKN3Rp2RaJjt8YzY2QTkaxrmX9TYd6omcLPWSqHLvjJK0/gyfE=; xptc=assortmentStoreId%2B3081; xpm=3%2B1700522123%2BTPvkWCN79GksgVjpd8lQp4~%2B0; b30msc=XgTuQhQdHEhnMqLxEbRYgw; _tap-li=1700522132702:0:2; _uetsid=0cce940087ff11eea88661d0630688d7; _uetvid=237553a085e911ee9eaec99bfa867851; auth=MTAyOTYyMDE4TzUL0tBWqaWlPLiIWSvGIIqP44I9XeKX8N5bcOOhCkNGFqMJTLhMlajVoxZh6%2F64P%2Bx3aAqHkDo7tLTswFgLqhE9gq7sWtCKMSvMMtAlsDnj17kGZ7Mu4K7gp6blyLzB767wuZloTfhm7Wk2KcjygsAEeU%2BeKCMhfP9XV060SY%2Fspww18DSfg4loIXetO33HWWxCKdp%2B8UHdguRD9DC%2FlTyW2FeTzNUdxbN2aHvb8W0UMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHWs0k33oHkBmchRaU9fj5kF7pZ6JaDMzmiWlGlRQ4nUMw5XFK3QDKKcB%2BPe5gKPHMTJomOWP4NHaOZmjOE06S78R4yUE7XpP0usJVgwSa5Hg5dejwrW41QOfpHzdmIzkekjyrOXbKKhH072NS%2FW0j%2FU%3D; locDataV3=eyJpc0RlZmF1bHRlZCI6dHJ1ZSwiaXNFeHBsaWNpdCI6ZmFsc2UsImludGVudCI6IlNISVBQSU5HIiwicGlja3VwIjpbeyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJodWJOb2RlSWQiOiIzMDgxIiwic3RvcmVIcnMiOiIwNjowMC0yMzowMCIsInN1cHBvcnRlZEFjY2Vzc1R5cGVzIjpbIlBJQ0tVUF9DVVJCU0lERSIsIlBJQ0tVUF9JTlNUT1JFIl0sInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQifV0sInNoaXBwaW5nQWRkcmVzcyI6eyJsYXRpdHVkZSI6MzguNDc0NSwibG9uZ2l0dWRlIjotMTIxLjM0MzgsInBvc3RhbENvZGUiOiI5NTgyOSIsImNpdHkiOiJTYWNyYW1lbnRvIiwic3RhdGUiOiJDQSIsImNvdW50cnlDb2RlIjoiVVNBIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJ0aW1lWm9uZSI6IkFtZXJpY2EvTG9zX0FuZ2VsZXMifSwiYXNzb3J0bWVudCI6eyJub2RlSWQiOiIzMDgxIiwiZGlzcGxheU5hbWUiOiJTYWNyYW1lbnRvIFN1cGVyY2VudGVyIiwiaW50ZW50IjoiUElDS1VQIn0sImluc3RvcmUiOmZhbHNlLCJkZWxpdmVyeSI6eyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJhY2Nlc3NQb2ludHMiOlt7ImFjY2Vzc1R5cGUiOiJERUxJVkVSWV9BRERSRVNTIn1dLCJodWJOb2RlSWQiOiIzMDgxIiwiaXNFeHByZXNzRGVsaXZlcnlPbmx5IjpmYWxzZSwic3VwcG9ydGVkQWNjZXNzVHlwZXMiOlsiREVMSVZFUllfQUREUkVTUyJdLCJzZWxlY3Rpb25UeXBlIjoiREVGQVVMVEVEIn0sInJlZnJlc2hBdCI6MTcwMDU0NTYxMjY4MiwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOmU3NDM5MThmLTljMDEtNDE4NS05ODg5LTAxYjM4M2YzOWE0NiJ9; _tap-googdsp=1700524013260:1700524013261:1; bm_mi=6D08B7F70C2629CB7335921BD0A476C4~YAAQVkDfbybwhNCLAQAAiz8h7xVv+k/UWG2WIDe/gPQ8AOZTnjAhiIOZ0bLj8CiGeNePuQWuPNwhDXS1oNp15tCC1PJSEhUXHiX+NSzkvUL7leDWiBNTKpbglVNmWH1NuG/LvNHQA2LBVlOMNtN+pa4w85WJ5WffbcN5bm4oXQMtRoZmDiaP9LejTQc7RtRDzb/UBiKxSj7B7tWFZWvoaMl7Byh7Zw98qmlF62SYLz85rGSvABr10DN471JChbH4Q/G82+KTkYcWus+IMdzcowTy7Z+Q8iX5zSmhRA2ELuD5Ie109BCzItjTb03IbEwxaZaoK0uGP1lkf74XjjcvevsgHZqUduo=~1; _tap-wmt-dw=1700522135009:0:2; _px3=5ea3ebae35f07f1831578749c875dce471cd3c238e9b1d48cc4d8ac7ed6ef5be:/ONT+vAVQYHGOAb5jG51c6bmUcQjbaW1zryLfFf3ZEFrqWRTeciC0Y4hk/ovL1XeinngRvREckraA7RQaqwaeA==:1000:tUHSmj1FtO2NPUlmIukbTE9SnWpD2T21PLjZBMihCAM89elMZOLre7HSRlFqK71v0f8yL9k4n8Sg8C5OWCQ9mrtXRYNZ7J2cHZIgW/Xriz9FwISvvFB2qUm7ermjWRrawUo7rSOqC6UHNjyG4cNmWupEtwDC+dn8hy+VsaMiY64jQxqICji2ZH+yxxSM8rRGtusit4qn5KaOrVpNR2530EK/u3KBOrrKvimdzwbEfh4=; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1700524027817; xptwj=qq:856f56eac45d86299598:ZRnl9M8BUfAYvTYtX+QCNLhEzqw6K+xHmpEjeoxuN7UABA+EM4HcK1WV764ZOO5/6syK6HJNzQ+7oCA1dX0quNAOWtTaQIDcRQupz54KdhbyjvwwBjDPRRo9S45zB814KqqVnZB4xW5GOyMmpEBETfwct40JrJ9yQKgEdWfokI/A+gc=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1700524028000@firstcreate:1700292842259\"; xptwg=3735093285:1780D552F836C50:3B5C8D6:477BA478:6AAF8424:7C4EFE87:; TS01a90220=01419f1d62bb3082af49e4e290c9e4f7b7d4d09f7a4d1f20d21c60c4e1a7da9ee45eaa7b433f2f9eae578eda14efecc83c7473e1aa; bm_sv=FBAB233769AD51E9AA3317FB35DBDC6D~YAAQVkDfb73yhNCLAQAA5XMh7xUG5PTSMRUK4v2TvEXPwemnWtVYlvkgiCKbkuxNXqf+vVZB8rZcQemv+ydTIl18pO1qMpban0Uf+/ci+ZkYfinA4Fd+fcZ1uHAyQxRGF/plo2v0Gq9TxZgp4pp/YFFuxSnGTqGRKl4RkFJbdxQSVM0CBeOhtUQWXZ0y9L4TE3TF7jw/A9Nu++2UrVU5zLqdE22Fo4Pdw6Yn6RnKbC9OZ7gJsF2DhCgy6+IkYxmg/SE=~1; _pxde=8e9d44f04024aa1db6ed77fcc7bbf79de1d603492bbaba0843e90b5cd14038cf:eyJ0aW1lc3RhbXAiOjE3MDA1MjQwMjg5MTR9',\n",
    "    'ACID=7cff0725-9085-4be0-bf3c-6839f8621f69; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMjYzMDM3NDkxNiwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAyNjMwMzc0OTE2LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDI2MzAzNzQ5MTUsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAyNjMwMzc0OTE2LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6N2NmZjA3MjUtOTA4NS00YmUwLWJmM2MtNjgzOWY4NjIxZjY5In0%3D; vtc=c_lPqL_NYDC6UXPeh9MF7U; _pxvid=5644c6ff-9b27-11ee-ac94-b6ae3dc3013d; pxcts=578a7ded-9b27-11ee-b1db-83fbf11618da; thx_guid=f069aa2ed76a293b146f2f45273ffc5e; QuantumMetricUserID=86dbb1aaa4c0846b6472227c8127c115; bm_mi=A9B761149346E34C2336993088CBA838~YAAQpCLHF0tB41iMAQAAT6lrexYSWryWW33ix0ebW0gEwR9R9Q3T0mju/pC7V7LqjuwHJrGseRkgFSZZiZVHV/zjTu4nCYLngf6698jYoiODJd20Ah32C8vCA7fNXyHUhtfSgpdKNkkOpAYXO8oYiiVKF1l5IHZeILy9fj41t1W9tF9DODYuxTNU8QPPnZr4w70z23lcK+3EBKd8IfqBgPT9kjLT3pKPY5P1EjKBk7Wvn9d44E3YtdQOdIlnmZMFqcniDHXEd0TgXXrvQ7NVo1BkEMaGynnbyeApl64EsIYaBB5nCsrN2NFOY0C1ZumHLPsxg5pg~1; ak_bmsc=28DD2AB37291D5EE54D348A14FFB7027~000000000000000000000000000000~YAAQpCLHF1FB41iMAQAA3atrexY3RQO5uSrgAn4o71OTeCY3Kb4oklPOhzO4ob+ZppkPcKbMuYwUEtBs6vEVlp9CoeIcig2wNR+YFPAMLag0FgMkBgPTNS7DAEU7mXMzqG7Zxes7pw+1iCYIJKVm0KAso3OE50SIiVGnpFKhhL7E2/cDwpWbUdrNVY30TGqqMk/uCOYcvkqNQE0wzVpTYkK2hWIeztHPT5fNqkdtRUylfN9FBTzfaUxix+emgkOw4S/zZxgkw4lNSo/YbiRZDpT409NuUxV4ht5j47adbQ1Q8F08pAvMkK3vGzOypF53bdu9Iw3g3D4PUZDOdQ2FFArxdUalCDwieOVrUbY5Xu55+30+fnRIFgzDM+aa72JMrHteO5326b1oz/QcUX1+EgWPmeSXJTk6/0EJvYS6yU9fnzsad5XVkCe4cRcS0AP/ZVfbCDorHjsVEachEwRjpKmMWx8q6DOuGFw00I1vRusHKdW1nrCknd+TjPY2u042+mVSAQCo0JoYGB8VZlNhvUof6KzcVkfB4ZG5; auth=MTAyOTYyMDE45uQqkShnPmrFyDklTStmr7wc0IXA0uhqKhIjidzMeOlor0FoZrStjvQEbiRm5ZqSmQEJ%2FPunCsfySE6LPKFrLFOCiFQAqcgu3n0iVe3t%2BsYE2L20v13oIDsvWtwF1M8S767wuZloTfhm7Wk2Kcjygi5k0VvBM%2FJjwcKWWhCnBS%2FsNVBmy9J1bR2VHO%2FdV8LIpQmp8XOq309QoW%2BZviaSOv8sqUVU54sFd4Bd6dus2ZEUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHVeQqO76rPmdncwFjpe%2BDY%2BOi7voHCqN3McdaIPwoybp22ykAEJaZ9rj2LqLTXsr9v3%2FEocn3Z%2BtLU09JmG1TrtCvNQVjLp%2Ffv%2BF%2BvOqp4HL4oel5ASvElr1Cex8QK9UZEjyrOXbKKhH072NS%2FW0j%2FU%3D; bstc=VDdH8jJ4m0c1jwDTa0PzoE; mobileweb=0; xptc=assortmentStoreId%2B3081; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=1mX_Y|7C2Eg|ERspM|HhvdQ|IS-p_|LPh6f|XbHrX|csP8O|gUDh7|oY0CV|qKfBf|yamTG; exp-ck=IS-p_1LPh6f1XbHrX1gUDh71qKfBf1; xpm=1%2B1702877712%2Bc_lPqL_NYDC6UXPeh9MF7U~%2B0; QuantumMetricSessionID=fb9f81cd396976e7c5f3318d82cf9aca; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1702878308583; xptwj=qq:1076adbafd040b695fce:7QGLRUcehov3qVy4qSYDvW4xgk6Uac2yX7HX1tE4u646HYe83hDIeMje3Thc/1gvh8z8MrbJNvYdlJL4OmUTikr/fII5bRfC3yxM6l2D8uZIkuA2LrEs6TpjeeKaJ8Uu78B/EOBkj1BzXZ7N6125enlMLSzG; xptwg=3146300320:19444130883AE60:3FBA1BE:1196AC61:D2F58FD2:22F02742:; _px3=6c9349297a823e7545b893fcfa8e6bea6eecb57dc1b11b7cf64522744342fc05:q5wbXoTzZgQ7QP9YgqzSywMmiYqoqw/ZrncImDY3CrTPeEkOqVzPo2zJ3AHIc+s/Ltf7IlS8awVgjpNXM98pxQ==:1000:cn/ohrnROd+TGJA88Xeo4/KYIo/UsLvu6vSoOqMnmEug9QezGcx/WE0rfSuHGKZjjjaYrqXq8pBrJ5nlgEeADLsTsoKQiFm1rc/m/6Q+wMMNleiOZAl9VZy3wpYUymfUCrFwGU+qqgFlXPCfQNN9o6vjljQksDltmcFQIof95UjvNs7UMihymHVA7zlP5BOXCURkIHiiJbM73dOLjmbFMFnxGHKXULDB8u2cb1oECR0=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1702878315000@firstcreate:1702630374882\"; TS01a90220=014e9abc5b76ea37289d24aa7bf6872327f3a801259b614de94e2e0e90c1b0f94c88be0edeb3c58cf3cba5b3568b2b79f66cc4dc9c; bm_sv=ADF605AFD1AA1E340A3C57CE2E012D0E~YAAQJPN0aPSze2mMAQAAbQJ1exaqQAmPQEugdiG3MQAUrup+aS98eOHJhIKWYEY4cQ16kvxr5PjD68+TG/q08uWKgkntuhl4BsnOGqcEXY7GjWBoB0neW+58ysLmfhee1rCKsVjS3iaBCoFZwcqIgkO3eDhiRFyKZXfNTkKDX7JBTyy63S1DAg2SrohZrD0KlpE5E2rFPlHxbpWohjTWhvXGlDIHA0lk8uQB+NvqN5R8yDL6bMxSm0YrJ6/p6h5kxvQ=~1; _pxde=3cb689d88ebd75dfd5e5c64a8389623aecf6bac16e69f216cf01a4e825c18d52:eyJ0aW1lc3RhbXAiOjE3MDI4NzgzMTUzNDl9',\n",
    "    'ACID=6ff15283-acba-4fe2-89c5-ac59a9b887d1; hasACID=true; thx_guid=0c376c1283b8b14be6de36c7c9897b80; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1696941267921; _m=9; vtc=WYxqCClwKT085jYR_DbfQI; _pxvid=5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; auth=MTAyOTYyMDE4IIJ07VmsLMduZmu3wWCW8eIqM67RZKVCKQnP%2BJhyprb8mcLFoQ86xyheUk7V1wBI53SlNalpIeiZEAnwibnwBxAuKnHyan446S83cruRp5IqJEEhjLzfZ9dTkdF%2F5mvs767wuZloTfhm7Wk2Kcjygt6CFmh5hT8BoAhiLFQG8TM4tK7YyL%2Bjr93Ekvm3gtoWXd1A1TJkrpfzbS%2B%2BXZ2ssMBCHcdxxw3SP0Sy3y18bhsUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHfClvOkjwxW8L0euuWDrN9AOTwJZ5k6XdH2IzYdedb%2BeKcBnww%2BqCeKjSX3bV3tRPjMVnRfkQSZ38Y3kHRhf5YWMEw3bsV%2BTWUdfiZ61nY1rm2KT4Gr0iVCCeIJhV8GhuUjyrOXbKKhH072NS%2FW0j%2FU%3D; locDataV3=eyJpc0RlZmF1bHRlZCI6dHJ1ZSwiaXNFeHBsaWNpdCI6ZmFsc2UsImludGVudCI6IlNISVBQSU5HIiwicGlja3VwIjpbeyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJodWJOb2RlSWQiOiIzMDgxIiwic3RvcmVIcnMiOiIwNjowMC0yMzowMCIsInN1cHBvcnRlZEFjY2Vzc1R5cGVzIjpbIlBJQ0tVUF9JTlNUT1JFIiwiUElDS1VQX0NVUkJTSURFIl0sInNlbGVjdGlvblR5cGUiOiJMU19TRUxFQ1RFRCJ9XSwic2hpcHBpbmdBZGRyZXNzIjp7ImxhdGl0dWRlIjozOC40NzQ1LCJsb25naXR1ZGUiOi0xMjEuMzQzOCwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeUNvZGUiOiJVU0EiLCJnaWZ0QWRkcmVzcyI6ZmFsc2UsInRpbWVab25lIjoiQW1lcmljYS9Mb3NfQW5nZWxlcyJ9LCJhc3NvcnRtZW50Ijp7Im5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJpbnRlbnQiOiJQSUNLVVAifSwiaW5zdG9yZSI6ZmFsc2UsImRlbGl2ZXJ5Ijp7ImJ1SWQiOiIwIiwibm9kZUlkIjoiMzA4MSIsImRpc3BsYXlOYW1lIjoiU2FjcmFtZW50byBTdXBlcmNlbnRlciIsIm5vZGVUeXBlIjoiU1RPUkUiLCJhZGRyZXNzIjp7InBvc3RhbENvZGUiOiI5NTgyOSIsImFkZHJlc3NMaW5lMSI6Ijg5MTUgR2VyYmVyIFJvYWQiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJjb3VudHJ5IjoiVVMiLCJwb3N0YWxDb2RlOSI6Ijk1ODI5LTAwMDAifSwiZ2VvUG9pbnQiOnsibGF0aXR1ZGUiOjM4LjQ4MjY3NywibG9uZ2l0dWRlIjotMTIxLjM2OTAyNn0sImlzR2xhc3NFbmFibGVkIjp0cnVlLCJzY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJ1blNjaGVkdWxlZEVuYWJsZWQiOnRydWUsImFjY2Vzc1BvaW50cyI6W3siYWNjZXNzVHlwZSI6IkRFTElWRVJZX0FERFJFU1MifV0sImh1Yk5vZGVJZCI6IjMwODEiLCJpc0V4cHJlc3NEZWxpdmVyeU9ubHkiOmZhbHNlLCJzdXBwb3J0ZWRBY2Nlc3NUeXBlcyI6WyJERUxJVkVSWV9BRERSRVNTIl0sInNlbGVjdGlvblR5cGUiOiJMU19TRUxFQ1RFRCJ9LCJyZWZyZXNoQXQiOjE2OTg0MDA0ODkwNzgsInZhbGlkYXRlS2V5IjoicHJvZDp2Mjo2ZmYxNTI4My1hY2JhLTRmZTItODljNS1hYzU5YTliODg3ZDEifQ%3D%3D; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIn0sInNoaXBwaW5nQWRkcmVzcyI6eyJ0aW1lc3RhbXAiOjE2ODU3ODc2OTkwNzksInR5cGUiOiJwYXJ0aWFsLWxvY2F0aW9uIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJwb3N0YWxDb2RlIjoiOTU4MjkiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJkZWxpdmVyeVN0b3JlTGlzdCI6W3sibm9kZUlkIjoiMzA4MSIsInR5cGUiOiJERUxJVkVSWSIsInRpbWVzdGFtcCI6MTY5ODM5Njg4OTA3Miwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIiwic2VsZWN0aW9uU291cmNlIjpudWxsfV19LCJwb3N0YWxDb2RlIjp7InRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwiYmFzZSI6Ijk1ODI5In0sIm1wIjpbXSwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOjZmZjE1MjgzLWFjYmEtNGZlMi04OWM1LWFjNTlhOWI4ODdkMSJ9; bstc=RDjQg_AX5lxPJuFgq3Nu74; mobileweb=0; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=92YHy|YnYws|yUqGy; exp-ck=YnYws4; pxcts=7ca9260b-74a6-11ee-b9ba-cadc77ff3193; xptc=assortmentStoreId%2B3081; xpm=1%2B1698396890%2BWYxqCClwKT085jYR_DbfQI~%2B0; TS01a90220=016ea84bd28377d28f6c5f8c825a73acff43907723c0c62c3b0f4412526365096a8d6e578fadcc7b7c5313bbb50b7e1d6a09790715; xptwj=qq:8c955083c3f2d971e73b:/AUbzj+G4qHiFzoBmTqw7sTxNCjaAsdlFcEBElDEMhfux6fBocZ2XuU9J43MZqv9xFdQR6jCZn1NHMlGARop+dGCrSwqG/FMfkjiqrzMhk6qDRXnGwdXkmVMkY17cQBnEuy2yF31oDbLB2VcFQm/6GzBoQa2xx4u4jyJ; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1698397083000@firstcreate:1696941267921\"; xptwg=2654488666:13D200536871A40:321F50E:2AE8C8EB:145FE951:E3F58838:; _px3=2259e2ff82c009ea11f3276f67eeec8c7933f2015f7c4f4f537a810e135667b2:WTyoBm43yjHnl6DaKRhXrBuw1dVxFeMmNao4s1nyzyzay9AT/JNRr2njauNA3Q05CZCkyHcGpiOlACvkOPaeLA==:1000:eqWykI7N/3Nxz6qb3xQH+stpzInFVztcX104+VKDHUoglkWCA2mLjEu+Zknx3FqHY6MZskTdWOhU7b/cxkcRsZzn2v7xd2d0SIFOVaBuJFFm4ddlo4ejUXkO/Ta7SH2GcvS0zq5pIVgCSlg9SjcmMala24rEipeabfBpgSjY1DFP4u3vQ5vVD9nlh6dHTGDAN6J86YkunMWOWKq/mltB5LLWDt5U+hsHxwlUQFOl53E=; _pxde=67376920231ea198f43cb2a2fd4e3570e6f0c4527d7e9406b6848b481178e72e:eyJ0aW1lc3RhbXAiOjE2OTgzOTcwODU3NDh9',\n",
    "    'ACID=6ff15283-acba-4fe2-89c5-ac59a9b887d1; hasACID=true; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1696941267921; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIn0sInNoaXBwaW5nQWRkcmVzcyI6eyJ0aW1lc3RhbXAiOjE2ODU3ODc2OTkwNzksInR5cGUiOiJwYXJ0aWFsLWxvY2F0aW9uIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJwb3N0YWxDb2RlIjoiOTU4MjkiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJkZWxpdmVyeVN0b3JlTGlzdCI6W3sibm9kZUlkIjoiMzA4MSIsInR5cGUiOiJERUxJVkVSWSIsInRpbWVzdGFtcCI6MTY5Njk0MTI2ODAwMSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIiwic2VsZWN0aW9uU291cmNlIjpudWxsfV19LCJwb3N0YWxDb2RlIjp7InRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwiYmFzZSI6Ijk1ODI5In0sIm1wIjpbXSwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOjZmZjE1MjgzLWFjYmEtNGZlMi04OWM1LWFjNTlhOWI4ODdkMSJ9; userAppVersion=us-web-1.102.0-0f3d752097f13fd03499487f7cfc0f9ff879d809-1005; abqme=true; vtc=WYxqCClwKT085jYR_DbfQI; _pxhd=5a7ffd639284c9b62b5b6953d2b6554b5e4fb23e72bdea13cb0d60c5e9cb2592:5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; TBV=7; _pxvid=5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; pxcts=5b1312c1-6769-11ee-9d4b-928400606778; xptwj=qq:19ae55e85ed74ecb934a:FzwixoJNbjsJTKIVOxs2Y3BCAjYnbpEJ9QAEPF+vcgu7rou9eHViyjDPVj+jQqEQsDVe8eLUcM9yr4bzIXF5/EpE+3GBy+nQfjIux03VKMmH4uP0zvUVBAnki5gXoud346PderEXI4ZdwzI5dEw9RZpxrSE=; _astc=dd455cd93be2a8805fa78a0c5637c0bc; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1696943665000@firstcreate:1696941267921\"; xptwg=3827596973:8C0849C3A8C7E8:1626C14:C8D91831:9FD2BC9F:ED33D50A:; TS012768cf=0178545c900bf5c440f69b21bbdea7b97f1bb93829c83cdf12d8a829eaa1f335330ed161b9a95404539366655b7d1af2acfeaa823d; TS01a90220=0178545c900bf5c440f69b21bbdea7b97f1bb93829c83cdf12d8a829eaa1f335330ed161b9a95404539366655b7d1af2acfeaa823d; TS2a5e0c5c027=0881c5dd0aab20006278b8f1c282bae24adaf3370d4de18765bc5b96e8045bf269a5218310a2b5f308691a3aad113000c4a62f799fcd6ee383055c96bd53eb55adecf63f1102cc5dd537a8a8e81ee1756147c51df92a6c584fb9a07c447c0309', \n",
    "    'ACID=13f858d3-9165-43cb-bab4-a63c55e6a6a8; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMjk1NTMwNTIxNSwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAyOTU1MzA1MjE1LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDI5NTUzMDUyMTQsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAyOTU1MzA1MjE1LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6MTNmODU4ZDMtOTE2NS00M2NiLWJhYjQtYTYzYzU1ZTZhNmE4In0%3D; vtc=bqXm-Yjh0fiyfC30O9pl40; pxcts=e0a91fc6-9e1b-11ee-b6a5-16e7f3c0c35f; _pxvid=dfcedab4-9e1b-11ee-852d-cc6ba9195b8b; thx_guid=d45cea97b66ab1e12333b05cf300756a; auth=MTAyOTYyMDE4eh0UBw4CZsMoITCUpLy%2FJSKZAhO1G27GRMvlySSJu45p1%2FLh4CYkzkJMDMayVQPkr4cn%2Flzfu3Cnm2UosjL24mxfJqc48PMpEXdaud0aTiSYybEajj235v6w6v39wTDg767wuZloTfhm7Wk2KcjygobRHThsmZk%2BGcqTfIab85Qi91RLvjJ4oWxX7pdsgCM7kaNdhw7fWS2J7XYV98BtLp94fDX6wtiILXdT4QaPibQUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHYAmxWm2QCSM81oB%2BzgtGh7GRphnRVqhmKz4T4aeRpfPdIrk6V7SOwO2Q2sHD6RhS27h8BprVsmSYkJBi2ZANdkS%2BmkqvUibwJ%2ByNBdR4lDAXTGRW5wSkfxBkI28si3Kp0jyrOXbKKhH072NS%2FW0j%2FU%3D; bstc=eiIdhbpg62SIN7bXqMg3z8; mobileweb=0; xptc=assortmentStoreId%2B3081; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=1mX_Y|M65NP|WuHSe|XmyU7|j21L4|oY0CV|qKfBf|qMQpD|r97uO; exp-ck=WuHSe1XmyU71j21L41qKfBf1r97uO1; xpm=1%2B1702962355%2BbqXm-Yjh0fiyfC30O9pl40~%2B0; bm_mi=549FCFD96C94678551FEF51A3881B212~YAAQX/N0aH0UHFmMAQAAD7F9gBYxuYoIp+e5g1nqvEyexI6hSo7fx80xYZvYxOzX42gMoKUiODVyjrfCTZxzUPZoY9b4p074yFBo9qtr3dvqcrWK+Ax3QnilXHYN/O3i9uGgMgaYzDbvFsofcB+UXepCydTAGXEwlgTjcuhCK/WHSSkwgTYZBCgoH4ZvNuWxbDqSZJVHpneNw6xdnWnxgDwCiwgTl9vrLyjdg09e4T2smzNssFE245sbVMdbiBsLplNv/n7syfFu/VVXuZyCtMvNaORC85FiXoDfVzz0LARwtlqQLr1WRjGOEklBkKaf65Mxcoy8~1; ak_bmsc=2EB05928C59E3D09719A985EA17D031E~000000000000000000000000000000~YAAQX/N0aIcUHFmMAQAAg7N9gBbfI4TKoREfgSIqsmk7VQxNHd1hZRptwqQOz86rLpb4Dm8hwXmGCtpOkNohVRQHY/oG6ctA+y0cssVoASKpTSd1fEYyHDqbC+DrYiN6LJ0q930S+F4z3j6ktqalEJm6Bh4BtsGU6d5h43XeXgGRDzTLgZf+D5taWBpWzCUAb/xXmehSq77svJniJ4HmGoQYRn4fH8UeuB958xg08dR7TF3qVNpEUpqlv1WV4x9fcXDK35s9YBcHcZJUpGXMIx0+3rA/JVyuVqSLJnTzu9yOhVYgISi07np3590KzU5WW1U9oTYdQVYyvkMD3EL1LbLzXiZ10P9fl+HnFHX0V52zIFkboWpvSkdK4HVlAR/7LrWEM6PaexEzR6dCESt+Az+opps+z9JZNT6Y9W2aalGBPdzj275+3AznMTCf3hNispVbb4Im9fKO4HjsyY/rQKVgXwkJ6+IVYPDvYRFJaS1jY42zuoQknNidJ0ajbhHEwI5WkMGbsQ6ZfxAYYFUHPOj4ZWUDO1N9gnSx; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1702963573470; xptwj=qq:60c655ed12ee68048e72:54KBHIYpe8ioxt3yuo3GuRpC1WCSieQoP/HSZ4zw05vUNBqTEGjZylRV7Ee/gQKdw+4AR39Pu3s9w8UN6fC+xQaysEu/16eRafxBSlOhJPWWTVuI1BF3b300k0De/jcPtI82jkh9uGX2O4ufX9jSWMwn87b+BBU=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1702963574000@firstcreate:1702955305178\"; xptwg=4227679326:558030DE400990:D7A337:D1486F6E:AEABFA5B:A48C033E:; TS01a90220=0178545c90c924ff084263c8cb94d8a93feb10ba5dcdaafa44d2d4bbee53181a33b55ab3c28c48628f7c56388b13afbe855a015aeb; bm_sv=60E1D169192E935CC6C00B0CDE127AEB~YAAQniLHF0lo0UKMAQAAEPaJgBa2H14Xw8z9j5FuLe+c+mOr9vtjWy+B1JIWMPOzFV3jf2sOwlAcIEVT7Zb0mGwx9GetlyN6e/PNbuNPGRK0tdoA/hf8gjCduTwLS8CC6z4no3eEnvY1ag+q3Dq0y4dikxDbYXvcfVujjYrVNac/v5cOfWcW40fMWhkJQX7hGxnZqmIyc+52KM8NeP9eIqEdybm44398ucnnT/mCybgp/Bn9xYLZwQBJSE7edA60x3A=~1; _px3=6e98f47b3f022f412f75d2fff334dd78b42e1c2da9dbb1ac36bcae228b1359cb:Vj9ElCDReT1PHAkl34AbO/7bhYH1iIhL+fexBxQwF/yJALJP177TUZi0QsuawJtQWCRpPtcCQy7FPqF2SO+Gpw==:1000:x43ZNK+0w/plDdsG3NfS1bezpRm0D1++lXrQuSJabg6/xCZEc7Ghp02iHShQZQWtATaAUPAyVYeZC2R8lPF2X7xPgH9OSstdEZ2g17OOrJ4Fv7E1MK6KZq0DiVn7FxyrcRAXcBHekyDMGiK6TTAugoF3QVB4LAQ7WM6kpnEbXlQLvndZW/IDW1JNGgVGCJvdRpV+VpqD6Ab7P2yv3UX4n+cRwoKWIa99iFjoZbmOuYM=; _pxde=3810374111437fdd342c82c0dbe956b1fb4871c6ade129e930a96584572ba38e:eyJ0aW1lc3RhbXAiOjE3MDI5NjM1NzUwNDJ9'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69411a-0122-4f09-9fa2-fdbe6fdca4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_number(url, cookie):\n",
    "    User = random.choice(User_Agent)\n",
    "    header = {\n",
    "        'User-Agent': User,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': random.choice(cookie),  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page_number_elements = soup.find_all(lambda tag: tag.name == 'a' and 'page-number' in tag.get('data-automation-id', ''))\n",
    "    page_numbers = [int(element.text) for element in page_number_elements]\n",
    "    if page_numbers:\n",
    "            last_page_number = max(page_numbers)\n",
    "            return last_page_number\n",
    "    elif \"Robot or human\" in response.text:\n",
    "        print(User)\n",
    "        print('Robot or human page')\n",
    "    else:\n",
    "        print(False)\n",
    "        \n",
    "\n",
    "def get_review_walmart(url, cookie):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "         'User-Agent': random.choice(User_Agent), \n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        # 'Cookie': random.choice(cookie),\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "        title_all = soup.find('a',class_ = 'w_x7ug f6 dark-gray')\n",
    "        if title_all:\n",
    "            title = title_all.get('href')\n",
    "            pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "            model = re.findall(pattern, title)\n",
    "#         model = re.search(r'\\b(\\d{4}e)\\b', title_all).group(1)\n",
    "        li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "        \n",
    "        if li_elements:\n",
    "            for li_tag in li_elements:\n",
    "                product = {}  # Create a new 'product' dictionary for each review\n",
    "                product['Model'] = title\n",
    "                product['Review rating'] = li_tag.select_one('.w_iUH7').text\n",
    "                product['Verified Purchase or not'] = li_tag.select_one('.pl2.green.b.f7.self-center').text if li_tag.select_one('.pl2.green.b.f7.self-center') else None\n",
    "                product['Review date'] = li_tag.select_one('.f7.gray').text if li_tag.select_one('.f7.gray') else None\n",
    "\n",
    "                review_title_element = li_tag.select_one('h3.b')\n",
    "                product['Review title'] = review_title_element.text if review_title_element else None\n",
    "\n",
    "                product['Review Content'] = li_tag.find('span', class_='tl-m mb3 db-m').text if li_tag.find('span', class_='tl-m mb3 db-m') else None\n",
    "                product['Review name'] = li_tag.select_one('.f6.gray').text if li_tag.select_one('.f6.gray') else None\n",
    "\n",
    "                syndication_element = li_tag.select_one('.b.ph1.dark.gray')\n",
    "                product['Syndicated source'] = syndication_element.text if syndication_element else None\n",
    "                product['URL'] = url\n",
    "\n",
    "                extracted_reviews.append(product)\n",
    "        elif \"Robot or human\" in response.text:\n",
    "            print('Robot or human page')\n",
    "            \n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error encountered: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    return extracted_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d58e0-b821-47df-bc50-44a958aaa0e1",
   "metadata": {},
   "source": [
    "## HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/651507182?sort=submission-desc',\n",
    "    'https://www.walmart.com/reviews/product/716614887?sort=submission-desc'\n",
    "] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c70e0-b139-49f6-9b15-5698180c6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "walmart_reviews = []\n",
    "# urls = [\n",
    "#     'https://www.walmart.com/reviews/product/651507182?sort=submission-desc',\n",
    "#     'https://www.walmart.com/reviews/product/716614887?sort=submission-desc'\n",
    "# ]\n",
    "for link in urls:\n",
    "    retry_count = 0\n",
    "    max_try = 20\n",
    "    print(link)\n",
    "    while True:\n",
    "        try:\n",
    "            last_page_number = get_page_number(link, cookie)\n",
    "            if last_page_number is None:\n",
    "                print(\"Failed to retrieve last page number. retry...\")\n",
    "                time.sleep(3)\n",
    "                continue  \n",
    "            print('Total pages:', last_page_number)\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "\n",
    "\n",
    "    for page_number in range(1, last_page_number + 1):\n",
    "        while retry_count < max_try:\n",
    "            try:\n",
    "                target_url = f'{link}?page={page_number}'\n",
    "                extracted_reviews = get_review_walmart(target_url, cookie)\n",
    "\n",
    "                if len(extracted_reviews) == 0:\n",
    "                    print('No reviews found. Retrying in 5 seconds...')\n",
    "                    retry_count += 1\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    walmart_reviews.extend(extracted_reviews)\n",
    "                    print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                    time.sleep(2)\n",
    "                    break  \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                retry_count += 1\n",
    "                time.sleep(3)\n",
    "        else:\n",
    "            break \n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381c50a-2008-44d0-bda4-df92eee638a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'data_new'\n",
    "df_walmart = pd.read_excel(path, sheet_name = sheets)\n",
    "df_walmart['Comp Model number'] =  df_walmart['Comp Model number'].astype(str)\n",
    "df_walmart['HP Model Number'] =  df_walmart['HP Model Number'].astype(str)\n",
    "df_walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b60ed-9112-432b-ae2f-47c4125beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart= pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer']=\"walmart\"\n",
    "\n",
    "from datetime import date \n",
    "walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "walmart.drop_duplicates(inplace = True)\n",
    "\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "\n",
    "walmart_hp_combine = pd.merge(walmart, df_walmart, on = \"HP Model Number\", how = \"left\" )\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "walmart_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating':'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title':'Review_Title',\n",
    "    'Review Content':'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "walmart_hp_combine \n",
    "\n",
    "# walmart_hp_combine.to_excel('walmart_hp_combine.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547792ed-9913-4647-8d2f-888c41fa331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_review = pd.concat([review_template, walmart_hp_combine], ignore_index = True)\n",
    "len(Final_review.columns)\n",
    "\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Response_Date'] = pd.to_datetime(Final_review['Response_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "Final_review['Syndicated_Source'].fillna('', inplace=True)\n",
    "\n",
    "Final_review.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2509d52-a0e3-4d58-a2b9-698d653f673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = Final_review.select_dtypes(include = 'object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2380736-4532-4e10-9c7c-18a121992aa8",
   "metadata": {},
   "source": [
    "## Combine with previous and save to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d72264-2625-4dc7-8c51-99b17262e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query previous review\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# dataframe = amazon_final_df\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    "\n",
    "\n",
    "existing_rows_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {schema}.{table}\n",
    "    WHERE Retailer in ('Walmart')\n",
    "\"\"\"\n",
    "result_df = pd.read_sql_query(existing_rows_query, engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f172fb0-b045-4167-9bf0-36c5d4b22a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Review_Date'] = pd.to_datetime(result_df['Review_Date']).dt.date\n",
    "\n",
    "# non_duplicated_df = Final_review[\n",
    "#                                 (~Final_review['Review_Name'].isin(result_df['Review_Name']))].drop_duplicates()\n",
    "\n",
    "result_df = result_df[['Review_Model', 'Retailer', 'Review_Name', 'Review_Rating', 'Review_Title', 'Review_Content', 'Review_Date']]\n",
    "\n",
    "# Merge the DataFrames on both 'Review_Name' and 'Review_Content'\n",
    "merged_df = pd.merge(Final_review, result_df, on=['Review_Model','Retailer','Review_Name', 'Review_Rating','Review_Title','Review_Content','Review_Date'], how='left', indicator=True)\n",
    "\n",
    "# Filter the rows where the indicator column is 'left_only'\n",
    "non_duplicated_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Drop the indicator column\n",
    "non_duplicated_df = non_duplicated_df.drop('_merge', axis=1)\n",
    "\n",
    "# Drop duplicates based on 'Review_Name' and 'Review_Content'\n",
    "non_duplicated_df = non_duplicated_df.drop_duplicates(subset=['Review_Model','Retailer','Review_Name', 'Review_Rating','Review_Title','Review_Content','Review_Date'])\n",
    "\n",
    "\n",
    "non_duplicated_df = non_duplicated_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "non_duplicated_df.sort_values(by = ['Review_Model','Review_Content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d5b93-a28f-4a8d-9d71-73d52c64a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    " \n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "dataframe = non_duplicated_df\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "chunk_size = 10000\n",
    "total_rows = len(dataframe)\n",
    "num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(num_chunk):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = (i + 1) * chunk_size\n",
    "    chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "    chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "    print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b0f73-1a88-4714-ab43-16ae05113efd",
   "metadata": {},
   "source": [
    "# HHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40dd17b-08b8-4533-9236-0aac2c6d4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx'\n",
    "sheet = 'HHO'\n",
    "hho = pd.read_excel(path, sheet_name=sheet)\n",
    "skus = hho['SKU'].to_list()\n",
    "len(skus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba5d39-ba4f-467c-a1b0-de9867aaa3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hho_rating(sku, df = None):\n",
    "    api_url = 'https://api.bazaarvoice.com/data/products.json'\n",
    "    params = {\n",
    "        'resource': 'products',\n",
    "        'filter': f'id:eq:{sku}',\n",
    "        # 'filter_reviews': 'contentlocale:eq:en_US,en_CA',\n",
    "        # 'filter_reviewcomments': 'contentlocale:eq:en_US,en_CA',\n",
    "        'filteredstats': 'Reviews',\n",
    "        'stats': 'Reviews,questions,answers',\n",
    "        'passkey': 'caBZoE5X0dmsywGCMoQmo6OPymWLQFnY37VernuC3SGkY',\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '8843-en_us'\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "    product_info = data['Results'][0]\n",
    "    rating_info = product_info['ReviewStatistics']\n",
    "\n",
    "    if df is None:\n",
    "        df = pd.DataFrame()\n",
    "    for rating in rating_info['RatingDistribution']:\n",
    "            df = df.append(\n",
    "                {\n",
    "                 'OriginalProductName': product_info['Name'],\n",
    "                 'URL': product_info['ProductPageUrl'],\n",
    "                   'SKU':sku},\n",
    "             \n",
    "                ignore_index=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88ae2f-8be4-448e-955a-f19e21db43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hho_review(sku):\n",
    "    \n",
    "    api_url = 'https://api.bazaarvoice.com/data/reviews.json'\n",
    "\n",
    "    params = {\n",
    "        'resource': 'reviews',\n",
    "        'action': 'REVIEWS_N_STATS',\n",
    "        'filter': f'productid:eq:{sku}',\n",
    "        'include': 'authors,products,comments',\n",
    "        'limit': 100,\n",
    "        'offset': 0,\n",
    "        'sort': 'submissiontime:desc',\n",
    "        'passkey': 'caBZoE5X0dmsywGCMoQmo6OPymWLQFnY37VernuC3SGkY',\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '8843-en_us'\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "\n",
    "    limit = 100\n",
    "    no_batch = math.ceil(data['TotalResults']/limit)\n",
    "    print('Total review',data['TotalResults'])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for x in range(0,no_batch):\n",
    "        offset = x*limit\n",
    "#         print('Offset',offset)\n",
    "        params = {\n",
    "        'resource': 'reviews',\n",
    "        'action': 'REVIEWS_N_STATS',\n",
    "        'filter': f'productid:eq:{sku}',\n",
    "        'include': 'authors,products,comments',\n",
    "        'limit': 100,\n",
    "        'offset': offset,\n",
    "        'sort': 'submissiontime:desc',\n",
    "        'passkey': 'caBZoE5X0dmsywGCMoQmo6OPymWLQFnY37VernuC3SGkY',\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '8843-en_us'\n",
    "        }\n",
    "\n",
    "        response = requests.get(api_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "        else:\n",
    "            print('status code != 200')\n",
    "\n",
    "        results_data = data['Results']\n",
    "        df_temp = pd.DataFrame(results_data)\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "        time.sleep(3)\n",
    "\n",
    "    df['SyndicationSource_Name'] = df.apply(lambda row: row['SyndicationSource'].get('Name') if row['IsSyndicated'] else None, axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ec00c-7aea-4df1-b49c-9f1bb03123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "result_rating = pd.DataFrame()\n",
    "for sku in skus:\n",
    "    print(sku)\n",
    "    result_rating = hho_rating(sku, result_rating)\n",
    "result_rating.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e269d5-c21f-46c9-aa1f-483d0e1ca487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "result_df = pd.DataFrame()\n",
    "for sku in skus:\n",
    "    print('Get review', sku)\n",
    "    data = hho_review(sku)\n",
    "    print('Review count',len(data))\n",
    "    if data is not None:\n",
    "        result_df = pd.concat([result_df,data],axis = 0)\n",
    "        print('All Review count',len(result_df))\n",
    "result_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260391d0-bb23-4b70-a5b4-f631d750cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_df.copy()\n",
    "result = result[result['ContentLocale'].isin(['en_US','en_CA'])]\n",
    "\n",
    "result['Verified_Purchase_Flag'] = result['BadgesOrder'].apply(lambda x: 'Verified Purchaser' if\n",
    "    isinstance(x, list) and len(x) > 0 and 'verifiedPurchaser' in x else \" \")\n",
    "\n",
    "result['Seeding_Flag'] = result['BadgesOrder'].apply(lambda x: 'Seeding' if\n",
    "    isinstance(x, list) and len(x) > 0 and 'incentivizedReview' in x else \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eedad3-7bbc-4c3b-88bb-b38177227be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for maching HP model and HP Class\n",
    "path = r\"C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'data_new'\n",
    "data_new = pd.read_excel(path, sheet_name = sheets)\n",
    "# data_new['HP Model Number'] =  data_new['HP Model Number'].astype(str)\n",
    "data_new\n",
    "# df_amazon.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34ffb6-5ebe-48d1-b208-b4c93c59942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['HP Model Number'] = result['OriginalProductName'].str.extract(r'(\\d+e?)')\n",
    "result_combine = pd.merge(result, data_new, on = \"HP Model Number\", how = \"left\" )\n",
    "\n",
    "from datetime import date  \n",
    "result_combine['Retailer']=\"HHO\"\n",
    "result_combine['Scraping_Date'] =pd.to_datetime(date.today())\n",
    "result_combine['Comp_Model'] = ''\n",
    "result_combine.rename(columns = {'HP_Model':'Review_Model'},inplace = True)\n",
    "result_combine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45272e-635c-4154-a40d-517debc2fbd5",
   "metadata": {},
   "source": [
    "## Save to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138916f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_combine['Competitor_Flag'] = result_combine['Review_Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "result_combine.rename(columns = {'LastModeratedTime':'Review_Date'},inplace = True)\n",
    "result_combine['Review_Date'] = pd.to_datetime(result_combine['Review_Date']).dt.date\n",
    "\n",
    "result_combine_url = pd.merge(result_combine, result_rating, on = \"SKU\", how = \"left\" )\n",
    "\n",
    "column_mapping = {\n",
    "    'UserNickname': 'Review_Name',\n",
    "      'Rating': 'Review_Rating',\n",
    "    'Title':'Review_Title',\n",
    "    'ReviewText':'Review_Content',\n",
    "    'SyndicationSource_Name':'Syndicated_Source',\n",
    "    'TotalPositiveFeedbackCount': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "result_combine_url = result_combine_url.rename(columns=column_mapping)\n",
    "result_combine_version = result_combine_url[['Review_Date','Review_Rating', \n",
    "       'People_Find_Helpful',  'Review_Content', 'Review_Title', 'Review_Name',\n",
    "        'Syndicated_Source','Verified_Purchase_Flag', 'Seeding_Flag', 'URL', \n",
    "       'Review_Model', 'Comp_Model', 'HP_Class',\n",
    "       'Segment', 'Retailer', 'Scraping_Date', 'Competitor_Flag']]\n",
    "\n",
    "# result_combine_version.drop_duplicates(inplace = True)\n",
    "\n",
    "Final_review = pd.concat([review_template, result_combine_version], ignore_index = True)\n",
    "\n",
    "Final_review['Country'] = 'US'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "string_columns = Final_review.select_dtypes(include = 'object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "Final_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af531925-e362-4c10-b237-7d940ac848c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    " \n",
    "server = 'DB-Cluster3.ijp.sgp.rd.hpicorp.net'\n",
    "database = 'STAR_Rating'\n",
    "schema = 'dbo'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "dataframe = Final_review\n",
    "table = \"Ink_web_reviews\"\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "chunk_size = 10000\n",
    "total_rows = len(dataframe)\n",
    "num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(num_chunk):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = (i + 1) * chunk_size\n",
    "    chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "    chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "    print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
