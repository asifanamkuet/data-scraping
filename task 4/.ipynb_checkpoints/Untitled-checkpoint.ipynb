{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e733e5-bbb1-4c54-becd-08e9830b6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesing\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    print('Tesing')\n",
    "    var1 = 10\n",
    "    var2 = 100\n",
    "\n",
    "    def sum(one, two):\n",
    "        summ = one + two\n",
    "        return summ\n",
    "        \n",
    "    summm = sum(var1,  var2)\n",
    "    print(summm)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7137c-a507-4237-bbc2-9a66c566c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Timestamp: 2024-09-08 00:25:35\n",
      "Running Tassel_raw_date_scraping.py\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=one_star&sortBy=recent\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=two_star&sortBy=recent\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=three_star&sortBy=recent\n",
      "Page: 1 three star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=four_star&sortBy=recent\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=five_star&sortBy=recent\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=one_star&sortBy=recent\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=two_star&sortBy=recent\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=three_star&sortBy=recent\n",
      "Page: 1 three star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=four_star&sortBy=recent\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=five_star&sortBy=recent\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=one_star&sortBy=recent\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=two_star&sortBy=recent\n",
      "Page: 1 two star\n",
      "Page 1 scraped 1 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=three_star&sortBy=recent\n",
      "Page: 1 three star\n",
      "Page 1 scraped 3 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 has no reviews, moving to the next page\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=four_star&sortBy=recent\n",
      "Page: 1 four star\n",
      "Page 1 scraped 4 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 has no reviews, retry\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## Define Logging function start\n",
    "def log_error_to_csv(error_type, error_message, error_traceback):\n",
    "    import logging\n",
    "    import csv\n",
    "    import traceback\n",
    "    with open('errors.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([error_type, error_message, error_traceback])\n",
    "## Define Logging function end\n",
    "\n",
    "## Make the full program into a function start\n",
    "def program():\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import date \n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import time \n",
    "    from requests_html import HTMLSession\n",
    "    import json\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import math\n",
    "    from datetime import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import openpyxl\n",
    "    from deep_translator import GoogleTranslator\n",
    "\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # Format the timestamp as a string\n",
    "    timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Print the timestamp\n",
    "    print(\"Current Timestamp:\", timestamp)\n",
    "    \n",
    "    print('Running Tassel_raw_date_scraping.py')\n",
    "    \n",
    "    # %%\n",
    "    excel_file_path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "    sheet_name = \"data_new\"\n",
    "    \n",
    "    # Read the Excel sheet into a DataFrame\n",
    "    df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "    df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "    df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "    df_amazon\n",
    "    \n",
    "    # %%\n",
    "    path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "    sheets = 'review_template'\n",
    "    review_template = pd.read_excel(path, sheet_name = sheets, engine='openpyxl')\n",
    "    review_template\n",
    "    \n",
    "    \n",
    "    # %% [markdown]\n",
    "    # # Amazon\n",
    "    \n",
    "    # %% [markdown]\n",
    "    # ## Function\n",
    "    \n",
    "    # %%\n",
    "    ### Not use docker\n",
    "    \n",
    "    # header = {\n",
    "    #         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "    #         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    #         'Accept-Encoding': 'gzip, deflate, br',\n",
    "    #         'Accept-Language': 'en-US,en;q=0.9',\n",
    "    #         'Cache-Control': 'max-age=0', \n",
    "    #         'Downlink': '10',\n",
    "    #         'Dpr': '1',\n",
    "    #         'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "    #         'Sec-Ch-Ua-Mobile': '?0',\n",
    "    #         'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "    #         'Sec-Fetch-Dest': 'document',\n",
    "    #         'Sec-Fetch-Mode': 'navigate',\n",
    "    #         'Sec-Fetch-Site': 'same-origin',\n",
    "    #         'Sec-Fetch-User': '?1',\n",
    "    #         'Upgrade-Insecure-Requests': '1'\n",
    "    #     }\n",
    "    \n",
    "    # url ='https://www.amazon.com/HP-DeskJet-2755e-Wireless-Printer/product-reviews/B08XYP6BJV/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "    # response = requests.get(url, headers=header)\n",
    "    # response.raise_for_status()\n",
    "    # soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    # %% [markdown]\n",
    "    # ## UK\n",
    "    \n",
    "    # %%\n",
    "    from datetime import datetime\n",
    "    global_cookies = {}\n",
    "    def accept_cookies(url):\n",
    "        global global_cookies\n",
    "    \n",
    "        # Define a stronger header\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'DNT': '1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1'\n",
    "        }\n",
    "    \n",
    "        session = requests.Session()\n",
    "    \n",
    "        try:\n",
    "            # Visit the URL\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()  # Raise an error for non-2xx status codes\n",
    "    \n",
    "            # Accept cookies policy\n",
    "            payload = {'accept': 'all'}\n",
    "            response = session.post(url, data=payload, headers=headers, timeout=30)\n",
    "            response.raise_for_status()  # Raise an error for non-2xx status codes\n",
    "    \n",
    "            # Extract all cookies from the response headers and update global cookies\n",
    "            for key, value in response.cookies.items():\n",
    "                global_cookies[key] = value\n",
    "    \n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Error occurred during accepting cookies: {e}\")\n",
    "            return None\n",
    "        return global_cookies\n",
    "    \n",
    "    def get_soup(url, retries=3):\n",
    "        global_cookies\n",
    "    \n",
    "        # Define a stronger header\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'DNT': '1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1'\n",
    "        }\n",
    "    \n",
    "        session = requests.Session()\n",
    "    \n",
    "        # Retry mechanism\n",
    "        for _ in range(retries):\n",
    "            try:\n",
    "                session.cookies.update(global_cookies)  # Use global cookies for subsequent requests\n",
    "                response = session.get(url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()  # Raise an error for non-2xx status codes\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # file_name = f\"{random.randint(5, 150)}.html\"\n",
    "                # with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                #     file.write(str(soup))\n",
    "                return soup\n",
    "            except requests.HTTPError as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                time.sleep(random.uniform(1, 5))  # Add a random delay before retrying\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Failed to retrieve the page after multiple retries.\")\n",
    "            return None\n",
    "    \n",
    "        \n",
    "    def get_soup_us(url):\n",
    "        global global_cookies\n",
    "    \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'DNT': '1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1'\n",
    "        }\n",
    "    \n",
    "        # Define the initial JSON cookies data\n",
    "        initial_cookies_json = '''\n",
    "        [\n",
    "            {\n",
    "                \"Host raw\": \"https://.amazon.co.uk/\",\n",
    "                \"Name raw\": \"ubid-acbuk\",\n",
    "                \"Path raw\": \"/\",\n",
    "                \"Content raw\": \"259-6909043-1437223\",\n",
    "                \"Expires\": \"22-05-2025 22:35:05\",\n",
    "                \"Expires raw\": \"1747931705\",\n",
    "                \"Send for\": \"Encrypted connections only\",\n",
    "                \"Send for raw\": \"true\",\n",
    "                \"HTTP only raw\": \"false\",\n",
    "                \"SameSite raw\": \"no_restriction\",\n",
    "                \"This domain only\": \"Valid for subdomains\",\n",
    "                \"This domain only raw\": \"false\",\n",
    "                \"Store raw\": \"firefox-default\",\n",
    "                \"First Party Domain\": \"\"\n",
    "            },\n",
    "            {\n",
    "                \"Host raw\": \"http://www.amazon.co.uk/\",\n",
    "                \"Name raw\": \"csm-hit\",\n",
    "                \"Path raw\": \"/\",\n",
    "                \"Content raw\": \"tb:XN6TY644ZA4CNHWK94S0+s-XN6TY644ZA4CNHWK94S0|1717379581783&t:1717379581783&adb:adblk_no\",\n",
    "                \"Expires\": \"19-05-2025 07:53:01\",\n",
    "                \"Expires raw\": \"1747619581\",\n",
    "                \"Send for\": \"Any type of connection\",\n",
    "                \"Send for raw\": \"false\",\n",
    "                \"HTTP only raw\": \"false\",\n",
    "                \"SameSite raw\": \"no_restriction\",\n",
    "                \"This domain only\": \"Valid for host only\",\n",
    "                \"This domain only raw\": \"true\",\n",
    "                \"Store raw\": \"firefox-default\",\n",
    "                \"First Party Domain\": \"\"\n",
    "            }\n",
    "        ]\n",
    "        '''\n",
    "    \n",
    "        # Parse the initial JSON data\n",
    "        initial_cookies_data = json.loads(initial_cookies_json)\n",
    "        initial_cookies = {cookie[\"Name raw\"]: cookie[\"Content raw\"] for cookie in initial_cookies_data}\n",
    "    \n",
    "        # Use the global cookies if available, otherwise use initial cookies\n",
    "        cookies_to_use = global_cookies if global_cookies else initial_cookies\n",
    "    \n",
    "        # Introduce a random delay to mimic human behavior\n",
    "        time.sleep(random.uniform(1, 5))\n",
    "    \n",
    "        # Make the request with headers and cookies\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, headers=headers, cookies=cookies_to_use, timeout=30)\n",
    "        \n",
    "        # Update the global cookies with the current response cookies\n",
    "        global_cookies.update(session.cookies.get_dict())\n",
    "    \n",
    "        # Parse the response content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # file_name = f\"{random.randint(5, 150)}.html\"\n",
    "        # with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        #     file.write(str(soup))\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    def amazon_review(soup, url):    \n",
    "        review = {}\n",
    "        extracted_reviews = []   \n",
    "        try:\n",
    "            model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "        except AttributeError: \n",
    "            try:\n",
    "                model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "            except AttributeError: \n",
    "                model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "      \n",
    "        reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    \n",
    "        for item in reviews:    \n",
    "            # review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "            # review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "    \n",
    "            review = {    \n",
    "                'Model': model,    \n",
    "                'Review date': item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[\n",
    "                    1],\n",
    "                \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "                \"URL\" : url  \n",
    "            }\n",
    "            \n",
    "            \n",
    "      \n",
    "            try:    \n",
    "                review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "            except AttributeError:    \n",
    "                review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "      \n",
    "            try:    \n",
    "                review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "      \n",
    "            try:    \n",
    "                review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review[\"Verified Purchase or not\"] = None    \n",
    "      \n",
    "            try:      \n",
    "                review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "            except AttributeError:        \n",
    "                review[\"Review name\"] = None  \n",
    "      \n",
    "            try:    \n",
    "                review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review[\"People_find_helpful\"] = None  \n",
    "                \n",
    "            try:\n",
    "                seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "                if seeding:\n",
    "                   review['Seeding or not'] = seeding\n",
    "                else:\n",
    "                    raise AttributeError\n",
    "            except AttributeError:  \n",
    "                try: \n",
    "                    review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, string='Vine Customer Review of Free Product')\n",
    "    \n",
    "                except AttributeError:\n",
    "                    review['Seeding or not'] = None\n",
    "    \n",
    "            try:\n",
    "                review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "            except AttributeError:   \n",
    "                 review['Aggregation'] = None\n",
    "        \n",
    "      \n",
    "            extracted_reviews.append(review)    \n",
    "        \n",
    "      \n",
    "        return extracted_reviews\n",
    "    \n",
    "    # %%\n",
    "    urls = ['https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format',\n",
    "           'https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format',\n",
    "           'https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "    \n",
    "    # %%\n",
    "    import datetime \n",
    "    from datetime import datetime\n",
    "    star = ['one','two','three','four','five'] \n",
    "    max_retry_attempts = 2\n",
    "    all_reviews = []\n",
    "    for link in urls:\n",
    "        print(link)\n",
    "        for y in star:\n",
    "            found_reviews = True\n",
    "            for x in range(1, 11):\n",
    "                retry_attempts = 0\n",
    "                while found_reviews is True:\n",
    "                    try:\n",
    "                        url = f'{link}&pageNumber={x}&filterByStar={y}_star&sortBy=recent'  \n",
    "                        print(url)\n",
    "                        print('Page:',x, f'{y} star')\n",
    "                        soup = get_soup(url)  # Get the soup object from the URL\n",
    "                        extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                       \n",
    "                        # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                        #         print('No review')  \n",
    "                        #         found_reviews = False\n",
    "                        #         break \n",
    "                        \n",
    "                        if len(extracted_reviews) > 0:\n",
    "                            all_reviews.extend(extracted_reviews)\n",
    "                            print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "                        \n",
    "                        # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                        #     print(f\"Page {page} has no reviews, retry\")\n",
    "                        #     continue\n",
    "                            \n",
    "                        if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                            print('No more pages left')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                        \n",
    "                        if x >= 1 and len(extracted_reviews) == 0:\n",
    "                            retry_attempts += 1\n",
    "                            if retry_attempts == max_retry_attempts:\n",
    "                                found_reviews = False\n",
    "                                print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                                break\n",
    "                            else:\n",
    "                                print(f\"Page {x} has no reviews, retry\")\n",
    "                                continue \n",
    "    \n",
    "                        \n",
    "                                \n",
    "                        else:\n",
    "                            break  \n",
    "            \n",
    "                        \n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "    \n",
    "                        # If any exception occurs, retry\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"An error occurred, retrying\")\n",
    "                            continue  # Retry the loop\n",
    "                else:\n",
    "                    # If all retry attempts failed, move to the next page\n",
    "                    continue\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "    \n",
    "    from datetime import date \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    amazon2= pd.DataFrame(all_reviews)\n",
    "    amazon2['Retailer']=\"Amazon\"\n",
    "    amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "    amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "    amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "    amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "    amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "    amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "    amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "    amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "    columns_to_drop = [  \n",
    "        'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "    ]  \n",
    "    # amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "    amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "    \n",
    "    amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "    amazon_hp_combine\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    amazon_final = amazon_hp_combine \n",
    "    amazon_final.drop_duplicates(inplace = True)\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "        lambda x: re.sub(\n",
    "            r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "            '',\n",
    "            x\n",
    "        ).strip()\n",
    "    )\n",
    "    \n",
    "    amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "    amazon_final['Country'] = 'UK'\n",
    "    amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "    \n",
    "    amazon_final_df= amazon_final.rename(columns={\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Retailer': 'Retailer',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'Review date': 'Review_Date',\n",
    "        'Review name': 'Review_Name',\n",
    "        'Review rating': 'Review_Rating',\n",
    "        'Review title': 'Review_Title',\n",
    "        'Review Content': 'Review_Content',\n",
    "        'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "        'People_find_helpful': 'People_Find_Helpful',\n",
    "        'Seeding or not': 'Seeding_Flag',\n",
    "        'URL': 'URL',\n",
    "        'scraping_date': 'Scraping_Date',\n",
    "        'Segment': 'Segment',\n",
    "        'Competitor_flag': 'Competitor_Flag',\n",
    "        'Aggregation':'Aggregation_Flag',\n",
    "        'Country': 'Country'\n",
    "    })\n",
    "    \n",
    "    amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "    amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "    amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "    amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "    amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "    amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "    amazon_final_df\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    final_review = pd.concat([review_template, amazon_final_df])\n",
    "    final_review.to_csv('uk.csv', index=False)\n",
    "    \n",
    "    # %% [markdown]\n",
    "    ## Spain\n",
    "    \n",
    "    # %%\n",
    "    urls =  ['https://www.amazon.es/HP-DeskJet-2820e-Impresora-Multifunci%C3%B3n/product-reviews/B0CFFWJHMF/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format',\n",
    "            'https://www.amazon.es/Impresora-Multifunci%C3%B3n-HP-impresi%C3%B3n-Fotocopia/product-reviews/B0CFG1PB4P/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format']\n",
    "    \n",
    "    # %%\n",
    "    from datetime import datetime\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    def amazon_review(soup, url, translate_to=None):    \n",
    "        review = {}\n",
    "        extracted_reviews = []   \n",
    "        try:\n",
    "            model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "        except AttributeError: \n",
    "            try:\n",
    "                model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "            except AttributeError: \n",
    "                model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "      \n",
    "        reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    \n",
    "        for item in reviews:    \n",
    "            # review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "            # review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "    \n",
    "            review = {    \n",
    "                'Model': model,\n",
    "                'Review date':\n",
    "                    item.find('span', {'data-hook': 'review-date'}).text.replace('Revisado en España', '').split('el')[\n",
    "                        1],\n",
    "                \"Orginal Review\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(), \n",
    "                \"URL\" : url  \n",
    "            }\n",
    "            \n",
    "            \n",
    "      \n",
    "            if translate_to and review[\"Orginal Review\"]:\n",
    "                translated_review = translator.translate(review[\"Orginal Review\"])\n",
    "                review[\"Review Content\"] = translated_review\n",
    "    \n",
    "            try:\n",
    "                review[\"Orginal Title\"] = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                review[\"Orginal Title\"] = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()\n",
    "    \n",
    "            if translate_to and review[\"Orginal Title\"]:\n",
    "                \n",
    "                translated_review = translator.translate(review[\"Orginal Title\"])\n",
    "                test = translated_review\n",
    "    \n",
    "            review[\"Orginal Title\"] = review[\"Orginal Title\"].split('\\n')[-1]    \n",
    "            review[\"Review Title\"] = test.split('\\n')[-1]\n",
    "            \n",
    "            #print(review[\"Review Title\"])\n",
    "            \n",
    "            review[\"URL\"] = url\n",
    "    \n",
    "            try:\n",
    "                review[\"Review rating\"] = (\n",
    "                    item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\",0 de 5 estrellas\", \"\").strip())\n",
    "            except AttributeError:\n",
    "                review[\"Review rating\"] = (\n",
    "                    item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\",0 de 5 estrellas\", \"\").strip())\n",
    "    \n",
    "            try:\n",
    "                review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                review[\"Verified Purchase or not\"] = None\n",
    "    \n",
    "            try:\n",
    "                review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()\n",
    "            except AttributeError:\n",
    "                review[\"Review name\"] = None\n",
    "    \n",
    "            try:\n",
    "                review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                review[\"People_find_helpful\"] = None\n",
    "                \n",
    "            try:\n",
    "                seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "                if seeding:\n",
    "                   review['Seeding or not'] = seeding\n",
    "                else:\n",
    "                    raise AttributeError\n",
    "            except AttributeError:  \n",
    "                try: \n",
    "                    review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, string='Vine Customer Review of Free Product')\n",
    "    \n",
    "                except AttributeError:\n",
    "                    review['Seeding or not'] = None\n",
    "    \n",
    "            try:\n",
    "                review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "            except AttributeError:   \n",
    "                 review['Aggregation'] = None\n",
    "        \n",
    "      \n",
    "            extracted_reviews.append(review)    \n",
    "        \n",
    "      \n",
    "        return extracted_reviews\n",
    "    \n",
    "    # %%\n",
    "    import datetime \n",
    "    from datetime import datetime\n",
    "    star = ['one','two','three','four','five'] \n",
    "    max_retry_attempts = 1\n",
    "    all_reviews = []\n",
    "    for link in urls:\n",
    "        print(link)\n",
    "        for y in star:\n",
    "            found_reviews = True\n",
    "            for x in range(1, 11):\n",
    "                retry_attempts = 0\n",
    "                while found_reviews is True:\n",
    "                    try:\n",
    "                        url =f'{link}&filterByStar={y}_star&pageNumber={x}&sortBy=recent'\n",
    "                        print('Page:',x, f'{y} star')\n",
    "                        soup = get_soup(url)  # Get the soup object from the URL\n",
    "                        extracted_reviews = amazon_review(soup, url,translate_to=\"en\")  # Extract reviews from the soup\n",
    "                        # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                        #         print('No review')  \n",
    "                        #         found_reviews = False\n",
    "                        #         break \n",
    "    \n",
    "                        if len(extracted_reviews) > 0:\n",
    "                            all_reviews.extend(extracted_reviews)\n",
    "                            print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "    \n",
    "                        # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                        #     print(f\"Page {page} has no reviews, retry\")\n",
    "                        #     continue\n",
    "    \n",
    "                        if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                            print('No more pages left')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "    \n",
    "                        if x >= 1 and len(extracted_reviews) == 0:\n",
    "                            retry_attempts += 1\n",
    "                            if retry_attempts == max_retry_attempts:\n",
    "                                found_reviews = False\n",
    "                                print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                                break\n",
    "                            else:\n",
    "                                print(f\"Page {x} has no reviews, retry\")\n",
    "                                continue \n",
    "    \n",
    "    \n",
    "    \n",
    "                        else:\n",
    "                            break  \n",
    "    \n",
    "    \n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "    \n",
    "                        # If any exception occurs, retry\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"An error occurred, retrying\")\n",
    "                            continue  # Retry the loop\n",
    "                else:\n",
    "                    # If all retry attempts failed, move to the next page\n",
    "                    continue\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    from datetime import date \n",
    "    amazon2 = pd.DataFrame(all_reviews)\n",
    "    #print(amazon2)\n",
    "    \n",
    "    # Function to parse the date column\n",
    "    def parse_date(date_str):\n",
    "        months = {\n",
    "            \"enero\": \"January\",\n",
    "            \"febrero\": \"February\",\n",
    "            \"marzo\": \"March\",\n",
    "            \"abril\": \"April\",\n",
    "            \"mayo\": \"May\",\n",
    "            \"junio\": \"June\",\n",
    "            \"julio\": \"July\",\n",
    "            \"agosto\": \"August\",\n",
    "            \"septiembre\": \"September\",\n",
    "            \"octubre\": \"October\",\n",
    "            \"noviembre\": \"November\",\n",
    "            \"diciembre\": \"December\"\n",
    "        }\n",
    "        date_str = date_str.strip()\n",
    "        day, month, year = date_str.split(' de ')\n",
    "        month = months[month]\n",
    "        date = datetime.strptime(f\"{day} {month} {year}\", \"%d %B %Y\")\n",
    "        return date\n",
    "    \n",
    "    # Apply the function to the Date column\n",
    "    amazon2['Review date'] = amazon2['Review date'].apply(parse_date)\n",
    "    \n",
    "    amazon2['Retailer'] = \"Amazon\"\n",
    "    amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "    amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "    #amazon2['Review Title'] = amazon2['Review Title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "    amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "    amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "    amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "    amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "    amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model']\n",
    "    \n",
    "    columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "    amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis=1)\n",
    "    amazon_hp_combine.drop_duplicates(inplace=True)\n",
    "    \n",
    "    amazon_final = amazon_hp_combine\n",
    "    amazon_final.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Clean Review Content\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(lambda x: re.sub(r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.', '', x).strip())\n",
    "    \n",
    "    amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "    amazon_final['Country'] = 'Spain'\n",
    "    amazon_final.sort_values(by=['Review date'], ascending=False, inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    amazon_final_df = amazon_final.rename(columns={\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Retailer': 'Retailer',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'Review date': 'Review_Date',\n",
    "        'Review name': 'Review_Name',\n",
    "        'Review rating': 'Review_Rating',\n",
    "        'Review Content': 'Review_Content',\n",
    "        \"Review Title\": \"Review_Title\",\n",
    "        'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "        'People_find_helpful': 'People_Find_Helpful',\n",
    "        'Seeding or not': 'Seeding_Flag',\n",
    "        'URL': 'URL',\n",
    "        'scraping_date': 'Scraping_Date',\n",
    "        'Segment': 'Segment',\n",
    "        'Competitor_flag': 'Competitor_Flag',\n",
    "        'Aggregation': 'Aggregation_Flag',\n",
    "        'Country': 'Country',\n",
    "        'Orginal Review': 'Orginal_Review'\n",
    "    })\n",
    "    amazon_final_df['Orginal_Title'] = amazon2['Orginal Title']\n",
    "    amazon_final_df['Orginal Title'] = \"\"\n",
    "    amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "    amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "    amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    amazon_final_df['Scraping_Date'] = pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "    \n",
    "    # Create Review_Rating_Label column\n",
    "    amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "    \n",
    "    # Define the required columns\n",
    "    required_columns = [\n",
    "        'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "        'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "        'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "        'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "        'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "        'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country', 'Orginal_Title','Orginal Title',\n",
    "    ]\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    for col in required_columns:\n",
    "        if col not in amazon_final_df.columns:\n",
    "            amazon_final_df[col] = None  # or use an appropriate default value\n",
    "    \n",
    "    # Select only the required columns\n",
    "    amazon_final_df = amazon_final_df[required_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    amazon_final_df.to_csv('es.csv', index=False)\n",
    "    \n",
    "    # %% [markdown]\n",
    "    # # US\n",
    "    \n",
    "    # %%\n",
    "    def amazon_review(soup, url):    \n",
    "        review = {}\n",
    "        extracted_reviews = []   \n",
    "        try:\n",
    "            model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "        except AttributeError: \n",
    "            try:\n",
    "                model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "            except AttributeError: \n",
    "                model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "      \n",
    "        reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "        \n",
    "        # NPI launched on 2024-01-15\n",
    "        date_string = \"2024-01-15\"\n",
    "        min_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    \n",
    "        for item in reviews:    \n",
    "            review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "            review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "            if review_date < min_date:\n",
    "                print('Review date is less than 2024-01-15')\n",
    "                break\n",
    "        \n",
    "            review = {    \n",
    "                'Model': model,    \n",
    "                'Review date': review_date,     \n",
    "                \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "                \"URL\": url  \n",
    "            }\n",
    "            \n",
    "            try:    \n",
    "                review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "            except AttributeError:    \n",
    "                review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "      \n",
    "            try:    \n",
    "                review['Review title'] = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review['Review title'] = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "      \n",
    "            try:    \n",
    "                review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review[\"Verified Purchase or not\"] = None    \n",
    "      \n",
    "            try:      \n",
    "                review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "            except AttributeError:        \n",
    "                review[\"Review name\"] = None  \n",
    "      \n",
    "            try:    \n",
    "                review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review[\"People_find_helpful\"] = None  \n",
    "                \n",
    "            try:\n",
    "                seeding = item.find(\"span\", {'class': \"a-color-success a-text-bold\"}).text.strip()\n",
    "                if 'Vine Customer Review of Free Product' in seeding:\n",
    "                    review['Seeding or not'] = 'Vine Customer Review of Free Product'\n",
    "                else:\n",
    "                    review['Seeding or not'] = None\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "    \n",
    "            try:\n",
    "                review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "            except AttributeError:   \n",
    "                review['Aggregation'] = None\n",
    "        \n",
    "            extracted_reviews.append(review)    \n",
    "        \n",
    "        return extracted_reviews\n",
    "    \n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    \n",
    "    urls = ['https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format',\n",
    "           'https://www.amazon.com/HP-DeskJet-Wireless-Included-588S6A/product-reviews/B0CT2QHQVF/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "    \n",
    "    # %%\n",
    "    import datetime \n",
    "    from datetime import datetime\n",
    "    star = ['one','two','three','four','five'] \n",
    "    max_retry_attempts = 2\n",
    "    all_reviews = []\n",
    "    for link in urls:\n",
    "        print(link)\n",
    "        for y in star:\n",
    "            found_reviews = True\n",
    "            for x in range(1, 11):\n",
    "                retry_attempts = 0\n",
    "                while found_reviews is True:\n",
    "                    try:\n",
    "                        url = f'{link}&pageNumber={x}&filterByStar={y}_star&sortBy=recent'  \n",
    "                        print(url)\n",
    "                        print('Page:',x, f'{y} star')\n",
    "                        soup = get_soup_us(url)  # Get the soup object from the URL\n",
    "                        extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                       \n",
    "                        # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                        #         print('No review')  \n",
    "                        #         found_reviews = False\n",
    "                        #         break \n",
    "                        \n",
    "                        if len(extracted_reviews) > 0:\n",
    "                            all_reviews.extend(extracted_reviews)\n",
    "                            print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "                        \n",
    "                        # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                        #     print(f\"Page {page} has no reviews, retry\")\n",
    "                        #     continue\n",
    "                            \n",
    "                        if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                            print('No more pages left')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                        \n",
    "                        if x >= 1 and len(extracted_reviews) == 0:\n",
    "                            retry_attempts += 1\n",
    "                            if retry_attempts == max_retry_attempts:\n",
    "                                found_reviews = False\n",
    "                                print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                                break\n",
    "                            else:\n",
    "                                print(f\"Page {x} has no reviews, retry\")\n",
    "                                continue \n",
    "    \n",
    "                        \n",
    "                                \n",
    "                        else:\n",
    "                            break  \n",
    "            \n",
    "                        \n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "    \n",
    "                        # If any exception occurs, retry\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"An error occurred, retrying\")\n",
    "                            continue  # Retry the loop\n",
    "                else:\n",
    "                    # If all retry attempts failed, move to the next page\n",
    "                    continue\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "    \n",
    "    from datetime import date \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    amazon2= pd.DataFrame(all_reviews)\n",
    "    amazon2['Retailer']=\"Amazon\"\n",
    "    amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "    amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "    amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "    amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "    amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "    amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "    amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "    amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "    columns_to_drop = [  \n",
    "        'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "    ]  \n",
    "    # amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "    amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "    \n",
    "    amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "    amazon_hp_combine\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    amazon_final = amazon_hp_combine \n",
    "    amazon_final.drop_duplicates(inplace = True)\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "    amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "        lambda x: re.sub(\n",
    "            r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "            '',\n",
    "            x\n",
    "        ).strip()\n",
    "    )\n",
    "    \n",
    "    amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "    amazon_final['Country'] = 'US'\n",
    "    amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "    \n",
    "    amazon_final_df= amazon_final.rename(columns={\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Retailer': 'Retailer',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'Review date': 'Review_Date',\n",
    "        'Review name': 'Review_Name',\n",
    "        'Review rating': 'Review_Rating',\n",
    "        'Review title': 'Review_Title',\n",
    "        'Review Content': 'Review_Content',\n",
    "        'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "        'People_find_helpful': 'People_Find_Helpful',\n",
    "        'Seeding or not': 'Seeding_Flag',\n",
    "        'URL': 'URL',\n",
    "        'scraping_date': 'Scraping_Date',\n",
    "        'Segment': 'Segment',\n",
    "        'Competitor_flag': 'Competitor_Flag',\n",
    "        'Aggregation':'Aggregation_Flag',\n",
    "        'Country': 'Country'\n",
    "    })\n",
    "    \n",
    "    amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "    amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "    amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "    amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "    amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "    amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "    amazon_final_df\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    final_review = pd.concat([review_template, amazon_final_df])\n",
    "    final_review\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_review.to_csv(r'us.csv', index=False)\n",
    "    \n",
    "    \n",
    "    #marge amazon\n",
    "    \n",
    "    es_df = pd.read_csv('es.csv')\n",
    "    header = es_df.columns\n",
    "    \n",
    "    # Read the data from es.csv, uk.csv, and us.csv without headers\n",
    "    es_data = pd.read_csv('es.csv', header=0)  # Include header only for es.csv\n",
    "    uk_data = pd.read_csv('uk.csv', header=0)  # Exclude header for uk.csv\n",
    "    us_data = pd.read_csv('us.csv', header=0)  # Exclude header for us.csv\n",
    "    \n",
    "    # Combine the data\n",
    "    combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data with the header from es.csv\n",
    "    combined_df.to_csv('amazon.csv', index=False, header=header)\n",
    "    \n",
    "    #marge amazon\n",
    "    \n",
    "    \n",
    "    \n",
    "    #marge amazon\n",
    "    \n",
    "    es_df = pd.read_csv('es.csv')\n",
    "    header = es_df.columns\n",
    "    \n",
    "    # Read the data from es.csv, uk.csv, and us.csv without headers\n",
    "    es_data = pd.read_csv('es.csv', header=0)  # Include header only for es.csv\n",
    "    uk_data = pd.read_csv('uk.csv', header=0)  # Exclude header for uk.csv\n",
    "    us_data = pd.read_csv('us.csv', header=0)  # Exclude header for us.csv\n",
    "    \n",
    "    # Combine the data\n",
    "    combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data with the header from es.csv\n",
    "    combined_df.to_csv('amazon.csv', index=False, header=header)\n",
    "    \n",
    "    #marge amazon\n",
    "    \n",
    "    # Best buy hp\n",
    "    \n",
    "    \n",
    "    def get_review_bestbuy(url):\n",
    "        extracted_reviews = []\n",
    "        retry_count = 0\n",
    "        header = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "            'Downlink': '10',\n",
    "            'Dpr': '1',\n",
    "            'Referer': url,\n",
    "            'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "            'Sec-Ch-Ua-Mobile': '?0',\n",
    "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'cross-site',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "    \n",
    "        response = requests.get(url, headers=header)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup \n",
    "    \n",
    "    def bestbuy_review(soup, url):    \n",
    "        bestbuy = {}\n",
    "        bestbuy_reviews = []  \n",
    "        Model = soup.find(\"h1\", {'class':\"heading-5 v-fw-regular\"})\n",
    "        if not Model:\n",
    "            Model = soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"})\n",
    "        Model = Model.text if Model else None\n",
    "        \n",
    "            \n",
    "        npi = soup.find('span',{'class':'c-reviews order-2'} ).text\n",
    "        review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "        if review_session:\n",
    "            for item in review_session:    \n",
    "                bestbuy = {    \n",
    "                     'Model':Model,\n",
    "                    # 'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "                    'URL':url \n",
    "                }\n",
    "                try:    \n",
    "                    bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review title']  = None\n",
    "    \n",
    "                try:\n",
    "                    bestbuy['Review_Name']  = item.find(\"div\", {\"class\": \"ugc-author v-fw-medium body-copy-lg\"}).text  \n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review_Name']  = None\n",
    "                    \n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review rating']  = None\n",
    "    \n",
    "                review_date_element = item.find(\"time\", {\"class\": \"submission-date\"})\n",
    "                if review_date_element:\n",
    "                    review_date_string = review_date_element['title']\n",
    "                    review_date_datetime = datetime.strptime(review_date_string, '%b %d, %Y %I:%M %p')\n",
    "                    formatted_review_date = review_date_datetime.strftime('%Y-%m-%d')\n",
    "                    bestbuy['Review_Date'] = formatted_review_date\n",
    "                else:\n",
    "                    bestbuy['Review_Date'] = \"\"\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review promotion']  = None\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review aggregation']  = None\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['Review Content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review Content']  = None\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "                except AttributeError:    \n",
    "                    bestbuy['Review Recommendation']  = None\n",
    "    \n",
    "                try:    \n",
    "                    network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "                    if network_badge:\n",
    "                        bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "                    else:\n",
    "                        bestbuy['Seeding or not']  = \"\"\n",
    "                except AttributeError:    \n",
    "                    bestbuy['Seeding or not']  = \"\"\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['People_find_helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "                except AttributeError:    \n",
    "                    bestbuy['People_find_helpful']  = None\n",
    "    \n",
    "                try:    \n",
    "                    bestbuy['People_find_unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "                except AttributeError:    \n",
    "                    bestbuy['People_find_unhelpful']  = None\n",
    "    \n",
    "    \n",
    "                bestbuy_reviews.append(bestbuy)    \n",
    "            \n",
    "        \n",
    "      \n",
    "        return npi, bestbuy_reviews \n",
    "    \n",
    "    urls = ['https://www.bestbuy.com/site/reviews/hp-deskjet-2855e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6574145?variant=A',\n",
    "           'https://www.bestbuy.com/site/reviews/hp-deskjet-4255e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6575024?variant=A']\n",
    "    \n",
    "    # %%\n",
    "    max_attempts = 5\n",
    "    bestbuy_reviews = []\n",
    "    \n",
    "    for link in urls:\n",
    "        print(link)\n",
    "        should_continue = True\n",
    "        attempt_count = 0  # Counter for attempts\n",
    "        for x in range(1, 100):\n",
    "            if not should_continue:\n",
    "                break\n",
    "            while True:\n",
    "                url = f'{link}&page={x}'\n",
    "                try:\n",
    "                    soup = get_review_bestbuy(url)\n",
    "                    npi, reviews = bestbuy_review(soup, url)\n",
    "                    if npi == 'Be the first to write a review':\n",
    "                        should_continue = False\n",
    "                    print(f'Extracted reviews on page {x}: {len(reviews)}')\n",
    "                    bestbuy_reviews.extend(reviews)\n",
    "    \n",
    "                    next_page_link = soup.find(\"a\", {\"aria-disabled\": \"true\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                    if x > 1 and next_page_link and next_page_link.get(\"aria-disabled\") == \"true\":\n",
    "                        should_continue = False\n",
    "                        print('No more pages left')\n",
    "                        break\n",
    "    \n",
    "                    if len(reviews) < 20:\n",
    "                        should_continue = False\n",
    "                        print('Only 1 page')\n",
    "                        break\n",
    "                    else:\n",
    "                        break \n",
    "                except Exception as e:\n",
    "                    attempt_count += 1\n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds... (Attempt {attempt_count}/{max_attempts})\")\n",
    "                    if attempt_count >= max_attempts:\n",
    "                        print(\"Maximum number of attempts reached. Exiting loop.\")\n",
    "                        should_continue = False\n",
    "                        break\n",
    "                    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    from datetime import date  \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    review = pd.DataFrame(bestbuy_reviews)\n",
    "    review['Retailer']=\"Best Buy\"\n",
    "    review['scraping_date'] = pd.to_datetime(date.today())\n",
    "    \n",
    "    review['HP Model Number'] = review['Model'].str.extract(r'(\\d+e*)')\n",
    "    \n",
    "    hp_combine = pd.merge(review, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "    \n",
    "    hp_combine['Review Model'] = hp_combine['HP Model'] \n",
    "    hp_combine['People_find_helpful'] = hp_combine['People_find_helpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "    hp_combine['People_find_unhelpful'] = hp_combine['People_find_unhelpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "    \n",
    "    \n",
    "    columns_to_drop = [  \n",
    "        'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "    ]  \n",
    "      \n",
    "    hp_combine_bestbuy = hp_combine.drop(columns_to_drop, axis = 1) \n",
    "    \n",
    "    hp_combine_bestbuy = hp_combine_bestbuy.drop_duplicates()\n",
    "    \n",
    "    hp_combine_bestbuy\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    final_review = pd.DataFrame()\n",
    "    bestbuy_final = hp_combine_bestbuy\n",
    "    bestbuy_final.drop_duplicates(inplace = True)\n",
    "    \n",
    "    bestbuy_final = bestbuy_final.sort_values(by = ['Review Model', 'Review title', 'Review Content', 'scraping_date'])\n",
    "    \n",
    "    bestbuy_final['Competitor_Flag'] = bestbuy_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "    bestbuy_final['Country'] = 'US'\n",
    "    \n",
    "    bestbuy_final_version = bestbuy_final.rename(columns={\n",
    "        'Review date': 'Review_Date',\n",
    "        'review_text': 'Review_Content',\n",
    "        'Review rating': 'Review_Rating',\n",
    "        'url': 'URL',\n",
    "        'review_title': 'Review_Title',\n",
    "        'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "        'reviewer_name': 'Review_Name',\n",
    "        'syndication': 'Syndicated_Source',\n",
    "        'stars': 'Review_Rating',\n",
    "        'Retailer': 'Retailer',\n",
    "        'scraping_date': 'Scraping_Date',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Review title': 'Review_Title',\n",
    "        'Review Content': 'Review_Content',\n",
    "        'Review date': 'Review_Date',\n",
    "        'URL': 'URL',\n",
    "        'Seeding or not': 'Seeding_Flag',\n",
    "        'Review name': 'Review_Name',\n",
    "        'People_find_helpful': 'People_Find_Helpful',\n",
    "        'Syndicated source': 'Syndicated_Source',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Competitor_Flag': 'Competitor_Flag'\n",
    "    })\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    bestbuy_final_version.drop(columns=['Review Recommendation', 'People_find_unhelpful'], inplace=True)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    bestbuy_final_version.to_csv('Bestbuy_NPI_review.csv', index=False)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    Final_review = pd.concat([final_review, bestbuy_final_version], ignore_index=True)\n",
    "    \n",
    "    # Convert data types\n",
    "    Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "    Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "    Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "    \n",
    "    # Create Review_Rating_Label column\n",
    "    Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "    \n",
    "    # Define the required columns\n",
    "    required_columns = [\n",
    "        'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "        'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "        'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "        'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "        'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "        'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "        'Orginal_Title', 'Orginal Title'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    for col in required_columns:\n",
    "        if col not in Final_review.columns:\n",
    "            Final_review[col] = None  # or use an appropriate default value\n",
    "    \n",
    "    # Select only the required columns\n",
    "    Final_review = Final_review[required_columns]\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    Final_review.to_csv('bestbuy.csv', index=False)\n",
    "    \n",
    "    # Display the unique Scraping_Date values (optional)\n",
    "    print(Final_review['Scraping_Date'].unique())\n",
    "    \n",
    "    # # Walmart\n",
    "    \n",
    "    import random\n",
    "    User_Agent = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/109.0',\n",
    "        'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-G973U) AppleWebKit/537.36 (KHTML, like Gecko)',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; U; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.5399.183 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/113.0'\n",
    "    ]\n",
    "                \n",
    "    \n",
    "    cookie = [\n",
    "        'ACID=e743918f-9c01-4185-9889-01b383f39a46; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMDI5Mjg0MjMwNCwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAwMjkyODQyMzA0LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDAyOTI4NDIzMDMsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAwMjkyODQyMzA0LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6ZTc0MzkxOGYtOWMwMS00MTg1LTk4ODktMDFiMzgzZjM5YTQ2In0%3D; vtc=TPvkWCN79GksgVjpd8lQp4; btc=TPvkWCN79GksgVjpd8lQp4; bsc=XjMXQ-W4dVqYGkP1JSNWEs; _pxvid=d8404a5f-85e4-11ee-a839-f5e2c99825fc; pxcts=dd47a175-85e4-11ee-b2aa-79f32486f419; _tap_path=/rum.gif; _tap-criteo=1700292852331:1700292852725:1; _tap-Ted=1700292852724:1700292852725:1; _tap-lrV=1700292862974:1700292862975:1; _tap-lrB=1700292878531:1700292878532:1; _tap-appnexus=1700292879419:1700292879718:1; _gcl_au=1.1.258469864.1700294686; bstc=XgTuQhQdHEhnMqLxEbRYgw; mobileweb=0; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=; ak_bmsc=928D3BE7B227834B8EF4992840D6AFA1~000000000000000000000000000000~YAAQT0Dfb2Hq79qLAQAAWF0E7xW+cNY2y9+fVR2/0QrvmtyiqK/i/uEwVaXReQYp0Z0y6FV2m0EQCZomlg6PikSMdPE5Yx+Al074WilmM5mGxgz2kYLAXQy0aPKfmDgo9ohHooJNwq7VcMShtmCmJ1ARCqwfhpgMsbNFrR7LgHL3FRZJhrg9alhKuKLZRfToimKXZ0sKxSm5rLcO6L57KGoWlZX3NmA0ddatFrENafTH0mQVlKWpVPKHtgOZZh4EsGHOO90Z1fy0P2hIJNmpHpoDLk1w+0tBu9Upo+8L5KPS0K1LcuVDuaXEx4+n6o2fMg+HrkC1AF5T7cKLiLDZlbL/C5kSesKN3Rp2RaJjt8YzY2QTkaxrmX9TYd6omcLPWSqHLvjJK0/gyfE=; xptc=assortmentStoreId%2B3081; xpm=3%2B1700522123%2BTPvkWCN79GksgVjpd8lQp4~%2B0; b30msc=XgTuQhQdHEhnMqLxEbRYgw; _tap-li=1700522132702:0:2; _uetsid=0cce940087ff11eea88661d0630688d7; _uetvid=237553a085e911ee9eaec99bfa867851; auth=MTAyOTYyMDE4TzUL0tBWqaWlPLiIWSvGIIqP44I9XeKX8N5bcOOhCkNGFqMJTLhMlajVoxZh6%2F64P%2Bx3aAqHkDo7tLTswFgLqhE9gq7sWtCKMSvMMtAlsDnj17kGZ7Mu4K7gp6blyLzB767wuZloTfhm7Wk2KcjygsAEeU%2BeKCMhfP9XV060SY%2Fspww18DSfg4loIXetO33HWWxCKdp%2B8UHdguRD9DC%2FlTyW2FeTzNUdxbN2aHvb8W0UMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHWs0k33oHkBmchRaU9fj5kF7pZ6JaDMzmiWlGlRQ4nUMw5XFK3QDKKcB%2BPe5gKPHMTJomOWP4NHaOZmjOE06S78R4yUE7XpP0usJVgwSa5Hg5dejwrW41QOfpHzdmIzkekjyrOXbKKhH072NS%2FW0j%2FU%3D; locDataV3=eyJpc0RlZmF1bHRlZCI6dHJ1ZSwiaXNFeHBsaWNpdCI6ZmFsc2UsImludGVudCI6IlNISVBQSU5HIiwicGlja3VwIjpbeyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJodWJOb2RlSWQiOiIzMDgxIiwic3RvcmVIcnMiOiIwNjowMC0yMzowMCIsInN1cHBvcnRlZEFjY2Vzc1R5cGVzIjpbIlBJQ0tVUF9DVVJCU0lERSIsIlBJQ0tVUF9JTlNUT1JFIl0sInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQifV0sInNoaXBwaW5nQWRkcmVzcyI6eyJsYXRpdHVkZSI6MzguNDc0NSwibG9uZ2l0dWRlIjotMTIxLjM0MzgsInBvc3RhbENvZGUiOiI5NTgyOSIsImNpdHkiOiJTYWNyYW1lbnRvIiwic3RhdGUiOiJDQSIsImNvdW50cnlDb2RlIjoiVVNBIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJ0aW1lWm9uZSI6IkFtZXJpY2EvTG9zX0FuZ2VsZXMifSwiYXNzb3J0bWVudCI6eyJub2RlSWQiOiIzMDgxIiwiZGlzcGxheU5hbWUiOiJTYWNyYW1lbnRvIFN1cGVyY2VudGVyIiwiaW50ZW50IjoiUElDS1VQIn0sImluc3RvcmUiOmZhbHNlLCJkZWxpdmVyeSI6eyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJhY2Nlc3NQb2ludHMiOlt7ImFjY2Vzc1R5cGUiOiJERUxJVkVSWV9BRERSRVNTIn1dLCJodWJOb2RlSWQiOiIzMDgxIiwiaXNFeHByZXNzRGVsaXZlcnlPbmx5IjpmYWxzZSwic3VwcG9ydGVkQWNjZXNzVHlwZXMiOlsiREVMSVZFUllfQUREUkVTUyJdLCJzZWxlY3Rpb25UeXBlIjoiREVGQVVMVEVEIn0sInJlZnJlc2hBdCI6MTcwMDU0NTYxMjY4MiwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOmU3NDM5MThmLTljMDEtNDE4NS05ODg5LTAxYjM4M2YzOWE0NiJ9; _tap-googdsp=1700524013260:1700524013261:1; bm_mi=6D08B7F70C2629CB7335921BD0A476C4~YAAQVkDfbybwhNCLAQAAiz8h7xVv+k/UWG2WIDe/gPQ8AOZTnjAhiIOZ0bLj8CiGeNePuQWuPNwhDXS1oNp15tCC1PJSEhUXHiX+NSzkvUL7leDWiBNTKpbglVNmWH1NuG/LvNHQA2LBVlOMNtN+pa4w85WJ5WffbcN5bm4oXQMtRoZmDiaP9LejTQc7RtRDzb/UBiKxSj7B7tWFZWvoaMl7Byh7Zw98qmlF62SYLz85rGSvABr10DN471JChbH4Q/G82+KTkYcWus+IMdzcowTy7Z+Q8iX5zSmhRA2ELuD5Ie109BCzItjTb03IbEwxaZaoK0uGP1lkf74XjjcvevsgHZqUduo=~1; _tap-wmt-dw=1700522135009:0:2; _px3=5ea3ebae35f07f1831578749c875dce471cd3c238e9b1d48cc4d8ac7ed6ef5be:/ONT+vAVQYHGOAb5jG51c6bmUcQjbaW1zryLfFf3ZEFrqWRTeciC0Y4hk/ovL1XeinngRvREckraA7RQaqwaeA==:1000:tUHSmj1FtO2NPUlmIukbTE9SnWpD2T21PLjZBMihCAM89elMZOLre7HSRlFqK71v0f8yL9k4n8Sg8C5OWCQ9mrtXRYNZ7J2cHZIgW/Xriz9FwISvvFB2qUm7ermjWRrawUo7rSOqC6UHNjyG4cNmWupEtwDC+dn8hy+VsaMiY64jQxqICji2ZH+yxxSM8rRGtusit4qn5KaOrVpNR2530EK/u3KBOrrKvimdzwbEfh4=; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1700524027817; xptwj=qq:856f56eac45d86299598:ZRnl9M8BUfAYvTYtX+QCNLhEzqw6K+xHmpEjeoxuN7UABA+EM4HcK1WV764ZOO5/6syK6HJNzQ+7oCA1dX0quNAOWtTaQIDcRQupz54KdhbyjvwwBjDPRRo9S45zB814KqqVnZB4xW5GOyMmpEBETfwct40JrJ9yQKgEdWfokI/A+gc=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1700524028000@firstcreate:1700292842259\"; xptwg=3735093285:1780D552F836C50:3B5C8D6:477BA478:6AAF8424:7C4EFE87:; TS01a90220=01419f1d62bb3082af49e4e290c9e4f7b7d4d09f7a4d1f20d21c60c4e1a7da9ee45eaa7b433f2f9eae578eda14efecc83c7473e1aa; bm_sv=FBAB233769AD51E9AA3317FB35DBDC6D~YAAQVkDfb73yhNCLAQAA5XMh7xUG5PTSMRUK4v2TvEXPwemnWtVYlvkgiCKbkuxNXqf+vVZB8rZcQemv+ydTIl18pO1qMpban0Uf+/ci+ZkYfinA4Fd+fcZ1uHAyQxRGF/plo2v0Gq9TxZgp4pp/YFFuxSnGTqGRKl4RkFJbdxQSVM0CBeOhtUQWXZ0y9L4TE3TF7jw/A9Nu++2UrVU5zLqdE22Fo4Pdw6Yn6RnKbC9OZ7gJsF2DhCgy6+IkYxmg/SE=~1; _pxde=8e9d44f04024aa1db6ed77fcc7bbf79de1d603492bbaba0843e90b5cd14038cf:eyJ0aW1lc3RhbXAiOjE3MDA1MjQwMjg5MTR9',\n",
    "        'ACID=7cff0725-9085-4be0-bf3c-6839f8621f69; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMjYzMDM3NDkxNiwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAyNjMwMzc0OTE2LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDI2MzAzNzQ5MTUsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAyNjMwMzc0OTE2LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6N2NmZjA3MjUtOTA4NS00YmUwLWJmM2MtNjgzOWY4NjIxZjY5In0%3D; vtc=c_lPqL_NYDC6UXPeh9MF7U; _pxvid=5644c6ff-9b27-11ee-ac94-b6ae3dc3013d; pxcts=578a7ded-9b27-11ee-b1db-83fbf11618da; thx_guid=f069aa2ed76a293b146f2f45273ffc5e; QuantumMetricUserID=86dbb1aaa4c0846b6472227c8127c115; bm_mi=A9B761149346E34C2336993088CBA838~YAAQpCLHF0tB41iMAQAAT6lrexYSWryWW33ix0ebW0gEwR9R9Q3T0mju/pC7V7LqjuwHJrGseRkgFSZZiZVHV/zjTu4nCYLngf6698jYoiODJd20Ah32C8vCA7fNXyHUhtfSgpdKNkkOpAYXO8oYiiVKF1l5IHZeILy9fj41t1W9tF9DODYuxTNU8QPPnZr4w70z23lcK+3EBKd8IfqBgPT9kjLT3pKPY5P1EjKBk7Wvn9d44E3YtdQOdIlnmZMFqcniDHXEd0TgXXrvQ7NVo1BkEMaGynnbyeApl64EsIYaBB5nCsrN2NFOY0C1ZumHLPsxg5pg~1; ak_bmsc=28DD2AB37291D5EE54D348A14FFB7027~000000000000000000000000000000~YAAQpCLHF1FB41iMAQAA3atrexY3RQO5uSrgAn4o71OTeCY3Kb4oklPOhzO4ob+ZppkPcKbMuYwUEtBs6vEVlp9CoeIcig2wNR+YFPAMLag0FgMkBgPTNS7DAEU7mXMzqG7Zxes7pw+1iCYIJKVm0KAso3OE50SIiVGnpFKhhL7E2/cDwpWbUdrNVY30TGqqMk/uCOYcvkqNQE0wzVpTYkK2hWIeztHPT5fNqkdtRUylfN9FBTzfaUxix+emgkOw4S/zZxgkw4lNSo/YbiRZDpT409NuUxV4ht5j47adbQ1Q8F08pAvMkK3vGzOypF53bdu9Iw3g3D4PUZDOdQ2FFArxdUalCDwieOVrUbY5Xu55+30+fnRIFgzDM+aa72JMrHteO5326b1oz/QcUX1+EgWPmeSXJTk6/0EJvYS6yU9fnzsad5XVkCe4cRcS0AP/ZVfbCDorHjsVEachEwRjpKmMWx8q6DOuGFw00I1vRusHKdW1nrCknd+TjPY2u042+mVSAQCo0JoYGB8VZlNhvUof6KzcVkfB4ZG5; auth=MTAyOTYyMDE45uQqkShnPmrFyDklTStmr7wc0IXA0uhqKhIjidzMeOlor0FoZrStjvQEbiRm5ZqSmQEJ%2FPunCsfySE6LPKFrLFOCiFQAqcgu3n0iVe3t%2BsYE2L20v13oIDsvWtwF1M8S767wuZloTfhm7Wk2Kcjygi5k0VvBM%2FJjwcKWWhCnBS%2FsNVBmy9J1bR2VHO%2FdV8LIpQmp8XOq309QoW%2BZviaSOv8sqUVU54sFd4Bd6dus2ZEUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHVeQqO76rPmdncwFjpe%2BDY%2BOi7voHCqN3McdaIPwoybp22ykAEJaZ9rj2LqLTXsr9v3%2FEocn3Z%2BtLU09JmG1TrtCvNQVjLp%2Ffv%2BF%2BvOqp4HL4oel5ASvElr1Cex8QK9UZEjyrOXbKKhH072NS%2FW0j%2FU%3D; bstc=VDdH8jJ4m0c1jwDTa0PzoE; mobileweb=0; xptc=assortmentStoreId%2B3081; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=1mX_Y|7C2Eg|ERspM|HhvdQ|IS-p_|LPh6f|XbHrX|csP8O|gUDh7|oY0CV|qKfBf|yamTG; exp-ck=IS-p_1LPh6f1XbHrX1gUDh71qKfBf1; xpm=1%2B1702877712%2Bc_lPqL_NYDC6UXPeh9MF7U~%2B0; QuantumMetricSessionID=fb9f81cd396976e7c5f3318d82cf9aca; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1702878308583; xptwj=qq:1076adbafd040b695fce:7QGLRUcehov3qVy4qSYDvW4xgk6Uac2yX7HX1tE4u646HYe83hDIeMje3Thc/1gvh8z8MrbJNvYdlJL4OmUTikr/fII5bRfC3yxM6l2D8uZIkuA2LrEs6TpjeeKaJ8Uu78B/EOBkj1BzXZ7N6125enlMLSzG; xptwg=3146300320:19444130883AE60:3FBA1BE:1196AC61:D2F58FD2:22F02742:; _px3=6c9349297a823e7545b893fcfa8e6bea6eecb57dc1b11b7cf64522744342fc05:q5wbXoTzZgQ7QP9YgqzSywMmiYqoqw/ZrncImDY3CrTPeEkOqVzPo2zJ3AHIc+s/Ltf7IlS8awVgjpNXM98pxQ==:1000:cn/ohrnROd+TGJA88Xeo4/KYIo/UsLvu6vSoOqMnmEug9QezGcx/WE0rfSuHGKZjjjaYrqXq8pBrJ5nlgEeADLsTsoKQiFm1rc/m/6Q+wMMNleiOZAl9VZy3wpYUymfUCrFwGU+qqgFlXPCfQNN9o6vjljQksDltmcFQIof95UjvNs7UMihymHVA7zlP5BOXCURkIHiiJbM73dOLjmbFMFnxGHKXULDB8u2cb1oECR0=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1702878315000@firstcreate:1702630374882\"; TS01a90220=014e9abc5b76ea37289d24aa7bf6872327f3a801259b614de94e2e0e90c1b0f94c88be0edeb3c58cf3cba5b3568b2b79f66cc4dc9c; bm_sv=ADF605AFD1AA1E340A3C57CE2E012D0E~YAAQJPN0aPSze2mMAQAAbQJ1exaqQAmPQEugdiG3MQAUrup+aS98eOHJhIKWYEY4cQ16kvxr5PjD68+TG/q08uWKgkntuhl4BsnOGqcEXY7GjWBoB0neW+58ysLmfhee1rCKsVjS3iaBCoFZwcqIgkO3eDhiRFyKZXfNTkKDX7JBTyy63S1DAg2SrohZrD0KlpE5E2rFPlHxbpWohjTWhvXGlDIHA0lk8uQB+NvqN5R8yDL6bMxSm0YrJ6/p6h5kxvQ=~1; _pxde=3cb689d88ebd75dfd5e5c64a8389623aecf6bac16e69f216cf01a4e825c18d52:eyJ0aW1lc3RhbXAiOjE3MDI4NzgzMTUzNDl9',\n",
    "        'ACID=6ff15283-acba-4fe2-89c5-ac59a9b887d1; hasACID=true; thx_guid=0c376c1283b8b14be6de36c7c9897b80; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1696941267921; _m=9; vtc=WYxqCClwKT085jYR_DbfQI; _pxvid=5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; auth=MTAyOTYyMDE4IIJ07VmsLMduZmu3wWCW8eIqM67RZKVCKQnP%2BJhyprb8mcLFoQ86xyheUk7V1wBI53SlNalpIeiZEAnwibnwBxAuKnHyan446S83cruRp5IqJEEhjLzfZ9dTkdF%2F5mvs767wuZloTfhm7Wk2Kcjygt6CFmh5hT8BoAhiLFQG8TM4tK7YyL%2Bjr93Ekvm3gtoWXd1A1TJkrpfzbS%2B%2BXZ2ssMBCHcdxxw3SP0Sy3y18bhsUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHfClvOkjwxW8L0euuWDrN9AOTwJZ5k6XdH2IzYdedb%2BeKcBnww%2BqCeKjSX3bV3tRPjMVnRfkQSZ38Y3kHRhf5YWMEw3bsV%2BTWUdfiZ61nY1rm2KT4Gr0iVCCeIJhV8GhuUjyrOXbKKhH072NS%2FW0j%2FU%3D; locDataV3=eyJpc0RlZmF1bHRlZCI6dHJ1ZSwiaXNFeHBsaWNpdCI6ZmFsc2UsImludGVudCI6IlNISVBQSU5HIiwicGlja3VwIjpbeyJidUlkIjoiMCIsIm5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJub2RlVHlwZSI6IlNUT1JFIiwiYWRkcmVzcyI6eyJwb3N0YWxDb2RlIjoiOTU4MjkiLCJhZGRyZXNzTGluZTEiOiI4OTE1IEdlcmJlciBSb2FkIiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeSI6IlVTIiwicG9zdGFsQ29kZTkiOiI5NTgyOS0wMDAwIn0sImdlb1BvaW50Ijp7ImxhdGl0dWRlIjozOC40ODI2NzcsImxvbmdpdHVkZSI6LTEyMS4zNjkwMjZ9LCJpc0dsYXNzRW5hYmxlZCI6dHJ1ZSwic2NoZWR1bGVkRW5hYmxlZCI6dHJ1ZSwidW5TY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJodWJOb2RlSWQiOiIzMDgxIiwic3RvcmVIcnMiOiIwNjowMC0yMzowMCIsInN1cHBvcnRlZEFjY2Vzc1R5cGVzIjpbIlBJQ0tVUF9JTlNUT1JFIiwiUElDS1VQX0NVUkJTSURFIl0sInNlbGVjdGlvblR5cGUiOiJMU19TRUxFQ1RFRCJ9XSwic2hpcHBpbmdBZGRyZXNzIjp7ImxhdGl0dWRlIjozOC40NzQ1LCJsb25naXR1ZGUiOi0xMjEuMzQzOCwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiY291bnRyeUNvZGUiOiJVU0EiLCJnaWZ0QWRkcmVzcyI6ZmFsc2UsInRpbWVab25lIjoiQW1lcmljYS9Mb3NfQW5nZWxlcyJ9LCJhc3NvcnRtZW50Ijp7Im5vZGVJZCI6IjMwODEiLCJkaXNwbGF5TmFtZSI6IlNhY3JhbWVudG8gU3VwZXJjZW50ZXIiLCJpbnRlbnQiOiJQSUNLVVAifSwiaW5zdG9yZSI6ZmFsc2UsImRlbGl2ZXJ5Ijp7ImJ1SWQiOiIwIiwibm9kZUlkIjoiMzA4MSIsImRpc3BsYXlOYW1lIjoiU2FjcmFtZW50byBTdXBlcmNlbnRlciIsIm5vZGVUeXBlIjoiU1RPUkUiLCJhZGRyZXNzIjp7InBvc3RhbENvZGUiOiI5NTgyOSIsImFkZHJlc3NMaW5lMSI6Ijg5MTUgR2VyYmVyIFJvYWQiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJjb3VudHJ5IjoiVVMiLCJwb3N0YWxDb2RlOSI6Ijk1ODI5LTAwMDAifSwiZ2VvUG9pbnQiOnsibGF0aXR1ZGUiOjM4LjQ4MjY3NywibG9uZ2l0dWRlIjotMTIxLjM2OTAyNn0sImlzR2xhc3NFbmFibGVkIjp0cnVlLCJzY2hlZHVsZWRFbmFibGVkIjp0cnVlLCJ1blNjaGVkdWxlZEVuYWJsZWQiOnRydWUsImFjY2Vzc1BvaW50cyI6W3siYWNjZXNzVHlwZSI6IkRFTElWRVJZX0FERFJFU1MifV0sImh1Yk5vZGVJZCI6IjMwODEiLCJpc0V4cHJlc3NEZWxpdmVyeU9ubHkiOmZhbHNlLCJzdXBwb3J0ZWRBY2Nlc3NUeXBlcyI6WyJERUxJVkVSWV9BRERSRVNTIl0sInNlbGVjdGlvblR5cGUiOiJMU19TRUxFQ1RFRCJ9LCJyZWZyZXNoQXQiOjE2OTg0MDA0ODkwNzgsInZhbGlkYXRlS2V5IjoicHJvZDp2Mjo2ZmYxNTI4My1hY2JhLTRmZTItODljNS1hYzU5YTliODg3ZDEifQ%3D%3D; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIn0sInNoaXBwaW5nQWRkcmVzcyI6eyJ0aW1lc3RhbXAiOjE2ODU3ODc2OTkwNzksInR5cGUiOiJwYXJ0aWFsLWxvY2F0aW9uIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJwb3N0YWxDb2RlIjoiOTU4MjkiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJkZWxpdmVyeVN0b3JlTGlzdCI6W3sibm9kZUlkIjoiMzA4MSIsInR5cGUiOiJERUxJVkVSWSIsInRpbWVzdGFtcCI6MTY5ODM5Njg4OTA3Miwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIiwic2VsZWN0aW9uU291cmNlIjpudWxsfV19LCJwb3N0YWxDb2RlIjp7InRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwiYmFzZSI6Ijk1ODI5In0sIm1wIjpbXSwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOjZmZjE1MjgzLWFjYmEtNGZlMi04OWM1LWFjNTlhOWI4ODdkMSJ9; bstc=RDjQg_AX5lxPJuFgq3Nu74; mobileweb=0; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=92YHy|YnYws|yUqGy; exp-ck=YnYws4; pxcts=7ca9260b-74a6-11ee-b9ba-cadc77ff3193; xptc=assortmentStoreId%2B3081; xpm=1%2B1698396890%2BWYxqCClwKT085jYR_DbfQI~%2B0; TS01a90220=016ea84bd28377d28f6c5f8c825a73acff43907723c0c62c3b0f4412526365096a8d6e578fadcc7b7c5313bbb50b7e1d6a09790715; xptwj=qq:8c955083c3f2d971e73b:/AUbzj+G4qHiFzoBmTqw7sTxNCjaAsdlFcEBElDEMhfux6fBocZ2XuU9J43MZqv9xFdQR6jCZn1NHMlGARop+dGCrSwqG/FMfkjiqrzMhk6qDRXnGwdXkmVMkY17cQBnEuy2yF31oDbLB2VcFQm/6GzBoQa2xx4u4jyJ; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1698397083000@firstcreate:1696941267921\"; xptwg=2654488666:13D200536871A40:321F50E:2AE8C8EB:145FE951:E3F58838:; _px3=2259e2ff82c009ea11f3276f67eeec8c7933f2015f7c4f4f537a810e135667b2:WTyoBm43yjHnl6DaKRhXrBuw1dVxFeMmNao4s1nyzyzay9AT/JNRr2njauNA3Q05CZCkyHcGpiOlACvkOPaeLA==:1000:eqWykI7N/3Nxz6qb3xQH+stpzInFVztcX104+VKDHUoglkWCA2mLjEu+Zknx3FqHY6MZskTdWOhU7b/cxkcRsZzn2v7xd2d0SIFOVaBuJFFm4ddlo4ejUXkO/Ta7SH2GcvS0zq5pIVgCSlg9SjcmMala24rEipeabfBpgSjY1DFP4u3vQ5vVD9nlh6dHTGDAN6J86YkunMWOWKq/mltB5LLWDt5U+hsHxwlUQFOl53E=; _pxde=67376920231ea198f43cb2a2fd4e3570e6f0c4527d7e9406b6848b481178e72e:eyJ0aW1lc3RhbXAiOjE2OTgzOTcwODU3NDh9',\n",
    "        'ACID=6ff15283-acba-4fe2-89c5-ac59a9b887d1; hasACID=true; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1696941267921; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIn0sInNoaXBwaW5nQWRkcmVzcyI6eyJ0aW1lc3RhbXAiOjE2ODU3ODc2OTkwNzksInR5cGUiOiJwYXJ0aWFsLWxvY2F0aW9uIiwiZ2lmdEFkZHJlc3MiOmZhbHNlLCJwb3N0YWxDb2RlIjoiOTU4MjkiLCJjaXR5IjoiU2FjcmFtZW50byIsInN0YXRlIjoiQ0EiLCJkZWxpdmVyeVN0b3JlTGlzdCI6W3sibm9kZUlkIjoiMzA4MSIsInR5cGUiOiJERUxJVkVSWSIsInRpbWVzdGFtcCI6MTY5Njk0MTI2ODAwMSwic2VsZWN0aW9uVHlwZSI6IkxTX1NFTEVDVEVEIiwic2VsZWN0aW9uU291cmNlIjpudWxsfV19LCJwb3N0YWxDb2RlIjp7InRpbWVzdGFtcCI6MTY4NTc4NzY5OTA3OSwiYmFzZSI6Ijk1ODI5In0sIm1wIjpbXSwidmFsaWRhdGVLZXkiOiJwcm9kOnYyOjZmZjE1MjgzLWFjYmEtNGZlMi04OWM1LWFjNTlhOWI4ODdkMSJ9; userAppVersion=us-web-1.102.0-0f3d752097f13fd03499487f7cfc0f9ff879d809-1005; abqme=true; vtc=WYxqCClwKT085jYR_DbfQI; _pxhd=5a7ffd639284c9b62b5b6953d2b6554b5e4fb23e72bdea13cb0d60c5e9cb2592:5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; TBV=7; _pxvid=5a44b304-6769-11ee-a5a3-89fdf7f6e6ba; pxcts=5b1312c1-6769-11ee-9d4b-928400606778; xptwj=qq:19ae55e85ed74ecb934a:FzwixoJNbjsJTKIVOxs2Y3BCAjYnbpEJ9QAEPF+vcgu7rou9eHViyjDPVj+jQqEQsDVe8eLUcM9yr4bzIXF5/EpE+3GBy+nQfjIux03VKMmH4uP0zvUVBAnki5gXoud346PderEXI4ZdwzI5dEw9RZpxrSE=; _astc=dd455cd93be2a8805fa78a0c5637c0bc; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1696943665000@firstcreate:1696941267921\"; xptwg=3827596973:8C0849C3A8C7E8:1626C14:C8D91831:9FD2BC9F:ED33D50A:; TS012768cf=0178545c900bf5c440f69b21bbdea7b97f1bb93829c83cdf12d8a829eaa1f335330ed161b9a95404539366655b7d1af2acfeaa823d; TS01a90220=0178545c900bf5c440f69b21bbdea7b97f1bb93829c83cdf12d8a829eaa1f335330ed161b9a95404539366655b7d1af2acfeaa823d; TS2a5e0c5c027=0881c5dd0aab20006278b8f1c282bae24adaf3370d4de18765bc5b96e8045bf269a5218310a2b5f308691a3aad113000c4a62f799fcd6ee383055c96bd53eb55adecf63f1102cc5dd537a8a8e81ee1756147c51df92a6c584fb9a07c447c0309', \n",
    "        'ACID=13f858d3-9165-43cb-bab4-a63c55e6a6a8; hasACID=true; _m=9; locGuestData=eyJpbnRlbnQiOiJTSElQUElORyIsImlzRXhwbGljaXQiOmZhbHNlLCJzdG9yZUludGVudCI6IlBJQ0tVUCIsIm1lcmdlRmxhZyI6ZmFsc2UsImlzRGVmYXVsdGVkIjp0cnVlLCJwaWNrdXAiOnsibm9kZUlkIjoiMzA4MSIsInRpbWVzdGFtcCI6MTcwMjk1NTMwNTIxNSwic2VsZWN0aW9uVHlwZSI6IkRFRkFVTFRFRCJ9LCJzaGlwcGluZ0FkZHJlc3MiOnsidGltZXN0YW1wIjoxNzAyOTU1MzA1MjE1LCJ0eXBlIjoicGFydGlhbC1sb2NhdGlvbiIsImdpZnRBZGRyZXNzIjpmYWxzZSwicG9zdGFsQ29kZSI6Ijk1ODI5IiwiY2l0eSI6IlNhY3JhbWVudG8iLCJzdGF0ZSI6IkNBIiwiZGVsaXZlcnlTdG9yZUxpc3QiOlt7Im5vZGVJZCI6IjMwODEiLCJ0eXBlIjoiREVMSVZFUlkiLCJ0aW1lc3RhbXAiOjE3MDI5NTUzMDUyMTQsInNlbGVjdGlvblR5cGUiOiJERUZBVUxURUQiLCJzZWxlY3Rpb25Tb3VyY2UiOm51bGx9XX0sInBvc3RhbENvZGUiOnsidGltZXN0YW1wIjoxNzAyOTU1MzA1MjE1LCJiYXNlIjoiOTU4MjkifSwibXAiOltdLCJ2YWxpZGF0ZUtleSI6InByb2Q6djI6MTNmODU4ZDMtOTE2NS00M2NiLWJhYjQtYTYzYzU1ZTZhNmE4In0%3D; vtc=bqXm-Yjh0fiyfC30O9pl40; pxcts=e0a91fc6-9e1b-11ee-b6a5-16e7f3c0c35f; _pxvid=dfcedab4-9e1b-11ee-852d-cc6ba9195b8b; thx_guid=d45cea97b66ab1e12333b05cf300756a; auth=MTAyOTYyMDE4eh0UBw4CZsMoITCUpLy%2FJSKZAhO1G27GRMvlySSJu45p1%2FLh4CYkzkJMDMayVQPkr4cn%2Flzfu3Cnm2UosjL24mxfJqc48PMpEXdaud0aTiSYybEajj235v6w6v39wTDg767wuZloTfhm7Wk2KcjygobRHThsmZk%2BGcqTfIab85Qi91RLvjJ4oWxX7pdsgCM7kaNdhw7fWS2J7XYV98BtLp94fDX6wtiILXdT4QaPibQUMk70P8glgOEpLOprhDfMDCcb9mgycy9jtT1uIyOBHYAmxWm2QCSM81oB%2BzgtGh7GRphnRVqhmKz4T4aeRpfPdIrk6V7SOwO2Q2sHD6RhS27h8BprVsmSYkJBi2ZANdkS%2BmkqvUibwJ%2ByNBdR4lDAXTGRW5wSkfxBkI28si3Kp0jyrOXbKKhH072NS%2FW0j%2FU%3D; bstc=eiIdhbpg62SIN7bXqMg3z8; mobileweb=0; xptc=assortmentStoreId%2B3081; xpth=x-o-mart%2BB2C~x-o-mverified%2Bfalse; xpa=1mX_Y|M65NP|WuHSe|XmyU7|j21L4|oY0CV|qKfBf|qMQpD|r97uO; exp-ck=WuHSe1XmyU71j21L41qKfBf1r97uO1; xpm=1%2B1702962355%2BbqXm-Yjh0fiyfC30O9pl40~%2B0; bm_mi=549FCFD96C94678551FEF51A3881B212~YAAQX/N0aH0UHFmMAQAAD7F9gBYxuYoIp+e5g1nqvEyexI6hSo7fx80xYZvYxOzX42gMoKUiODVyjrfCTZxzUPZoY9b4p074yFBo9qtr3dvqcrWK+Ax3QnilXHYN/O3i9uGgMgaYzDbvFsofcB+UXepCydTAGXEwlgTjcuhCK/WHSSkwgTYZBCgoH4ZvNuWxbDqSZJVHpneNw6xdnWnxgDwCiwgTl9vrLyjdg09e4T2smzNssFE245sbVMdbiBsLplNv/n7syfFu/VVXuZyCtMvNaORC85FiXoDfVzz0LARwtlqQLr1WRjGOEklBkKaf65Mxcoy8~1; ak_bmsc=2EB05928C59E3D09719A985EA17D031E~000000000000000000000000000000~YAAQX/N0aIcUHFmMAQAAg7N9gBbfI4TKoREfgSIqsmk7VQxNHd1hZRptwqQOz86rLpb4Dm8hwXmGCtpOkNohVRQHY/oG6ctA+y0cssVoASKpTSd1fEYyHDqbC+DrYiN6LJ0q930S+F4z3j6ktqalEJm6Bh4BtsGU6d5h43XeXgGRDzTLgZf+D5taWBpWzCUAb/xXmehSq77svJniJ4HmGoQYRn4fH8UeuB958xg08dR7TF3qVNpEUpqlv1WV4x9fcXDK35s9YBcHcZJUpGXMIx0+3rA/JVyuVqSLJnTzu9yOhVYgISi07np3590KzU5WW1U9oTYdQVYyvkMD3EL1LbLzXiZ10P9fl+HnFHX0V52zIFkboWpvSkdK4HVlAR/7LrWEM6PaexEzR6dCESt+Az+opps+z9JZNT6Y9W2aalGBPdzj275+3AznMTCf3hNispVbb4Im9fKO4HjsyY/rQKVgXwkJ6+IVYPDvYRFJaS1jY42zuoQknNidJ0ajbhHEwI5WkMGbsQ6ZfxAYYFUHPOj4ZWUDO1N9gnSx; AID=wmlspartner%3D0%3Areflectorid%3D0000000000000000000000%3Alastupd%3D1702963573470; xptwj=qq:60c655ed12ee68048e72:54KBHIYpe8ioxt3yuo3GuRpC1WCSieQoP/HSZ4zw05vUNBqTEGjZylRV7Ee/gQKdw+4AR39Pu3s9w8UN6fC+xQaysEu/16eRafxBSlOhJPWWTVuI1BF3b300k0De/jcPtI82jkh9uGX2O4ufX9jSWMwn87b+BBU=; com.wm.reflector=\"reflectorid:0000000000000000000000@lastupd:1702963574000@firstcreate:1702955305178\"; xptwg=4227679326:558030DE400990:D7A337:D1486F6E:AEABFA5B:A48C033E:; TS01a90220=0178545c90c924ff084263c8cb94d8a93feb10ba5dcdaafa44d2d4bbee53181a33b55ab3c28c48628f7c56388b13afbe855a015aeb; bm_sv=60E1D169192E935CC6C00B0CDE127AEB~YAAQniLHF0lo0UKMAQAAEPaJgBa2H14Xw8z9j5FuLe+c+mOr9vtjWy+B1JIWMPOzFV3jf2sOwlAcIEVT7Zb0mGwx9GetlyN6e/PNbuNPGRK0tdoA/hf8gjCduTwLS8CC6z4no3eEnvY1ag+q3Dq0y4dikxDbYXvcfVujjYrVNac/v5cOfWcW40fMWhkJQX7hGxnZqmIyc+52KM8NeP9eIqEdybm44398ucnnT/mCybgp/Bn9xYLZwQBJSE7edA60x3A=~1; _px3=6e98f47b3f022f412f75d2fff334dd78b42e1c2da9dbb1ac36bcae228b1359cb:Vj9ElCDReT1PHAkl34AbO/7bhYH1iIhL+fexBxQwF/yJALJP177TUZi0QsuawJtQWCRpPtcCQy7FPqF2SO+Gpw==:1000:x43ZNK+0w/plDdsG3NfS1bezpRm0D1++lXrQuSJabg6/xCZEc7Ghp02iHShQZQWtATaAUPAyVYeZC2R8lPF2X7xPgH9OSstdEZ2g17OOrJ4Fv7E1MK6KZq0DiVn7FxyrcRAXcBHekyDMGiK6TTAugoF3QVB4LAQ7WM6kpnEbXlQLvndZW/IDW1JNGgVGCJvdRpV+VpqD6Ab7P2yv3UX4n+cRwoKWIa99iFjoZbmOuYM=; _pxde=3810374111437fdd342c82c0dbe956b1fb4871c6ade129e930a96584572ba38e:eyJ0aW1lc3RhbXAiOjE3MDI5NjM1NzUwNDJ9'\n",
    "    ]\n",
    "    \n",
    "    def get_page_number(url, cookie):\n",
    "        User = random.choice(User_Agent)\n",
    "        header = {\n",
    "            'User-Agent': User,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Cookie': random.choice(cookie),  # Replace with the actual Cookie\n",
    "            'Downlink': '10',\n",
    "            'Dpr': '1',\n",
    "            'Referer': url,\n",
    "            'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "            'Sec-Ch-Ua-Mobile': '?0',\n",
    "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        try:\n",
    "            user_agents = [\n",
    "                # Chrome on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Firefox on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "                # Safari on Mac\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Edge on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.48',\n",
    "                # Opera on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 OPR/77.0.4054.277',\n",
    "                # Chrome on Linux\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Firefox on Linux\n",
    "                'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "                # Chrome on Android\n",
    "                'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36',\n",
    "                # Safari on iOS\n",
    "                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1',\n",
    "                # Firefox on Android\n",
    "                'Mozilla/5.0 (Android 10; Mobile; rv:88.0) Gecko/88.0 Firefox/88.0',\n",
    "                # Opera on Android\n",
    "                'Mozilla/5.0 (Linux; Android 10; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36 OPR/64.2.3282.60115',\n",
    "                # Edge on Android\n",
    "                'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36 EdgA/46.6.4.5151',\n",
    "                # Samsung Browser on Android\n",
    "                'Mozilla/5.0 (Linux; Android 10; SAMSUNG SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/15.0 Chrome/91.0.4472.120 Mobile Safari/537.36',\n",
    "                # UC Browser on Android\n",
    "                'Mozilla/5.0 (Linux; U; Android 10; en-US; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/91.0.4472.120 UCBrowser/13.3.8.1305 Mobile Safari/537.36',\n",
    "                # Opera Mini on Android\n",
    "                'Opera/9.80 (Android; Opera Mini/58.0.2254/172.56; U; en) Presto/2.12.423 Version/12.16',\n",
    "                # BlackBerry Browser\n",
    "                'Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, Like Gecko) Version/6.0.0.337 Mobile Safari/534.1+',\n",
    "                # Internet Explorer 11 on Windows\n",
    "                'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',\n",
    "                # Edge Legacy on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/44.18362.449.0',\n",
    "                # Safari on macOS\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "                # Internet Explorer 10 on Windows\n",
    "                'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)',\n",
    "                # Safari on iPad\n",
    "                'Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1',\n",
    "                # Microsoft Edge (Chromium-based) on Windows\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.48',\n",
    "                # Silk Browser on Fire OS\n",
    "                'Mozilla/5.0 (Linux; U; Android 4.1.2; en-us; SCH-I535 4G Build/JZO54K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Silk/2.2',\n",
    "                # PlayStation 4 Browser\n",
    "                'Mozilla/5.0 (PlayStation 4 7.02) AppleWebKit/605.1.15 (KHTML, like Gecko)',\n",
    "                # Opera on macOS\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 OPR/77.0.4054.277',\n",
    "                # Brave Browser on macOS\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Brave/91.1.26.67',\n",
    "                # Chrome on iOS\n",
    "                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/91.0.4472.80 Mobile/15E148 Safari/604.1',\n",
    "                # Firefox on iOS\n",
    "                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/34.0 Mobile/15E148 Safari/605.1.15',\n",
    "                # Chrome on macOS\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Chrome on Chrome OS\n",
    "                'Mozilla/5.0 (X11; CrOS x86_64 14150.64.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "                # Chrome on BlackBerry\n",
    "                'Mozilla/5.0 (BB10; Touch) AppleWebKit/537.35+ (KHTML, like Gecko) Version/10.3.3.2205 Mobile Safari/537.35+',\n",
    "                # Chrome on Windows Phone\n",
    "                'Mozilla/5.0 (Windows Phone 10.0; Android 6.0.1; Microsoft; Lumia 950 XL) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36 Edge/40.15254.603',\n",
    "                # Firefox on Windows Phone\n",
    "                'Mozilla/5.0 (Windows Phone 10.0; Android 6.0.1; Microsoft; Lumia 950 XL) Gecko/20100101 Firefox/88.0',\n",
    "                # Samsung Internet on Android\n",
    "                'Mozilla/5.0 (Linux; Android 10; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/14.2 Chrome/91.0.4472.120 Mobile Safari/537.36',\n",
    "                # Chrome on KaiOS\n",
    "                'Mozilla/5.0 (Mobile; LYF/F30C/000JGJ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.210 Mobile Safari/537.36',\n",
    "                # Safari on tvOS\n",
    "                'Mozilla/5.0 (Apple TV; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/604.1',\n",
    "                # Silk Browser on Kindle Fire\n",
    "                'Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; KFTT Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30 Silk/3.17',\n",
    "                # Chrome on Oculus Browser\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 OculusBrowser/14.7.0 Mobile VR Safari/537.36',\n",
    "                # Firefox on Oculus Browser\n",
    "                'Mozilla/5.0 (X11; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0 OculusBrowser/14.7.0',\n",
    "                # Chrome on HoloLens\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 HoloLens/2.0.210325.1003 Safari/537.36',\n",
    "                # Safari on WatchOS\n",
    "                'Mozilla/5.0 (Apple Watch; CPU iPhone OS 8_2 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12D508 Safari/600.1.4',\n",
    "                # Firefox on Linux x86_64\n",
    "                'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "                # Chrome on Linux x86_64\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Chrome on macOS x86_64\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Chrome on Windows x86_64\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                # Safari on iOS x86_64\n",
    "                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1',\n",
    "                # Firefox on iOS x86_64\n",
    "                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/34.0 Mobile/15E148 Safari/605.1.15',\n",
    "            ]\n",
    "    \n",
    "            headers = {\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate, br',\n",
    "                'Referer': 'https://www.walmart.com/',  # Referer header might be required for some websites\n",
    "                'Connection': 'keep-alive',\n",
    "                'Cache-Control': 'max-age=0',\n",
    "            }\n",
    "    \n",
    "            # Choose a random user agent\n",
    "            headers['User-Agent'] = random.choice(user_agents)\n",
    "    \n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_number_elements = soup.find_all(\n",
    "                lambda tag: tag.name == 'a' and 'page-number' in tag.get('data-automation-id', ''))\n",
    "            print(\"response recorded\")\n",
    "            page_numbers = [int(element.text) for element in page_number_elements]\n",
    "    \n",
    "            if page_numbers:\n",
    "                last_page_number = max(page_numbers)\n",
    "                return last_page_number\n",
    "            else:\n",
    "                print(\"No page numbers found. Assuming only one page.\")\n",
    "                # return 1\n",
    "                return max(page_numbers)\n",
    "    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error encountered: {e}. Retrying in 20 seconds...\")\n",
    "            time.sleep(20)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 20 seconds...\")\n",
    "            time.sleep(20)\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def get_review_walmart(url, cookie):\n",
    "        extracted_reviews = []\n",
    "        retry_count = 0\n",
    "        header = {\n",
    "            'User-Agent': random.choice(User_Agent),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Cookie': random.choice(cookie),\n",
    "            'Downlink': '10',\n",
    "            'Dpr': '1',\n",
    "            'Referer': url,\n",
    "            'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "            'Sec-Ch-Ua-Mobile': '?0',\n",
    "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "    \n",
    "        sheets = \"api\"\n",
    "        api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "        api_key = api['API'][0]\n",
    "        api_key\n",
    "    \n",
    "        try:\n",
    "            url = f\"https://api.scrapingdog.com/scrape?dynamic=true&api_key={api_key}&url={url}\"\n",
    "            # url = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={link}\"\n",
    "            print(url)\n",
    "            response = requests.get(url, headers=header)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "            title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "            if title_all:\n",
    "                title = title_all.get('href')\n",
    "                pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "                model = re.findall(pattern, title)\n",
    "            #         model = re.search(r'\\b(\\d{4}e)\\b', title_all).group(1)\n",
    "            li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "    \n",
    "            if li_elements:\n",
    "                for li_tag in li_elements:\n",
    "                    product = {}\n",
    "                    product['Model'] = title\n",
    "                    product['Review rating'] = li_tag.select_one('.w_iUH7').text\n",
    "                    product['Verified Purchase or not'] = li_tag.select_one(\n",
    "                        '.pl2.green.b.f7.self-center').text if li_tag.select_one('.pl2.green.b.f7.self-center') else None\n",
    "                    product['Review date'] = li_tag.select_one('.f7.gray').text if li_tag.select_one('.f7.gray') else None\n",
    "    \n",
    "                    review_title_element = li_tag.select_one('h3.b')\n",
    "                    product['Review title'] = review_title_element.text if review_title_element else None\n",
    "    \n",
    "                    product['Review Content'] = li_tag.find('span', class_='tl-m mb3 db-m').text if li_tag.find('span',\n",
    "                                                                                                                class_='tl-m mb3 db-m') else None\n",
    "                    product['Review name'] = li_tag.select_one('.f6.gray').text if li_tag.select_one('.f6.gray') else None\n",
    "    \n",
    "                    syndication_element = li_tag.select_one('.b.ph1.dark.gray')\n",
    "                    product['Syndicated source'] = syndication_element.text if syndication_element else None\n",
    "                    product['URL'] = url\n",
    "    \n",
    "                    extracted_reviews.append(product)\n",
    "            elif \"Robot or human\" in response.text:\n",
    "                print()\n",
    "    \n",
    "    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error encountered: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "        return extracted_reviews\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    urls = [\n",
    "        'https://www.walmart.com/reviews/product/5129928602',\n",
    "        'https://www.walmart.com/reviews/product/5129928603'\n",
    "    ]\n",
    "    \n",
    "    # %%\n",
    "    import time\n",
    "    \n",
    "    walmart_reviews = []\n",
    "    \n",
    "    for link in urls:\n",
    "        # initial value don't modify\n",
    "        retry_count = 0\n",
    "    \n",
    "        # you can modify with your need\n",
    "        max_try = 5\n",
    "        retry_limit = max_try\n",
    "        print(link)\n",
    "        while retry_count < max_try:\n",
    "            try:\n",
    "                last_page_number = get_page_number(link, cookie)\n",
    "                if last_page_number is None:\n",
    "                    retry_count += 1\n",
    "                    if retry_count <= retry_limit:\n",
    "                        print(\"Failed to retrieve last page number. Retrying... Also Extract the data\")\n",
    "                        if retry_count == 1:\n",
    "                            for page_number in range(1, last_page_number + 1):\n",
    "                                retry_count = 0  # Reset retry count for each page\n",
    "                                while retry_count < max_try:\n",
    "                                    try:\n",
    "                                        target_url = f'{link}?page={page_number}'\n",
    "                                        extracted_reviews = get_review_walmart(target_url, cookie)\n",
    "    \n",
    "                                        if len(extracted_reviews) == 0:\n",
    "                                            print('No reviews found. Retrying in 5 seconds...')\n",
    "                                            retry_count += 1\n",
    "                                            time.sleep(5)\n",
    "                                        else:\n",
    "                                            walmart_reviews.extend(extracted_reviews)\n",
    "                                            print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                                            time.sleep(2)\n",
    "                                            break\n",
    "    \n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                                        retry_count += 1\n",
    "                                        time.sleep(3)\n",
    "                                else:\n",
    "                                    print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "    \n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"Failed to retrieve last page number after multiple retries. Changing the link.\")\n",
    "                        # Change the link here\n",
    "                        link = new_link\n",
    "                        retry_count = 0  # Reset retry count\n",
    "                        continue\n",
    "                print('Total pages:', last_page_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                time.sleep(3)\n",
    "            retry_count += 1\n",
    "        else:\n",
    "            print(\"Max retries exceeded for this link. Moving to the next link.\")\n",
    "            continue  # Move to the next link if max retries exceeded\n",
    "    \n",
    "        if last_page_number is None:\n",
    "            print(\"Skipping processing for this link due to inability to retrieve last page number.\")\n",
    "            continue  # Move to the next link if last_page_number is None\n",
    "    \n",
    "        for page_number in range(1, last_page_number + 1):\n",
    "            retry_count = 0  # Reset retry count for each page\n",
    "            while retry_count < max_try:\n",
    "                try:\n",
    "                    target_url = f'{link}?page={page_number}'\n",
    "                    extracted_reviews = get_review_walmart(target_url, cookie)\n",
    "    \n",
    "                    if len(extracted_reviews) == 0:\n",
    "                        print('No reviews found. Retrying in 5 seconds...')\n",
    "                        retry_count += 1\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        walmart_reviews.extend(extracted_reviews)\n",
    "                        print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                        time.sleep(2)\n",
    "                        break\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                    retry_count += 1\n",
    "                    time.sleep(3)\n",
    "            else:\n",
    "                print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    walmart = pd.DataFrame(walmart_reviews)\n",
    "    walmart['Retailer'] = \"Walmart\"\n",
    "    \n",
    "    walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "    walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "    walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "    walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "    walmart.drop_duplicates(inplace=True)\n",
    "    \n",
    "    walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "    \n",
    "    walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "    \n",
    "    walmart_hp_combine = pd.merge(walmart, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "    walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model']\n",
    "    \n",
    "    columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "    walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "    walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "    walmart_hp_combine['Country'] = 'US'\n",
    "    \n",
    "    column_mapping = {\n",
    "        'Review date': 'Review_Date',\n",
    "        'review_text': 'Review_Content',\n",
    "        'Review rating': 'Review_Rating',\n",
    "        'url': 'URL',\n",
    "        'review_title': 'Review_Title',\n",
    "        'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "        'reviewer_name': 'Review_Name',\n",
    "        'syndication': 'Syndicated_Source',\n",
    "        'stars': 'Review_Rating',\n",
    "        'Retailer': 'Retailer',\n",
    "        'scraping_date': 'Scraping_Date',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Review title': 'Review_Title',\n",
    "        'Review Content': 'Review_Content',\n",
    "        'Review date': 'Review_Date',\n",
    "        'URL': 'URL',\n",
    "        'Seeding or not': 'Seeding_Flag',\n",
    "        'Review name': 'Review_Name',\n",
    "        'People_find_helpful': 'People_Find_Helpful',\n",
    "        'Syndicated source': 'Syndicated_Source',\n",
    "        'Comp Model': 'Comp_Model',\n",
    "        'HP Class': 'HP_Class',\n",
    "        'Review Model': 'Review_Model',\n",
    "        'Competitor_Flag': 'Competitor_Flag'\n",
    "    }\n",
    "    \n",
    "    # Rename columns in the original DataFrame\n",
    "    walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "    \n",
    "    # Concatenate with an empty DataFrame\n",
    "    Final_review = pd.concat([pd.DataFrame(), walmart_hp_combine], ignore_index=True)\n",
    "    \n",
    "    # Add default values for some columns\n",
    "    Final_review['Country'] = 'US'\n",
    "    Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "    Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64', errors='ignore')\n",
    "    Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "    # Handle missing 'People_Find_Helpful' column\n",
    "    if 'People_Find_Helpful' in Final_review.columns:\n",
    "        Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    else:\n",
    "        Final_review['People_Find_Helpful'] = 0\n",
    "    \n",
    "    Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "    \n",
    "    # Fill NaN values in string columns with empty string\n",
    "    string_columns = Final_review.select_dtypes(include='object').columns\n",
    "    Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    required_columns = [\n",
    "        'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "        'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "        'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "        'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "        'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "        'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "        'Orginal_Title', 'Orginal Title'\n",
    "    ]\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in Final_review.columns:\n",
    "            Final_review[col] = None\n",
    "    \n",
    "    # Reorder columns to match the required_columns list\n",
    "    Final_review = Final_review[required_columns]\n",
    "    \n",
    "    previous = pd.read_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv')\n",
    "    previous['Review_Date'] = pd.to_datetime(previous['Review_Date']).dt.date\n",
    "    previous['Scraping_Date'] =  pd.to_datetime(previous['Scraping_Date']).dt.date\n",
    "    previous ['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clean_review(text):\n",
    "        text = str(text)\n",
    "    \n",
    "        cleaned_text = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', text).strip()\n",
    "        return cleaned_text\n",
    "    \n",
    "    #previous['Review_Content'] = previous['Review_Content'].apply(clean_review)\n",
    "    Final_review['Review_Content'] = Final_review['Review_Content'].apply(clean_review)\n",
    "    \n",
    "    # def extract_first_ten_words(row):\n",
    "    #     words = row.split()\n",
    "    #     return ''.join(words[:10])\n",
    "    \n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "    \n",
    "        # Remove non-English characters and punctuations\n",
    "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        # Remove extra whitespaces and convert to lowercase\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "        cleaned = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', cleaned_text)\n",
    "        english_words = re.findall(r'\\b[a-z]+\\b', cleaned)\n",
    "        first_ten_words = ''.join(english_words[:10])\n",
    "        return first_ten_words\n",
    "    \n",
    "    \n",
    "    # Print the total number of reviews\n",
    "    print('Total walmart review:', len(Final_review))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save to CSV\n",
    "    Final_review.to_csv('walmart.csv', index=False)\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv('walmart.csv')\n",
    "    \n",
    "    # Drop rows where the 'Review_Content' column is blank\n",
    "    df_cleaned = df.dropna(subset=['Review_Content'])\n",
    "    \n",
    "    # Save the cleaned DataFrame back to a CSV file\n",
    "    df_cleaned.to_csv('walmart.csv', index=False)\n",
    "    \n",
    "    # Read es.csv, uk.csv, and us.csv\n",
    "    es_data = pd.read_csv('es.csv')\n",
    "    uk_data = pd.read_csv('uk.csv')\n",
    "    us_data = pd.read_csv('us.csv')\n",
    "    \n",
    "    # Add columns \"Orginal_Review\" and \"Orginal_Title\" to uk_data\n",
    "    uk_data['Orginal_Review'] = \"\"\n",
    "    uk_data['Orginal_Title'] = \"\"\n",
    "    \n",
    "    # Concatenate es_data, uk_data, and us_data\n",
    "    combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "    \n",
    "    # Remove the column \"Orginal_Review\"\n",
    "    combined_df.drop(columns=['Orginal_Review'], inplace=True)\n",
    "    \n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv('amazon.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Function to read CSV without header\n",
    "    def read_csv_without_header(file_path, column_names):\n",
    "        df = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "        df.columns = column_names[:len(df.columns)]  # Assign column names based on the number of columns in the file\n",
    "        return df\n",
    "    \n",
    "    # List of files to merge\n",
    "    file_paths = ['amazon.csv', 'bestbuy.csv', 'walmart.csv']\n",
    "    \n",
    "    # Read the CSV files without headers\n",
    "    dfs = [read_csv_without_header(file_path, required_columns) for file_path in file_paths]\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Handle missing 'People_Find_Helpful' column\n",
    "    if 'People_Find_Helpful' in final_df.columns:\n",
    "        final_df['People_Find_Helpful'] = final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "    else:\n",
    "        final_df['People_Find_Helpful'] = 0\n",
    "    \n",
    "    \n",
    "    # Ensure all required columns are present and in the correct order\n",
    "    for col in required_columns:\n",
    "        if col not in final_df.columns:\n",
    "            final_df[col] = None\n",
    "    \n",
    "    final_df = final_df[required_columns]\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    final_df.to_csv('merged_reviews.csv', index=False)\n",
    "    \n",
    "    # Load the two CSV files\n",
    "    merged_reviews = pd.read_csv('merged_reviews.csv')\n",
    "    tassel_emea_review_raw = pd.read_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv')\n",
    "    \n",
    "    # Combine the data, keeping the header of Tassel_EMEA_Review_Raw.csv\n",
    "    combined_data = pd.concat([tassel_emea_review_raw, merged_reviews], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data to Tassel_EMEA_Review_Raw.csv\n",
    "    combined_data.to_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "    \n",
    "    # # # Remove the original CSV files\n",
    "    # # for file_path in file_paths:\n",
    "    # #     os.remove(file_path)\n",
    "    \n",
    "    df = pd.read_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv')\n",
    "    \n",
    "    # Function to get the first character of review content\n",
    "    def first_character(content):\n",
    "        if pd.isna(content):  # Check if content is NaN\n",
    "            return content\n",
    "        return content[:10]\n",
    "    \n",
    "    # Apply the function to create a new column with the first character\n",
    "    df['Review_content_first_char'] = df['Review_Content'].apply(first_character)\n",
    "    \n",
    "    # Identify duplicates based on 'Review_Name' and 'Review_content_first_char'\n",
    "    duplicates = df.duplicated(subset=['Review_Name', 'Review_content_first_char'], keep='first')\n",
    "    \n",
    "    # Keep the first occurrence of duplicates and rows with blank 'Review_Content'\n",
    "    df_no_duplicates = df[~(duplicates & ~df['Review_Content'].isnull())]\n",
    "    \n",
    "    # Drop the temporary column\n",
    "    df_no_duplicates = df_no_duplicates.drop(columns=['Review_content_first_char'])\n",
    "    \n",
    "    # Save the result to a new CSV file\n",
    "    df_no_duplicates.to_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "    \n",
    "    print('Tassel_raw_data_scraping completed. Tassel_raw file saved')\n",
    "\n",
    "## Make the full program into a function end\n",
    "\n",
    "\n",
    "\n",
    "## Use Try and except to handdle all the error start\n",
    "try:\n",
    "    program()\n",
    "except Exception as e:\n",
    "    \n",
    "    # Step 2: Log the exception details\n",
    "    logging.error(\"An error occurred\", exc_info=True)\n",
    "    \n",
    "    # Extract error details\n",
    "    error_type = type(e).__name__\n",
    "    error_message = str(e)\n",
    "    error_traceback = traceback.format_exc()\n",
    "\n",
    "    # Step 3: Save error details into CSV\n",
    "    log_error_to_csv(error_type, error_message, error_traceback)\n",
    "## Use Try and except to handdle all the error end\n",
    "print(\"Error has been logged and saved into the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08ae54-32f8-402c-9ee9-9e413f5ef693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
