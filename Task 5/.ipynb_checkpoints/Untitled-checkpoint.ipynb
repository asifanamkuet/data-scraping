{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d45591-128d-4a02-92af-f79e3591083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Timestamp: 2024-09-12 17:05:44\n",
      "Running MMK_raw_date_scraping.py\n",
      "https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format\n",
      "Page: 1 four star\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'traceback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 274\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage:\u001b[39m\u001b[38;5;124m'\u001b[39m,page, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m star\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 274\u001b[0m soup \u001b[38;5;241m=\u001b[39m get_soup_uk(url)  \u001b[38;5;66;03m# Get the soup object from the URL\u001b[39;00m\n\u001b[1;32m    275\u001b[0m extracted_reviews \u001b[38;5;241m=\u001b[39m amazon_review(soup, url)  \u001b[38;5;66;03m# Extract reviews from the soup\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 151\u001b[0m, in \u001b[0;36mget_soup_uk\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    150\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(req\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m file_name \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 313\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 313\u001b[0m     error_trace \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()  \u001b[38;5;66;03m# Gets the full traceback including line numbers\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mextract_tb(e\u001b[38;5;241m.\u001b[39m__traceback__)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull traceback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_trace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traceback' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "import urllib.parse\n",
    "import urllib3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Disable urllib3 warnings\n",
    "urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the timestamp\n",
    "print(\"Current Timestamp:\", timestamp)\n",
    "\n",
    "print('Running MMK_raw_date_scraping.py')\n",
    "\n",
    "# %%\n",
    "excel_file_path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheet_name = \"data_new\"\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "df_amazon\n",
    "\n",
    "# %%\n",
    "path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(path, sheet_name = sheets, engine='openpyxl')\n",
    "review_template\n",
    "\n",
    "# %% [markdown]\n",
    "# # Amazon\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Function\n",
    "\n",
    "# %%\n",
    "### Not use docker\n",
    "\n",
    "# header = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "#         'Accept-Encoding': 'gzip, deflate, br',\n",
    "#         'Accept-Language': 'en-US,en;q=0.9',\n",
    "#         'Cache-Control': 'max-age=0', \n",
    "#         'Downlink': '10',\n",
    "#         'Dpr': '1',\n",
    "#         'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "#         'Sec-Ch-Ua-Mobile': '?0',\n",
    "#         'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "#         'Sec-Fetch-Dest': 'document',\n",
    "#         'Sec-Fetch-Mode': 'navigate',\n",
    "#         'Sec-Fetch-Site': 'same-origin',\n",
    "#         'Sec-Fetch-User': '?1',\n",
    "#         'Upgrade-Insecure-Requests': '1'\n",
    "#     }\n",
    "\n",
    "# url ='https://www.amazon.com/HP-DeskJet-2755e-Wireless-Printer/product-reviews/B08XYP6BJV/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "# response = requests.get(url, headers=header)\n",
    "# response.raise_for_status()\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "# %%\n",
    "from datetime import datetime\n",
    "\n",
    "#docker section\n",
    "# def get_soup(url):   \n",
    "#     r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})  \n",
    "#     soup = BeautifulSoup(r.text, 'html.parser')  \n",
    "#     return soup  \n",
    "\n",
    "def get_soup(url):\n",
    "    headers = {\n",
    "        \"Host\": \"www.amazon.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    \n",
    "    cookies = {\n",
    "        \"id_pkel\": \"n1\",\n",
    "        \"session-id\": \"144-9199197-9117656\",\n",
    "        \"ubid-main\": \"135-8023421-1366807\",\n",
    "        \"session-token\": \"\\\"WjDVNZM+O7J0Uz5dF6X2d9g6ZDG1IVa60RPRNMP80QoFhsFXGdCzXQBirDiUy6GIC45zVsgkMMYoLujGjsVbwiLKfXWjf5OqKrPvEM8xcJKsopD+v4jxAMGRkqwNxf4y03hdKl7ADMBB+jMnq98IxeB7Ff0UDTQipORQfD86d5ufBZdCw9shOOS4PQeOaaNkxH2SpByFoUj0cfJo3Xobkd5Jtk+IawRueyn6AzOJWHxrOrLRnPfmK97xs576bI/ZP7Khzu5ceeba2GgywkXMcx8DmvPLOoB/Pa+xDFVs/QW/SYITb9Fnzo1oRa7kjQCMBANSU6ne4nfJzgeKFGy1VApsYVWZkacTOJ4jh3ngn25lof8jALAOug==\\\"\",\n",
    "        \"x-main\": \"\\\"S4E2GNCGB?gBCTzWoCC0218uFrS8EwJDQk6b2W3Adl4?cRgCm@dySpzpaLHSoSuC\\\"\",\n",
    "        \"at-main\": \"Atza|IwEBIE3-hffP2C1yNhcqx8qW0Fcc2mAck9cHK6aFfhQ78bAD0OtodTMq3n9zprEP6IB0i91VFyfbHRIlKbcGMqBOatvTdICIafKIJvWkloQcouLLUxw5cKvZCYzS0ykzPTX6som3E_4oe_7QanG-I4fcMfDRQy24q2seWuetb4wGPJ4ZJrjp-kVOA3yQkwp-TdoM37oHcdzcjTJuo1H1ZQSPteaNdrn2_JayWxnlW0DkdydlGA\",\n",
    "        \"sess-at-main\": \"\\\"Bp6VcqgGeR6LC8dWAI8cO5Pcas+MvRwocmHfyYTn8f4=\\\"\",\n",
    "        \"sst-main\": \"Sst1|PQGUMWlHWFFWVJQrDxhEjaAUCbsO4AzlBA1FxHpHo0wRpn3WQZvdHMAkeCvVVXiERIpb0CTzH2nJm9yxTrVWKa5w0IRP-C_riT6-lZviIWfah-k5EtGL4TYflylydylCDneaE_FV9HL7p799xYygYCCMfh_Ez1bAK6V3dqwIRUjiUPpZQyECWoppnBtkyJTUCUu94Ajefkw2rybfRsoAR6s805MTmwwv2Xi_Xdcqspo0mmp-60OngDc_8SgmxMDCVOAEiToEVuET_bkTbqizJuo0H1SIdJ8oo1XcoafaEB9l0Yo\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"i18n-prefs\": \"USD\",\n",
    "        \"lc-main\": \"en_US\",\n",
    "        \"csm-hit\": \"tb:s-ANP47MY7T2WVGPBX5308|1724236316404&t:1724236316472&adb:adblk_no\"\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def get_soup_uk(url):\n",
    "    headers = {\n",
    "        \"Host\": \"www.amazon.co.uk\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    \n",
    "    cookies = {\n",
    "        \"id_pkel\": \"n1\",\n",
    "        \"session-id\": \"144-9199197-9117656\",\n",
    "        \"ubid-main\": \"135-8023421-1366807\",\n",
    "        \"session-token\": \"\\\"WjDVNZM+O7J0Uz5dF6X2d9g6ZDG1IVa60RPRNMP80QoFhsFXGdCzXQBirDiUy6GIC45zVsgkMMYoLujGjsVbwiLKfXWjf5OqKrPvEM8xcJKsopD+v4jxAMGRkqwNxf4y03hdKl7ADMBB+jMnq98IxeB7Ff0UDTQipORQfD86d5ufBZdCw9shOOS4PQeOaaNkxH2SpByFoUj0cfJo3Xobkd5Jtk+IawRueyn6AzOJWHxrOrLRnPfmK97xs576bI/ZP7Khzu5ceeba2GgywkXMcx8DmvPLOoB/Pa+xDFVs/QW/SYITb9Fnzo1oRa7kjQCMBANSU6ne4nfJzgeKFGy1VApsYVWZkacTOJ4jh3ngn25lof8jALAOug==\\\"\",\n",
    "        \"x-main\": \"\\\"S4E2GNCGB?gBCTzWoCC0218uFrS8EwJDQk6b2W3Adl4?cRgCm@dySpzpaLHSoSuC\\\"\",\n",
    "        \"at-main\": \"Atza|IwEBIE3-hffP2C1yNhcqx8qW0Fcc2mAck9cHK6aFfhQ78bAD0OtodTMq3n9zprEP6IB0i91VFyfbHRIlKbcGMqBOatvTdICIafKIJvWkloQcouLLUxw5cKvZCYzS0ykzPTX6som3E_4oe_7QanG-I4fcMfDRQy24q2seWuetb4wGPJ4ZJrjp-kVOA3yQkwp-TdoM37oHcdzcjTJuo1H1ZQSPteaNdrn2_JayWxnlW0DkdydlGA\",\n",
    "        \"sess-at-main\": \"\\\"Bp6VcqgGeR6LC8dWAI8cO5Pcas+MvRwocmHfyYTn8f4=\\\"\",\n",
    "        \"sst-main\": \"Sst1|PQGUMWlHWFFWVJQrDxhEjaAUCbsO4AzlBA1FxHpHo0wRpn3WQZvdHMAkeCvVVXiERIpb0CTzH2nJm9yxTrVWKa5w0IRP-C_riT6-lZviIWfah-k5EtGL4TYflylydylCDneaE_FV9HL7p799xYygYCCMfh_Ez1bAK6V3dqwIRUjiUPpZQyECWoppnBtkyJTUCUu94Ajefkw2rybfRsoAR6s805MTmwwv2Xi_Xdcqspo0mmp-60OngDc_8SgmxMDCVOAEiToEVuET_bkTbqizJuo0H1SIdJ8oo1XcoafaEB9l0Yo\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"i18n-prefs\": \"USD\",\n",
    "        \"lc-main\": \"en_US\",\n",
    "        \"csm-hit\": \"tb:s-ANP47MY7T2WVGPBX5308|1724236316404&t:1724236316472&adb:adblk_no\"\n",
    "    }\n",
    "\n",
    "    req = requests.get(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "   file_name = str(random.randint(1, 1000)) + \".html\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(soup)\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amazon_review(soup, url):    \n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "\n",
    "    # NPI lanched in 2024-01-15\n",
    "    date_string = \"2024-01-15\"\n",
    "    min_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "\n",
    "    for item in reviews:    \n",
    "        review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "        review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "        if review_date < min_date:\n",
    "            print('Review date is less than 2024-01-15')\n",
    "            break\n",
    "    \n",
    "        review = {    \n",
    "            'Model': model,    \n",
    "            'Review date': review_date,     \n",
    "            \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        \n",
    "  \n",
    "        try:    \n",
    "            review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "        except AttributeError:    \n",
    "            review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "  \n",
    "        try:    \n",
    "            review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "  \n",
    "        try:    \n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        try:      \n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "        except AttributeError:        \n",
    "            review[\"Review name\"] = None  \n",
    "  \n",
    "        try:    \n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"People_find_helpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            if seeding:\n",
    "               review['Seeding or not'] = seeding\n",
    "            else:\n",
    "                raise AttributeError\n",
    "        except AttributeError:  \n",
    "            try: \n",
    "                review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, text='Vine Customer Review of Free Product')\n",
    "\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "\n",
    "        try:\n",
    "            review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "             review['Aggregation'] = None\n",
    "    \n",
    "  \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "  \n",
    "    return extracted_reviews\n",
    "\n",
    "# %% [markdown]\n",
    "# ## HP Review\n",
    "\n",
    "# %%\n",
    "urls = ['https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format' ]\n",
    "            # 'https://www.amazon.com/product-reviews/B0CFM7BTW8/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format'\n",
    "        # 'https://www.amazon.com/product-reviews/B0CFM82NS2/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        # 'https://www.amazon.com/product-reviews/B0CFM7VJNK/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        # 'https://www.amazon.com/product-reviews/B0CFM8KL9G/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        # 'https://www.amazon.com/product-reviews/B0CFM94G5H/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format',\n",
    "        # 'https://www.amazon.co.uk/product-reviews/B0CJY274VN/ref=cm_cr_arp_d_viewopt_sr?formatType=current_format'\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['four','five'] \n",
    "max_retry_attempts = 20\n",
    "all_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for page in range(1, 11):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url = f'{link}&filterByStar={y}_star&pageNumber={page}&sortBy=recent'  \n",
    "                    print('Page:',page, f'{y} star')\n",
    "                    soup = get_soup_uk(url)  # Get the soup object from the URL\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                            print('No review')  \n",
    "                            found_reviews = False\n",
    "                            break \n",
    "                    \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews)\n",
    "                        print(f\"Page {page} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if page >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {page} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {page} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    error_trace = traceback.format_exc()  # Gets the full traceback including line numbers\n",
    "                    print(f\"An error occurred on line {traceback.extract_tb(e.__traceback__)[-1].lineno}: {str(e)}\")\n",
    "                    print(f\"Full traceback: {error_trace}\")\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None)\n",
    "amazon2= pd.DataFrame(all_reviews)\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "# amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "amazon_hp_combine\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Convert to dataframe\n",
    "\n",
    "# %%\n",
    "amazon_final = amazon_hp_combine \n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'US'\n",
    "amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Aggregation':'Aggregation_Flag',\n",
    "    'Country': 'Country'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "amazon_final_df.head()\n",
    "\n",
    "# amazon_final_df.to_csv(r'amazon_review.csv',index = False)\n",
    "\n",
    "# %%\n",
    "final_review = pd.concat([review_template, amazon_final_df])\n",
    "final_review.head()\n",
    "\n",
    "# %%\n",
    "# # Query previous amazon review\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# server = 'SQL-Cluster01.ijp.sgp.rd.hpicorp.net'\n",
    "# database = 'STAR_Rating'\n",
    "# schema = 'dbo'\n",
    "# driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# # dataframe = amazon_final_df\n",
    "# table = \"Ink_web_reviews\"\n",
    "\n",
    "# engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    "\n",
    "\n",
    "# existing_rows_query = f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM {schema}.{table}\n",
    "#     WHERE Retailer in ('Amazon')\n",
    "# \"\"\"\n",
    "# result_df = pd.read_sql_query(existing_rows_query, engine)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# result_df['Review_Date'] = result_df['Review_Date'].dt.date\n",
    "\n",
    "# non_duplicated_df = amazon_final_df[(~amazon_final_df['Review_Date'].isin(result_df['Review_Date']))&\n",
    "#                                    (~amazon_final_df['Review_Content'].isin(result_df['Review_Content']))].drop_duplicates()\n",
    "# non_duplicated_df\n",
    "\n",
    "# %%\n",
    "# from sqlalchemy import create_engine, text\n",
    " \n",
    "# server = 'SQL-Cluster01.ijp.sgp.rd.hpicorp.net'\n",
    "# database = 'STAR_Rating'\n",
    "# schema = 'dbo'\n",
    "# driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# dataframe = non_duplicated_df\n",
    "# table = \"Ink_web_reviews\"\n",
    "\n",
    "# engine = create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}\", echo=True)\n",
    " \n",
    "# chunk_size = 10000\n",
    "# total_rows = len(dataframe)\n",
    "# num_chunk = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# for i in range(num_chunk):\n",
    "#     start_index = i * chunk_size\n",
    "#     end_index = (i + 1) * chunk_size\n",
    "#     chunk = dataframe.iloc[start_index:end_index]\n",
    "    \n",
    "#     chunk.to_sql(table, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "#     print(f\"Chunk {i+1}/{num_chunk} saved to SQL.\")\n",
    "\n",
    "# %%\n",
    "# ## Change column setting (Rating to one decimal place)\n",
    "# update_query = f'''\n",
    "#  ALTER TABLE dbo.Ink_web_reviews\n",
    "#  ALTER COLUMN Review_Date DATE\n",
    "#  '''\n",
    "# with engine.connect() as connection:\n",
    "#         connection.execute(update_query)\n",
    "\n",
    "# print(\"Precision updated in SQL table.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Bestbuy\n",
    "\n",
    "# %%\n",
    "from datetime import datetime\n",
    "def get_review_bestbuy(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.61',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "def bestbuy_review(soup, url):    \n",
    "    bestbuy = {}\n",
    "    bestbuy_reviews = []  \n",
    "    Model = soup.find(\"h1\", {'class':\"heading-5 v-fw-regular\"})\n",
    "    if not Model:\n",
    "        Model = soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"})\n",
    "    Model = Model.text if Model else None\n",
    "    \n",
    "        \n",
    "    npi = soup.find('span',{'class':'c-reviews order-2'} ).text\n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "    if review_session:\n",
    "        for item in review_session:    \n",
    "            bestbuy = {    \n",
    "                 'Model':Model,\n",
    "                # 'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "                'URL':url \n",
    "            }\n",
    "            try:    \n",
    "                bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review title']  = None\n",
    "\n",
    "            try:\n",
    "                bestbuy['Review_Name']  = item.find(\"div\", {\"class\": \"ugc-author v-fw-medium body-copy-lg\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review_Name']  = None\n",
    "                \n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review rating']  = None\n",
    "\n",
    "            review_date_element = item.find(\"time\", {\"class\": \"submission-date\"})\n",
    "            if review_date_element:\n",
    "                review_date_string = review_date_element['title']\n",
    "                review_date_datetime = datetime.strptime(review_date_string, '%b %d, %Y %I:%M %p')\n",
    "                formatted_review_date = review_date_datetime.strftime('%Y-%m-%d')\n",
    "                bestbuy['Review_Date'] = formatted_review_date\n",
    "            else:\n",
    "                bestbuy['Review_Date'] = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review promotion']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review aggregation']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Content']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Recommendation']  = None\n",
    "\n",
    "            try:    \n",
    "                network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "                if network_badge:\n",
    "                    bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "                else:\n",
    "                    bestbuy['Seeding or not']  = \"\"\n",
    "            except AttributeError:    \n",
    "                bestbuy['Seeding or not']  = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_helpful']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_unhelpful']  = None\n",
    "\n",
    "\n",
    "            bestbuy_reviews.append(bestbuy)    \n",
    "        \n",
    "    \n",
    "  \n",
    "    return npi, bestbuy_reviews \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Best buy hp\n",
    "\n",
    "# %%\n",
    "# ### Novellie\n",
    "# urls = [\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-envy-inspire-7255e-wireless-all-in-one-inkjet-photo-printer-with-3-months-of-instant-ink-included-with-hp-white-sandstone/6492187?variant=A&sort=MOST_RECENT',\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-envy-inspire-7955e-wireless-all-in-one-inkjet-photo-printer-with-3-months-of-instant-ink-included-with-hp-white-sandstone/6478251?variant=A&sort=MOST_RECENT'\n",
    "\n",
    "# ]\n",
    "\n",
    "# %%\n",
    "urls = ['https://www.bestbuy.com/site/reviews/hp-officejet-pro-8135e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565476?variant=A'\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9125e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565475?variant=A',\n",
    "# 'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9135e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6565473?variant=A',\n",
    "#         'https://www.bestbuy.com/site/reviews/hp-officejet-pro-8139e-wireless-all-in-one-inkjet-printer-with-12-months-of-instant-ink-included-with-hp-white/6565474?variant=A',\n",
    "#         'https://www.bestbuy.com/site/reviews/hp-officejet-pro-9730e-wireless-all-in-one-wide-format-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6578444?variant=A'\n",
    "]\n",
    "\n",
    "# %%\n",
    "max_attempts = 5\n",
    "bestbuy_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    should_continue = True\n",
    "    attempt_count = 0  # Counter for attempts\n",
    "    for x in range(1, 100):\n",
    "        if not should_continue:\n",
    "            break\n",
    "        while True:\n",
    "            url = f'{link}&page={x}'\n",
    "            try:\n",
    "                soup = get_review_bestbuy(url)\n",
    "                npi, reviews = bestbuy_review(soup, url)\n",
    "                if npi == 'Be the first to write a review':\n",
    "                    should_continue = False\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')\n",
    "                bestbuy_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-disabled\": \"true\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                if x > 1 and next_page_link and next_page_link.get(\"aria-disabled\") == \"true\":\n",
    "                    should_continue = False\n",
    "                    print('No more pages left')\n",
    "                    break\n",
    "\n",
    "                if len(reviews) < 20:\n",
    "                    should_continue = False\n",
    "                    print('Only 1 page')\n",
    "                    break\n",
    "                else:\n",
    "                    break \n",
    "            except Exception as e:\n",
    "                attempt_count += 1\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds... (Attempt {attempt_count}/{max_attempts})\")\n",
    "                if attempt_count >= max_attempts:\n",
    "                    print(\"Maximum number of attempts reached. Exiting loop.\")\n",
    "                    should_continue = False\n",
    "                    break\n",
    "                time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "review = pd.DataFrame(bestbuy_reviews)\n",
    "review['Retailer']=\"Best Buy\"\n",
    "review['scraping_date'] = pd.to_datetime(date.today())\n",
    "\n",
    "review['HP Model Number'] = review['Model'].str.extract(r'(\\d+e*)')\n",
    "\n",
    "hp_combine = pd.merge(review, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "\n",
    "hp_combine['Review Model'] = hp_combine['HP Model'] \n",
    "hp_combine['People_find_helpful'] = hp_combine['People_find_helpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "hp_combine['People_find_unhelpful'] = hp_combine['People_find_unhelpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "\n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "hp_combine_bestbuy = hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "hp_combine_bestbuy = hp_combine_bestbuy.drop_duplicates()\n",
    "\n",
    "hp_combine_bestbuy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "bestbuy_final = hp_combine_bestbuy\n",
    "bestbuy_final.drop_duplicates(inplace = True)\n",
    "\n",
    "bestbuy_final = bestbuy_final.sort_values(by = ['Review Model', 'Review title', 'Review Content', 'scraping_date'])\n",
    "\n",
    "bestbuy_final['Competitor_Flag'] = bestbuy_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "bestbuy_final['Country'] = 'US'\n",
    "\n",
    "bestbuy_final_version= bestbuy_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review_Date': 'Review_Date',\n",
    "    # 'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Review promotion': 'Promotion_Flag',\n",
    "    'Review aggregation': 'Aggregation_Flag'\n",
    "})\n",
    "\n",
    "bestbuy_final_version.drop(columns = ['Review Recommendation',  'People_find_unhelpful'],inplace = True)\n",
    "# bestbuy_final_version.to_csv('Bestbuy_NPI_review.csv',index = False)\n",
    "\n",
    "\n",
    "# %%\n",
    "Final_review = pd.concat([final_review, bestbuy_final_version], ignore_index = True)\n",
    "# Final_review = pd.concat([review_template, bestbuy_final_version], ignore_index = True)\n",
    "Final_review\n",
    "\n",
    "# %%\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review['Scraping_Date'] =  pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "Final_review.info()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Staple\n",
    "\n",
    "def max_pages(sku, url):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "\n",
    "    # Create payload as a dictionary\n",
    "    payload = {\n",
    "        'tenantType': 'StaplesDotCom',\n",
    "        'sku': sku,\n",
    "        'offset': 0,\n",
    "        'limit': 20,\n",
    "        'includeRelated': 'false',\n",
    "        'filterByRating': 0,\n",
    "        'relatedOnly': 'false',\n",
    "        'includeRatingOnlyReviews': 'true',\n",
    "        'filterByPhotos': 'false',\n",
    "        'sortBy': 'date',\n",
    "        'sortOrder': 'desc'\n",
    "    }\n",
    "\n",
    "    # Convert the payload dictionary to a query string\n",
    "    query_string = urllib.parse.urlencode(payload)\n",
    "\n",
    "    # Construct the full URL with the query string\n",
    "    full_url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "    # Set headers for the GET request\n",
    "    headers = {\n",
    "        'Referer': url\n",
    "        # Add any other necessary headers here\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(full_url, headers=headers, verify=False)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content\n",
    "        data = response.json()\n",
    "        review_count = data['reviewList']['total']\n",
    "        max_pages = math.ceil(review_count / 20)\n",
    "        return max_pages\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def staple_review(url, sku, max_pages):\n",
    "    base_url = 'https://www.staples.com/sdc/ptd/api/reviewProxy/getReviews'\n",
    "\n",
    "    headers = {\n",
    "        'Referer': url\n",
    "        # Add any other necessary headers here\n",
    "    }\n",
    "\n",
    "    all_data = []  # List to store data from all pages\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        payload = {\n",
    "            'tenantType': 'StaplesDotCom',\n",
    "            'sku': sku,\n",
    "            'offset': (page - 1) * 20,\n",
    "            'limit': 20,\n",
    "            'includeRelated': 'false',\n",
    "            'filterByRating': 0,\n",
    "            'relatedOnly': 'false',\n",
    "            'includeRatingOnlyReviews': 'true',\n",
    "            'filterByPhotos': 'false',\n",
    "            'sortBy': 'date',\n",
    "            'sortOrder': 'desc'\n",
    "        }\n",
    "\n",
    "        # Convert the payload dictionary to a query string\n",
    "        query_string = urllib.parse.urlencode(payload)\n",
    "\n",
    "        # Construct the full URL with the query string\n",
    "        full_url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "        # Make the GET request with SSL verification disabled\n",
    "        response = requests.get(full_url, headers=headers, verify=False)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON content\n",
    "            data = response.json()\n",
    "            reviews = data['reviewList']['reviews']\n",
    "            df = pd.DataFrame(reviews)\n",
    "            all_data.append(df)\n",
    "            # print('Reviews count on page', page, ':', len(df))\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code} on page {page}\")\n",
    "\n",
    "    # print('Total Reviews count:', sum(len(df) for df in all_data))\n",
    "\n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    result_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# %%\n",
    "def extract_sku_from_url(url):\n",
    "    # Use regular expression to extract numeric value from the end of the URL\n",
    "    match = re.search(r'/(\\d+)$', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# ### Novellie\n",
    "# urls = ['https://www.staples.com/ptd/review/24514342',\n",
    "# 'https://www.staples.com/ptd/review/24503811']\n",
    "\n",
    "urls = ['https://www.staples.com/ptd/review/24583383',\n",
    "'https://www.staples.com/ptd/review/24583386',\n",
    "'https://www.staples.com/ptd/review/24583387',\n",
    "       'https://www.staples.com/ptd/review/24583384'\n",
    "       'https://www.staples.com/ptd/review/24583385']\n",
    "\n",
    "### 3M \n",
    "      # 'https://www.staples.com/ptd/review/24532269',\n",
    "      #   'https://www.staples.com/ptd/review/24455371',\n",
    "      #  'https://www.staples.com/ptd/review/24455373',\n",
    "      #  'https://www.staples.com/ptd/review/24455374' ]\n",
    "\n",
    "# %%\n",
    "staples_df_hp = pd.DataFrame()\n",
    "\n",
    "for url in urls:\n",
    "    sku = extract_sku_from_url(url)\n",
    "    print('Get reviews from', url)\n",
    "    print('Total pages',max_pages(sku,url))\n",
    "    page = max_pages(sku,url)\n",
    "    data = staple_review(url, sku, page)\n",
    "    print('Total Reviews scraped:', len(data))\n",
    "    if data is not None:\n",
    "        staples_df_hp = pd.concat([staples_df_hp, data], axis=0)\n",
    "        \n",
    "\n",
    "# %%\n",
    "def extract_source_name(user_dict):\n",
    "    if isinstance(user_dict, dict):\n",
    "        return user_dict.get('sourceName', '')\n",
    "    else:\n",
    "        return ''\n",
    "if 'syndication' in staples_df_hp.columns:    \n",
    "    staples_df_hp['syndication'] = staples_df_hp['syndication'].apply(extract_source_name)\n",
    "staple_final = staples_df_hp[staples_df_hp['published'] == True]\n",
    "staple_final['Model'] = staple_final['catalogItems'].apply(lambda x: x[0]['title'] if x else None)\n",
    "if 'syndication' in staple_final.columns:\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating',  'user', 'syndication', 'incentivized', 'Model']]\n",
    "else:\n",
    "    # If the condition is not met, exclude the 'syndication' column\n",
    "    staple_final = staple_final[['id', 'dateCreated', 'title', 'text', 'rating', 'user', 'incentivized', 'Model']]\n",
    "\n",
    "def tidy_up_user(user):\n",
    "    if isinstance(user, dict) and 'nickName' in user:\n",
    "        return user['nickName']\n",
    "    else:\n",
    "        return 'blank'\n",
    "    \n",
    "\n",
    "    \n",
    "staple_final['user'] = staple_final['user'].apply(tidy_up_user)\n",
    "\n",
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "staple_final['Retailer']=\"Staples\"\n",
    "staple_final['scraping_date'] =pd.to_datetime(date.today())\n",
    "staple_final['HP Model Number'] = staple_final['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "# staple_final['Review date'] = pd.to_datetime(staple['Review date'])\n",
    "\n",
    "staple_hp_combine = pd.merge(staple_final, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "staple_hp_combine['Review Model'] = staple_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model','id']  \n",
    "  \n",
    "staple_hp_combine.drop(columns_to_drop, axis = 1,inplace = True) \n",
    "\n",
    "\n",
    "staple_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'dateCreated': 'Review_Date',\n",
    "    'text': 'Review_Content',\n",
    "    # 'URL': 'URL',\n",
    "    'title': 'Review_Title',\n",
    "    # 'Response name': 'Response_Name',\n",
    "    # 'Response text': 'Response_Text',\n",
    "    # 'Response date': 'Response_Date',\n",
    "    'incentivized': 'Seeding_Flag',\n",
    "    'user': 'Review_Name',\n",
    "    # 'People_find_helpful': 'People_Find_Helpful',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'rating': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    # 'Segment': 'Segment',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model'\n",
    "    # 'reviewedDate': 'Review_Date'\n",
    "    # 'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "staple_final = staple_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "staple_final['Competitor_Flag'] = staple_final['Review_Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "# staple_final.drop('id', axis = 1) \n",
    "staple_final.drop_duplicates(inplace = True)\n",
    "staple_final['Review_Date'] = pd.to_datetime(staple_final['Review_Date']).dt.date\n",
    "\n",
    "# %%\n",
    "Final_review_all = pd.concat([Final_review, staple_final], ignore_index=True)\n",
    "pattern = r'(\\w+ \\d{1,2}, \\d{4})'\n",
    "Final_review_all['Response_Date'] = Final_review_all['Response_Date'].fillna('').str.extract(pattern)\n",
    "Final_review_all['Response_Date'] = pd.to_datetime(Final_review_all['Response_Date'],errors = 'coerce').dt.date.fillna('')\n",
    "Final_review_all['Review_Date'] = pd.to_datetime(Final_review_all['Review_Date']).dt.date\n",
    "Final_review_all['Review_Rating'] = Final_review_all['Review_Rating'].astype('int64')\n",
    "Final_review_all['People_Find_Helpful'] = Final_review_all['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review_all['Scraping_Date'] =  pd.to_datetime(Final_review_all['Scraping_Date']).dt.date\n",
    "Final_review_all.sort_values(by = ['Review_Date'], ascending=False)\n",
    "Final_review_filter = Final_review_all\n",
    "Final_review_filter ['Review_Date'] = pd.to_datetime(Final_review_filter['Review_Date']).dt.date\n",
    "Final_review_filter ['Review_Rating'] = Final_review_filter['Review_Rating'].astype(int)\n",
    "Final_review_filter['Review_Rating_Label'] = Final_review_filter['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "Final_review_filter\n",
    "\n",
    "# %%\n",
    "previous = pd.read_csv(r\"C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\MMK\\MMK_web_review_raw data.csv\")\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date']).dt.date\n",
    "previous['People_Find_Helpful'] = previous['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "previous['Scraping_Date'] =  pd.to_datetime(previous['Scraping_Date']).dt.date\n",
    "previous ['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "def extract_first_ten_words(row):\n",
    "    words = row.split()\n",
    "    return ''.join(words[:10])\n",
    "\n",
    "# Apply the function to create a new column\n",
    "# previous['FirstTenWords'] = previous['Review_Content'].fillna(\"\").apply(extract_first_ten_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned_text)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "previous['FirstTenWords'] = previous['Review_Content'].fillna(0).apply(clean_text)\n",
    "\n",
    "Final_review_filter['FirstTenWords'] = Final_review_filter['Review_Content'].fillna(0).apply(clean_text)\n",
    "df_concat = pd.concat([previous, Final_review_filter],ignore_index=True)\n",
    "df_concat.sort_values(by = ['Review_Date','Review_Model','Retailer','FirstTenWords','Scraping_Date'],inplace = True)\n",
    "df_concat_final = df_concat.drop_duplicates(subset=['Review_Model', 'Retailer',  'Review_Date','Review_Rating', 'FirstTenWords'], keep='first')\n",
    "df_concat_final.sort_values(by = ['Scraping_Date','Review_Date'], ascending=False)\n",
    "df_concat_final['Scraping_Date'] = pd.to_datetime(df_concat_final['Scraping_Date']).dt.date\n",
    "# df_concat_final.loc[:,'Scraping_Date'] = pd.to_datetime(df_concat_final['Scraping_Date']).dt.date\n",
    "df_concat_final.drop(columns = 'FirstTenWords',inplace = True)\n",
    "df_concat_final.head()\n",
    "\n",
    "# %%\n",
    "df_concat_final.to_csv(r\"C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\MMK\\MMK_web_review_raw data.csv\", index = False)\n",
    "\n",
    "print('MMK_raw_data_scraping completed. MMK_raw file saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3af0a-e184-4bda-acab-47277e00d46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
