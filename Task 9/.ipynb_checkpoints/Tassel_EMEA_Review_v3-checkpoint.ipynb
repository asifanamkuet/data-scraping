{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab9aac2-0671-43bb-bb0b-a92287badf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Timestamp: 2024-10-30 21:43:43\n",
      "Running Tassel_raw_date_scraping.py\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=one_star&sortBy=recent\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=one_star&sortBy=recent\n",
      "Page: 3 one star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=one_star&sortBy=recent\n",
      "Page: 4 one star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=one_star&sortBy=recent\n",
      "Page: 5 one star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=one_star&sortBy=recent\n",
      "Page: 6 one star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=one_star&sortBy=recent\n",
      "Page: 7 one star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=one_star&sortBy=recent\n",
      "Page: 8 one star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=one_star&sortBy=recent\n",
      "Page: 9 one star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=one_star&sortBy=recent\n",
      "Page: 10 one star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=two_star&sortBy=recent\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=two_star&sortBy=recent\n",
      "Page: 3 two star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=two_star&sortBy=recent\n",
      "Page: 4 two star\n",
      "Page 4 scraped 4 reviews\n",
      "No more pages left\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=three_star&sortBy=recent\n",
      "Page: 1 three star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=three_star&sortBy=recent\n",
      "Page: 3 three star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=three_star&sortBy=recent\n",
      "Page: 4 three star\n",
      "Page 4 scraped 9 reviews\n",
      "No more pages left\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=four_star&sortBy=recent\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=four_star&sortBy=recent\n",
      "Page: 3 four star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=four_star&sortBy=recent\n",
      "Page: 4 four star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=four_star&sortBy=recent\n",
      "Page: 5 four star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=four_star&sortBy=recent\n",
      "Page: 6 four star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=four_star&sortBy=recent\n",
      "Page: 7 four star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=four_star&sortBy=recent\n",
      "Page: 8 four star\n",
      "Page 8 scraped 3 reviews\n",
      "No more pages left\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=five_star&sortBy=recent\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=five_star&sortBy=recent\n",
      "Page: 3 five star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=five_star&sortBy=recent\n",
      "Page: 4 five star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=five_star&sortBy=recent\n",
      "Page: 5 five star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=five_star&sortBy=recent\n",
      "Page: 6 five star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=five_star&sortBy=recent\n",
      "Page: 7 five star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=five_star&sortBy=recent\n",
      "Page: 8 five star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=five_star&sortBy=recent\n",
      "Page: 9 five star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=five_star&sortBy=recent\n",
      "Page: 10 five star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.es/HP-DeskJet-2820e-Impresora-Multifunci%C3%B3n/product-reviews/B0CFFWJHMF/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format\n",
      "Page: 1 one star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 one star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 one star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 one star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 one star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 one star\n",
      "Page 6 scraped 10 reviews\n",
      "Page: 7 one star\n",
      "Page 7 scraped 10 reviews\n",
      "Page: 8 one star\n",
      "Page 8 scraped 10 reviews\n",
      "Page: 9 one star\n",
      "Page 9 scraped 10 reviews\n",
      "Page: 10 one star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 two star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 two star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 two star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 two star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 two star\n",
      "Page 6 scraped 10 reviews\n",
      "Page: 7 two star\n",
      "Page 7 scraped 10 reviews\n",
      "Page: 8 two star\n",
      "Page 8 scraped 5 reviews\n",
      "No more pages left\n",
      "Page: 1 three star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 three star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 three star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 three star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 three star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 three star\n",
      "Page 6 scraped 10 reviews\n",
      "Page: 7 three star\n",
      "Page 7 scraped 10 reviews\n",
      "Page: 8 three star\n",
      "Page 8 scraped 10 reviews\n",
      "Page: 9 three star\n",
      "Page 9 scraped 10 reviews\n",
      "Page: 10 three star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 four star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 four star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 four star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 four star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 four star\n",
      "Error encountered: 'NoneType' object has no attribute 'split'. Retrying in 3 seconds...\n",
      "Page: 7 four star\n",
      "Error encountered: 'NoneType' object has no attribute 'split'. Retrying in 3 seconds...\n",
      "Page: 8 four star\n",
      "Page 8 scraped 10 reviews\n",
      "Page: 9 four star\n",
      "Page 9 scraped 10 reviews\n",
      "Page: 10 four star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "Page: 2 five star\n",
      "Page 2 scraped 10 reviews\n",
      "Page: 3 five star\n",
      "Page 3 scraped 10 reviews\n",
      "Page: 4 five star\n",
      "Page 4 scraped 10 reviews\n",
      "Page: 5 five star\n",
      "Page 5 scraped 10 reviews\n",
      "Page: 6 five star\n",
      "Page 6 scraped 10 reviews\n",
      "Page: 7 five star\n",
      "Page 7 scraped 10 reviews\n",
      "Page: 8 five star\n",
      "Page 8 scraped 10 reviews\n",
      "Page: 9 five star\n",
      "Page 9 scraped 10 reviews\n",
      "Page: 10 five star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=one_star&sortBy=recent\n",
      "Page: 1 one star\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STARK\\AppData\\Local\\Temp\\ipykernel_12380\\3855273845.py:663: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=one_star&sortBy=recent\n",
      "Page: 2 one star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=one_star&sortBy=recent\n",
      "Page: 3 one star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=one_star&sortBy=recent\n",
      "Page: 4 one star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=one_star&sortBy=recent\n",
      "Page: 5 one star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=one_star&sortBy=recent\n",
      "Page: 6 one star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=one_star&sortBy=recent\n",
      "Page: 7 one star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=one_star&sortBy=recent\n",
      "Page: 8 one star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=one_star&sortBy=recent\n",
      "Page: 9 one star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=one_star&sortBy=recent\n",
      "Page: 10 one star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=two_star&sortBy=recent\n",
      "Page: 1 two star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=two_star&sortBy=recent\n",
      "Page: 2 two star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=two_star&sortBy=recent\n",
      "Page: 3 two star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=two_star&sortBy=recent\n",
      "Page: 4 two star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=two_star&sortBy=recent\n",
      "Page: 5 two star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=two_star&sortBy=recent\n",
      "Page: 6 two star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=two_star&sortBy=recent\n",
      "Page: 7 two star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=two_star&sortBy=recent\n",
      "Page: 8 two star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=two_star&sortBy=recent\n",
      "Page: 9 two star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=two_star&sortBy=recent\n",
      "Page: 10 two star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=three_star&sortBy=recent\n",
      "Page: 1 three star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=three_star&sortBy=recent\n",
      "Page: 2 three star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=three_star&sortBy=recent\n",
      "Page: 3 three star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=three_star&sortBy=recent\n",
      "Page: 4 three star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=three_star&sortBy=recent\n",
      "Page: 5 three star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=three_star&sortBy=recent\n",
      "Page: 6 three star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=three_star&sortBy=recent\n",
      "Page: 7 three star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=three_star&sortBy=recent\n",
      "Page: 8 three star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=three_star&sortBy=recent\n",
      "Page: 9 three star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=three_star&sortBy=recent\n",
      "Page: 10 three star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=four_star&sortBy=recent\n",
      "Page: 1 four star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=four_star&sortBy=recent\n",
      "Page: 2 four star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=four_star&sortBy=recent\n",
      "Page: 3 four star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=four_star&sortBy=recent\n",
      "Page: 4 four star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=four_star&sortBy=recent\n",
      "Page: 5 four star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=four_star&sortBy=recent\n",
      "Page: 6 four star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=four_star&sortBy=recent\n",
      "Page: 7 four star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=four_star&sortBy=recent\n",
      "Page: 8 four star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=four_star&sortBy=recent\n",
      "Page: 9 four star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=four_star&sortBy=recent\n",
      "Page: 10 four star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=1&filterByStar=five_star&sortBy=recent\n",
      "Page: 1 five star\n",
      "Page 1 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=2&filterByStar=five_star&sortBy=recent\n",
      "Page: 2 five star\n",
      "Page 2 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=3&filterByStar=five_star&sortBy=recent\n",
      "Page: 3 five star\n",
      "Page 3 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=4&filterByStar=five_star&sortBy=recent\n",
      "Page: 4 five star\n",
      "Page 4 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=5&filterByStar=five_star&sortBy=recent\n",
      "Page: 5 five star\n",
      "Page 5 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=6&filterByStar=five_star&sortBy=recent\n",
      "Page: 6 five star\n",
      "Page 6 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=7&filterByStar=five_star&sortBy=recent\n",
      "Page: 7 five star\n",
      "Page 7 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=8&filterByStar=five_star&sortBy=recent\n",
      "Page: 8 five star\n",
      "Page 8 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=9&filterByStar=five_star&sortBy=recent\n",
      "Page: 9 five star\n",
      "Page 9 scraped 10 reviews\n",
      "https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format&pageNumber=10&filterByStar=five_star&sortBy=recent\n",
      "Page: 10 five star\n",
      "Page 10 scraped 10 reviews\n",
      "No more pages left\n",
      "https://www.bestbuy.com/site/reviews/hp-deskjet-2855e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6574145?variant=A\n",
      "Extracted reviews on page 1: 20\n",
      "Extracted reviews on page 2: 20\n",
      "Extracted reviews on page 3: 20\n",
      "Extracted reviews on page 4: 20\n",
      "Extracted reviews on page 5: 20\n",
      "Extracted reviews on page 6: 20\n",
      "Extracted reviews on page 7: 11\n",
      "No more pages left\n",
      "[datetime.date(2024, 10, 30)]\n",
      "https://www.walmart.com/reviews/product/5129928603\n",
      "Total pages: 79\n",
      "Review count in page 1: 10\n",
      "Review count in page 2: 10\n",
      "Review count in page 3: 10\n",
      "Review count in page 4: 10\n",
      "Review count in page 5: 10\n",
      "Review count in page 6: 10\n",
      "Review count in page 7: 10\n",
      "Review count in page 8: 10\n",
      "Review count in page 9: 10\n",
      "Review count in page 10: 10\n",
      "Review count in page 11: 10\n",
      "Review count in page 12: 10\n",
      "Review count in page 13: 10\n",
      "Review count in page 14: 10\n",
      "Review count in page 15: 10\n",
      "Review count in page 16: 10\n",
      "Review count in page 17: 10\n",
      "Review count in page 18: 10\n",
      "Review count in page 19: 10\n",
      "Review count in page 20: 10\n",
      "Review count in page 21: 10\n",
      "Review count in page 22: 10\n",
      "Review count in page 23: 10\n",
      "Review count in page 24: 10\n",
      "Review count in page 25: 10\n",
      "Review count in page 26: 10\n",
      "Review count in page 27: 10\n",
      "Review count in page 28: 10\n",
      "Review count in page 29: 10\n",
      "Review count in page 30: 10\n",
      "Review count in page 31: 10\n",
      "Review count in page 32: 10\n",
      "Review count in page 33: 10\n",
      "Review count in page 34: 10\n",
      "Review count in page 35: 10\n",
      "Review count in page 36: 10\n",
      "Review count in page 37: 10\n",
      "Review count in page 38: 10\n",
      "Review count in page 39: 10\n",
      "Review count in page 40: 10\n",
      "Review count in page 41: 10\n",
      "Review count in page 42: 10\n",
      "Review count in page 43: 10\n",
      "Review count in page 44: 10\n",
      "Review count in page 45: 10\n",
      "Review count in page 46: 10\n",
      "Review count in page 47: 10\n",
      "Review count in page 48: 10\n",
      "Review count in page 49: 10\n",
      "Review count in page 50: 10\n",
      "Review count in page 51: 10\n",
      "Review count in page 52: 10\n",
      "Review count in page 53: 10\n",
      "Review count in page 54: 10\n",
      "Review count in page 55: 10\n",
      "Review count in page 56: 10\n",
      "Review count in page 57: 10\n",
      "Review count in page 58: 10\n",
      "Review count in page 59: 10\n",
      "Review count in page 60: 10\n",
      "Review count in page 61: 10\n",
      "Review count in page 62: 10\n",
      "Review count in page 63: 10\n",
      "Review count in page 64: 10\n",
      "Review count in page 65: 10\n",
      "Review count in page 66: 10\n",
      "Review count in page 67: 10\n",
      "Review count in page 68: 10\n",
      "Review count in page 69: 10\n",
      "Review count in page 70: 10\n",
      "Review count in page 71: 10\n",
      "Review count in page 72: 10\n",
      "Review count in page 73: 10\n",
      "Review count in page 74: 10\n",
      "Review count in page 75: 10\n",
      "Review count in page 76: 10\n",
      "Review count in page 77: 10\n",
      "Review count in page 78: 10\n",
      "Review count in page 79: 4\n",
      "Total walmart review: 784\n",
      "Tassel_raw_data_scraping completed. Tassel_raw file saved\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import openpyxl\n",
    "from deep_translator import GoogleTranslator\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pyvirtualdisplay import Display\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "#define system path start\n",
    "\n",
    "path = r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\'\n",
    "excel_path = r'C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\\\'\n",
    "\n",
    "#define system path start\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the timestamp\n",
    "print(\"Current Timestamp:\", timestamp)\n",
    "\n",
    "print('Running Tassel_raw_date_scraping.py')\n",
    "\n",
    "# %%\n",
    "\n",
    "# change here\n",
    "excel_file_path = excel_path + \"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheet_name = \"data_new\"\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "df_amazon\n",
    "\n",
    "# %%\n",
    "# change here\n",
    "excel_file_path = excel_path + \"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(excel_file_path, sheet_name=sheets, engine='openpyxl')\n",
    "review_template\n",
    "\n",
    "def get_soup(url, retries=3):\n",
    "    global global_cookies\n",
    "\n",
    "    # Define a stronger header\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'DNT': '1',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1'\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Retry mechanism\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            session.cookies.update(global_cookies)  # Use global cookies for subsequent requests\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()  # Raise an error for non-2xx status codes\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # file_name = f\"{random.randint(5, 150)}.html\"\n",
    "            # with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            #     file.write(str(soup))\n",
    "            return soup\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            time.sleep(random.uniform(1, 5))  # Add a random delay before retrying\n",
    "            continue\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page after multiple retries.\")\n",
    "        return None\n",
    "\n",
    "def get_soup_amazon(url, host):\n",
    "    headers = {\n",
    "        \"Host\": host,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    if(host == 'www.amazon.co.uk'):\n",
    "       cookies = {\n",
    "        \"csm-sid\": \"903-9836176-6967009\",\n",
    "        \"x-amz-captcha-1\": \"1726144856604350\",\n",
    "        \"x-amz-captcha-2\": \"qEybXoxNsed3VxMGE+SE+A==\",\n",
    "        \"id_pkel\": \"n0\",\n",
    "        \"sp-cdn\": \"\\\"L5Z9:IN\\\"\",\n",
    "        \"x-acbuk\": \"\\\"yuiOffCDUkJx6URQInGTJ6XlUQeW?LLFW@ncc752HDvrhB8fkYnPput0cMPxg0EX\\\"\",\n",
    "        \"at-acbuk\": \"Atza|IwEBIOJtMeigSR4gCXGlTlCuhLTgNhVubQ8O2XychooYXDzNlzAIwvUNIA1H9Zbs5XoG0JF9h4Jvrs3ijJJgvfP55z197zpL87ls6TjIIje3iyVhtts6DbdJC6vA4kKDrqn6nJG4nHrl6d24Tn9UWbWk86EmVf-jQyP-dwer9n_q9JdqJFgfShB5hc_rgeVn-agSqkZq35K8l4QaOcluKKTEvSzXihQ3vXWoSnUSVTF9g14PKQ\",\n",
    "        \"sess-at-acbuk\": \"\\\"Tweb+5/58q3Thu7TyShnr2SOz3X55HMLWo/zS1WYuOE=\\\"\",\n",
    "        \"sst-acbuk\": \"Sst1|PQG3xWZaRJ0cBjL730W8I9--CbrxJB47zNixmDqU977lfvr8ouxMuhU5oREdiMWO_OOkdA6JqghqnG5ONKrAfz67rh64YZXpn3iAyqWrVbT7ZMPBwLBdskZThleZxTWVDTz2JuXiQbwQnMAGiJyj2aXq7DIgfWNIHlld_LuZssXyU_gqXfZk6po0t54FIOm-fuF-c0_F99l5ZVvvPNORVp-7DzLI7pRB9_ReVvHHIGZqrtu83UuaVpNANunIdxs2E0AlacQOBKyy2jDf0aMvc0XvaUaJC1pMjZKUsqy0jbwIWeI\",\n",
    "        \"session-id\": \"262-8938359-6381428\",\n",
    "        \"ubid-acbuk\": \"258-3471506-1778545\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"i18n-prefs\": \"GBP\",\n",
    "        \"lc-acbuk\": \"en_GB\",\n",
    "        \"session-token\": \"SgGzvM1R1CODsgTJghK26IeqWqRWsmNSNe1IN0LjKrLslahRjIkjR3HgYcZ8ML+Jbq/pdsXAL9KIQNYYrR3nyTjw0rkNhyk77y+9eTVbJ2DrqH8Uuz8MFonWRdn/jEH0Y3iisIO7FjGHagDu61sjqXvBp2HD5k5MAmTjAg4kzjQGQwCda/uNRnZZrSYoizhPLA77PGER+iTbYjlNEOa4HSOXncLeyoKceWUWVAr0znP4WAnRGhhXlSJHAmySTgtuT8PtDkS6Phb5ktBls684WVXUG8viRChzQy1Ig12Ni51sjicqTnupDflMU9bug1pRv7wQ+0sgVpuwIp0bKwnCivudiJ6kwUWnDM7tqGj7zGJCUuurovD3cuNaYTFA5mFu\"\n",
    "    }\n",
    "    elif(host == 'www.amazon.es'):\n",
    "        cookies = {\n",
    "        \"id_pkel\": \"n0\",\n",
    "        \"session-id\": \"257-5598184-5873262\",\n",
    "        \"ubid-acbes\": \"258-5854927-0158657\",\n",
    "        \"session-token\": \"\\\"+7oftLYwnh8EHMol3vfBI6jJ2TV6aFbg2grjvif+ogyH7X+fkVWduOC+zg804drGOSzTtxkE5PeBu0bbZjbAo+JQMYttPWD3QTSKlw59C1IlpIygdvk7QH9dKIuSgfuCkZWOh5y2iZ3y0DiyRX1Q+zbn7hTiXXFV3LhwDZTB8tsIqEae6oYKwygqTOxLlXLzVoPIQk+ecEg3c6pr13qLM+hnlgXVS9tVrbIS+6vA032G+tkjUj9EP2/37Uj52m5ZFgMfPFecMZjYKvjPYooXut3a1kPCQ9R+pCR9EI4H9GVkd/aQvV4jAxU69ugDrZ6+gGFtKAbG9JkTOA5KNAH5R5aNnLNNAtd8BFx/5x3y0hic+mradYAv5Q==\\\"\",\n",
    "        \"x-acbes\": \"\\\"ddqjWg6xf@srOWMvkRnhx6d@Rbfd3jVwuvviv@CrOM?fr@I55XYacOGLiBCiBtkd\\\"\",\n",
    "        \"at-acbes\": \"Atza|IwEBIILAcon2nBuMboH4I12LftjcdWL6_QTeBZAMtq09wzbMF6tHWIDCAsTb1fNYndEyQvpB6WDz7riwKAHM3sS5PWG2NOjqknvQGcXBME_DoS_fv5T37vVKqzfeBEPGlykMpESkuGVQwGAdUlIiZ-Ocok1I5wmvvE2d5nG92Os2v9OqxeRAqde4Qws6snM9jbEUxvJbERF-UKKpZUMmMgQrna4SOvFBrR1iHHk0ZZMkqay-iw\",\n",
    "        \"sess-at-acbes\": \"\\\"B9PYBW+c3zxV2RtmYVJQIVSH624Xafu7AH4POuxetPQ=\\\"\",\n",
    "        \"sst-acbes\": \"Sst1|PQFIZrA21csrcZqsrrMbIciMCbNQQ0SCX72wbZmJmVmjIMIvc5jEHk2Prq7gGl5gLHU063SPzp29c62OhqJiMwO7M_fjtLGmh96TjbV7PncHTOWQ77FKYYeKtaTUch6iQunuE2azLVn6jinuNjU1VRFO5C6kMTy48MJzqwfprbm9tY6mOH5zgU_nRtWg5sDwVw9Nb-BEyUNoBs48wbDB_WoiX5z1qwCvVAskroUY0TRIo5Xidu_2PJQXTlXKdCj8Wb7fScmcfbjTzJsS9Rw76_ThEZP0GH2T19LcB_MLj96L5ic\",\n",
    "        \"session-id-time\": \"2082787201l\",\n",
    "        \"i18n-prefs\": \"EUR\",\n",
    "        \"lc-acbes\": \"es_ES\",\n",
    "        \"csm-hit\": \"tb:s-4JQH4X82C92A432W46EC|1728318586057&t:1728318586700&adb:adblk_no\"\n",
    "    }\n",
    "    else:\n",
    "        cookies = {\n",
    "        \"at-main\": \"Atza|IwEBIF8fWp8N6wwKJntOIGx2Iz2Tzmp0jmlFToAsLpZyql2gf_tMuytB0Wq_9Q6eSfQckLBIoWEVn4pjZ_WMdDle2G7wLmfsrW3nnEmKMUA02jmAZpsXM5KEJwH0Hb8C04WZKpjfJYdhzyYBSz_T5gyHOw_FlXF_pCj3oquUHroknOq0G-ILvkZxvRbJLNkKk30UJt2O28Gi7VUcDib4Qpo31ltrWxt0eN8EzCdIZxVLAmNz-g\",\n",
    "        \"sess-at-main\": \"\\\"xfy9iZnuMPNqiCnv39ubW/JVEoJMw6HX+jT+zpzijho=\\\"\",\n",
    "        \"sst-main\": \"Sst1|PQENTri85vVuQqInRi4tn1sRCTJnoGB_U9PX9G7i6l9QYLTaHul93ueh3jEdoZdJPHouFM3VHOWwx4eNe2lRG-rWM3V17_1bEH0WaT59C0eZS-VZJ3800scFSPo_bOjGuT4Z6H1oBZTcH3pblvW2QxJOlBwm2zBYZOav6LRw319WRJDfYmmrvjE3Gqiyi1KdeCgGL6vrLVl5DHhDjBygvSDWNHYVUn7WC4gcvLW-qbNmVseZTnwShfY-r0shikWcVLo5K0QnjZppa1tQIWEi1RP2y4I1T8m0T7pBIHs3tHCEY4s\",\n",
    "        \"x-main\": \"\\\"EhNPt8@KYq7hUHIQ1Qh6mEAdTrkwApsU2OL0whAniY?qsHfGS@@fHwWgmIjwV@4U\\\"\",\n",
    "        \"i18n-prefs\": \"USD\",\n",
    "        \"lc-main\": \"en_US\",\n",
    "        \"session-token\": \"lNWWXnGpmO6H/Ngtu2aSRGaWec4m8rvxAFHkkp3oWLoqoqm/5UWej6oMXJ7yrYUdI+OdVIczGKJQWL82ftyFq4KIwwi5Ec8DqPTt1CRVZ5zmlunkiJc9JT9Hz7vB4Ko8KeAtmAtL1BsFTJcb3Oin3uYdbtUstW0ZmD0UNjJvoZVk035kC1UveGPv2Up9mC9Gdk672LMS8AyFpz3kuR/bKQTRkC7WfsdiCrW3vOMWVIkipm37dsnsTrPMCgLrdAYAShnfuZoF5/hr1grf2CxtctH/xlvV0Gm8K6++DnjVfWG4W/B4jt7IYppw8SvROoKzEOvgiHrcfejzOIU0y29I8BK8CAXcrsPWlPBaJmvLtqTCF+Dv9gXsNA3HstUH0FSu\",\n",
    "        \"ubid-main\": \"135-8431336-5346218\",\n",
    "        \"session-id\": \"143-9100146-8969639\",\n",
    "        \"session-id-time\": \"2082787201l\"\n",
    "    }\n",
    "\n",
    "        \n",
    "\n",
    "    req = requests.get(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amazon_review(soup, url):    \n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "\n",
    "    for item in reviews:    \n",
    "        # review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "        # review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "\n",
    "        review = {    \n",
    "            'Model': model,    \n",
    "            'Review date': item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[\n",
    "                1],\n",
    "            \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        \n",
    "  \n",
    "        try:    \n",
    "            review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "        except AttributeError:    \n",
    "            review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "  \n",
    "        try:    \n",
    "            review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "  \n",
    "        try:    \n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        try:      \n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "        except AttributeError:        \n",
    "            review[\"Review name\"] = None  \n",
    "  \n",
    "        try:    \n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"People_find_helpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            if seeding:\n",
    "               review['Seeding or not'] = seeding\n",
    "            else:\n",
    "                raise AttributeError\n",
    "        except AttributeError:  \n",
    "            try: \n",
    "                review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, string='Vine Customer Review of Free Product')\n",
    "\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "\n",
    "        try:\n",
    "            review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "             review['Aggregation'] = None\n",
    "    \n",
    "  \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "  \n",
    "    return extracted_reviews\n",
    "\n",
    "# %%\n",
    "urls = ['https://www.amazon.co.uk/HP-DeskJet-Wireless-included-Reliable/product-reviews/B0CFFBXYSH/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "       # 'https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CFFC6LRR/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format',\n",
    "       # 'https://www.amazon.co.uk/HP-DeskJet-Wireless-Included-Reliable/product-reviews/B0CB722L39/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "\n",
    "# %%\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['one','two','three','four','five'] \n",
    "max_retry_attempts = 2\n",
    "all_reviews = []\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for x in range(1, 11):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url = f'{link}&pageNumber={x}&filterByStar={y}_star&sortBy=recent'  \n",
    "                    print(url)\n",
    "                    print('Page:',x, f'{y} star')\n",
    "                    soup = get_soup_amazon(url,'www.amazon.co.uk')  # Get the soup object from the URL\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                    #         print('No review')  \n",
    "                    #         found_reviews = False\n",
    "                    #         break \n",
    "                    \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews)\n",
    "                        print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if x >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {x} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None)\n",
    "amazon2= pd.DataFrame(all_reviews)\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "# amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "amazon_hp_combine\n",
    "\n",
    "\n",
    "# %%\n",
    "amazon_final = amazon_hp_combine \n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'UK'\n",
    "amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Aggregation':'Aggregation_Flag',\n",
    "    'Country': 'Country'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "amazon_final_df\n",
    "\n",
    "\n",
    "# %%\n",
    "final_review = pd.concat([review_template, amazon_final_df])\n",
    "final_review.to_csv('uk.csv', index=False)\n",
    "\n",
    "# %% [markdown]\n",
    "## Spain\n",
    "\n",
    "# %%\n",
    "urls =  ['https://www.amazon.es/HP-DeskJet-2820e-Impresora-Multifunci%C3%B3n/product-reviews/B0CFFWJHMF/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format']\n",
    "        # 'https://www.amazon.es/Impresora-Multifunci%C3%B3n-HP-impresi%C3%B3n-Fotocopia/product-reviews/B0CFG1PB4P/ref=cm_cr_arp_d_viewopt_fmt?formatType=current_format']\n",
    "\n",
    "# %%\n",
    "from datetime import datetime\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "def amazon_review(soup, url, translate_to=None):    \n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "\n",
    "    for item in reviews:    \n",
    "        # review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "        # review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "\n",
    "        review = {    \n",
    "            'Model': model,\n",
    "            'Review date':\n",
    "                item.find('span', {'data-hook': 'review-date'}).text.replace('Revisado en Espaa', '').split('el')[\n",
    "                    1],\n",
    "            \"Orginal Review\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(), \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        \n",
    "  \n",
    "        if translate_to and review[\"Orginal Review\"]:\n",
    "            translated_review = translator.translate(review[\"Orginal Review\"])\n",
    "            review[\"Review Content\"] = translated_review\n",
    "\n",
    "        try:\n",
    "            review[\"Orginal Title\"] = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()\n",
    "        except AttributeError:\n",
    "            review[\"Orginal Title\"] = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()\n",
    "\n",
    "        if translate_to and review[\"Orginal Title\"]:\n",
    "            \n",
    "            translated_review = translator.translate(review[\"Orginal Title\"])\n",
    "            test = translated_review\n",
    "\n",
    "        review[\"Orginal Title\"] = review[\"Orginal Title\"].split('\\n')[-1]    \n",
    "        review[\"Review Title\"] = test.split('\\n')[-1]\n",
    "        \n",
    "        #print(review[\"Review Title\"])\n",
    "        \n",
    "        review[\"URL\"] = url\n",
    "\n",
    "        try:\n",
    "            review[\"Review rating\"] = (\n",
    "                item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\",0 de 5 estrellas\", \"\").strip())\n",
    "        except AttributeError:\n",
    "            review[\"Review rating\"] = (\n",
    "                item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\",0 de 5 estrellas\", \"\").strip())\n",
    "\n",
    "        try:\n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()\n",
    "        except AttributeError:\n",
    "            review[\"Verified Purchase or not\"] = None\n",
    "\n",
    "        try:\n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()\n",
    "        except AttributeError:\n",
    "            review[\"Review name\"] = None\n",
    "\n",
    "        try:\n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()\n",
    "        except AttributeError:\n",
    "            review[\"People_find_helpful\"] = None\n",
    "            \n",
    "        try:\n",
    "            seeding= item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            if seeding:\n",
    "               review['Seeding or not'] = seeding\n",
    "            else:\n",
    "                raise AttributeError\n",
    "        except AttributeError:  \n",
    "            try: \n",
    "                review['Seeding or not'] = item .find('span', {'class': 'a-color-success a-text-bold'}, string='Vine Customer Review of Free Product')\n",
    "\n",
    "            except AttributeError:\n",
    "                review['Seeding or not'] = None\n",
    "\n",
    "        try:\n",
    "            review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "             review['Aggregation'] = None\n",
    "    \n",
    "  \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "  \n",
    "    return extracted_reviews\n",
    "\n",
    "# %%\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['one','two','three','four','five'] \n",
    "max_retry_attempts = 1\n",
    "all_reviews = []\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for x in range(1, 11):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url =f'{link}&filterByStar={y}_star&pageNumber={x}&sortBy=recent'\n",
    "                    print('Page:',x, f'{y} star')\n",
    "                    soup = get_soup_amazon(url,'www.amazon.es')  # Get the soup object from the URL\n",
    "                    extracted_reviews = amazon_review(soup, url,translate_to=\"en\")  # Extract reviews from the soup\n",
    "                    # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                    #         print('No review')  \n",
    "                    #         found_reviews = False\n",
    "                    #         break \n",
    "\n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews)\n",
    "                        print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "\n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "\n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "\n",
    "                    if x >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {x} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        break  \n",
    "\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import date \n",
    "amazon2 = pd.DataFrame(all_reviews)\n",
    "#print(amazon2)\n",
    "\n",
    "# Function to parse the date column\n",
    "def parse_date(date_str):\n",
    "    months = {\n",
    "        \"enero\": \"January\",\n",
    "        \"febrero\": \"February\",\n",
    "        \"marzo\": \"March\",\n",
    "        \"abril\": \"April\",\n",
    "        \"mayo\": \"May\",\n",
    "        \"junio\": \"June\",\n",
    "        \"julio\": \"July\",\n",
    "        \"agosto\": \"August\",\n",
    "        \"septiembre\": \"September\",\n",
    "        \"octubre\": \"October\",\n",
    "        \"noviembre\": \"November\",\n",
    "        \"diciembre\": \"December\"\n",
    "    }\n",
    "    date_str = date_str.strip()\n",
    "    day, month, year = date_str.split(' de ')\n",
    "    month = months[month]\n",
    "    date = datetime.strptime(f\"{day} {month} {year}\", \"%d %B %Y\")\n",
    "    return date\n",
    "\n",
    "# Apply the function to the Date column\n",
    "amazon2['Review date'] = amazon2['Review date'].apply(parse_date)\n",
    "\n",
    "amazon2['Retailer'] = \"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "#amazon2['Review Title'] = amazon2['Review Title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model']\n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis=1)\n",
    "amazon_hp_combine.drop_duplicates(inplace=True)\n",
    "\n",
    "amazon_final = amazon_hp_combine\n",
    "amazon_final.drop_duplicates(inplace=True)\n",
    "\n",
    "# Clean Review Content\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(lambda x: re.sub(r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.', '', x).strip())\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'Spain'\n",
    "amazon_final.sort_values(by=['Review date'], ascending=False, inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "amazon_final_df = amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review Content': 'Review_Content',\n",
    "    \"Review Title\": \"Review_Title\",\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Aggregation': 'Aggregation_Flag',\n",
    "    'Country': 'Country',\n",
    "    'Orginal Review': 'Orginal_Review'\n",
    "})\n",
    "amazon_final_df['Orginal_Title'] = amazon2['Orginal Title']\n",
    "amazon_final_df['Orginal Title'] = \"\"\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] = pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "\n",
    "# Create Review_Rating_Label column\n",
    "amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country', 'Orginal_Title','Orginal Title',\n",
    "]\n",
    "\n",
    "# Ensure all required columns are present\n",
    "for col in required_columns:\n",
    "    if col not in amazon_final_df.columns:\n",
    "        amazon_final_df[col] = None  # or use an appropriate default value\n",
    "\n",
    "# Select only the required columns\n",
    "amazon_final_df = amazon_final_df[required_columns]\n",
    "\n",
    "# Save to CSV\n",
    "amazon_final_df.to_csv('es.csv', index=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# # US\n",
    "\n",
    "# %%\n",
    "def amazon_review(soup, url):    \n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = soup.title.text.replace(\"Amazon.com: Customer reviews: \",\"\")    \n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}).string.strip()  \n",
    "        except AttributeError: \n",
    "            model = soup.find(\"div\", attrs={\"class\": \"a-row product-title\"}).string.strip()  \n",
    "  \n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    # NPI launched on 2024-01-15\n",
    "    date_string = \"2024-01-15\"\n",
    "    min_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "\n",
    "    for item in reviews:    \n",
    "        review_date_string = item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in', '').split('on')[1].strip()\n",
    "        review_date = datetime.strptime(review_date_string, \"%B %d, %Y\")\n",
    "        if review_date < min_date:\n",
    "            print('Review date is less than 2024-01-15')\n",
    "            break\n",
    "    \n",
    "        review = {    \n",
    "            'Model': model,    \n",
    "            'Review date': review_date,     \n",
    "            \"Review Content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "            \"URL\": url  \n",
    "        }\n",
    "        \n",
    "        try:    \n",
    "            review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "        except AttributeError:    \n",
    "            review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\").strip())    \n",
    "  \n",
    "        try:    \n",
    "            review['Review title'] = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review['Review title'] = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "  \n",
    "        try:    \n",
    "            review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        try:      \n",
    "            review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "        except AttributeError:        \n",
    "            review[\"Review name\"] = None  \n",
    "  \n",
    "        try:    \n",
    "            review[\"People_find_helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.strip()    \n",
    "        except AttributeError:    \n",
    "            review[\"People_find_helpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            seeding = item.find(\"span\", {'class': \"a-color-success a-text-bold\"}).text.strip()\n",
    "            if 'Vine Customer Review of Free Product' in seeding:\n",
    "                review['Seeding or not'] = 'Vine Customer Review of Free Product'\n",
    "            else:\n",
    "                review['Seeding or not'] = None\n",
    "        except AttributeError:\n",
    "            review['Seeding or not'] = None\n",
    "\n",
    "        try:\n",
    "            review['Aggregation'] = item.find(\"a\", {\"data-hook\": \"format-strip\"}).text.strip()\n",
    "        except AttributeError:   \n",
    "            review['Aggregation'] = None\n",
    "    \n",
    "        extracted_reviews.append(review)    \n",
    "    \n",
    "    return extracted_reviews\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "urls = ['https://www.amazon.com/HP-DeskJet-Wireless-included-588S5A/product-reviews/B0CT2R7199/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "       # 'https://www.amazon.com/HP-DeskJet-Wireless-Included-588S6A/product-reviews/B0CT2QHQVF/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format']\n",
    "\n",
    "# %%\n",
    "import datetime \n",
    "from datetime import datetime\n",
    "star = ['one','two','three','four','five'] \n",
    "max_retry_attempts = 2\n",
    "all_reviews = []\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    for y in star:\n",
    "        found_reviews = True\n",
    "        for x in range(1, 11):\n",
    "            retry_attempts = 0\n",
    "            while found_reviews is True:\n",
    "                try:\n",
    "                    url = f'{link}&pageNumber={x}&filterByStar={y}_star&sortBy=recent'  \n",
    "                    print(url)\n",
    "                    print('Page:',x, f'{y} star')\n",
    "                    soup = get_soup_amazon(url,'www.amazon.com')  # Get the soup object from the URL\n",
    "                    extracted_reviews = amazon_review(soup, url)  # Extract reviews from the soup\n",
    "                   \n",
    "                    # if soup.find('div', {'class': 'a-section a-spacing-top-large a-text-center no-reviews-section'}):  \n",
    "                    #         print('No review')  \n",
    "                    #         found_reviews = False\n",
    "                    #         break \n",
    "                    \n",
    "                    if len(extracted_reviews) > 0:\n",
    "                        all_reviews.extend(extracted_reviews)\n",
    "                        print(f\"Page {x} scraped {len(extracted_reviews)} reviews\")\n",
    "                    \n",
    "                    # if (page == 1 and len(extracted_reviews) == 0):\n",
    "                    #     print(f\"Page {page} has no reviews, retry\")\n",
    "                    #     continue\n",
    "                        \n",
    "                    if soup.find('li', {'class': 'a-disabled a-last'}):  \n",
    "                        print('No more pages left')  \n",
    "                        found_reviews = False\n",
    "                        break \n",
    "                    \n",
    "                    if x >= 1 and len(extracted_reviews) == 0:\n",
    "                        retry_attempts += 1\n",
    "                        if retry_attempts == max_retry_attempts:\n",
    "                            found_reviews = False\n",
    "                            print(f\"Page {x} has no reviews, moving to the next page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Page {x} has no reviews, retry\")\n",
    "                            continue \n",
    "\n",
    "                    \n",
    "                            \n",
    "                    else:\n",
    "                        break  \n",
    "        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "\n",
    "                    # If any exception occurs, retry\n",
    "                    retry_attempts += 1\n",
    "                    if retry_attempts == max_retry_attempts:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"An error occurred, retrying\")\n",
    "                        continue  # Retry the loop\n",
    "            else:\n",
    "                # If all retry attempts failed, move to the next page\n",
    "                continue\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "from datetime import date \n",
    "pd.set_option('display.max_columns', None)\n",
    "amazon2= pd.DataFrame(all_reviews)\n",
    "amazon2['Retailer']=\"Amazon\"\n",
    "amazon2['scraping_date'] = pd.to_datetime(date.today())\n",
    "amazon2['Review date'] = pd.to_datetime(amazon2['Review date'])\n",
    "amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "amazon2['People_find_helpful'] = amazon2['People_find_helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "amazon_filter = amazon2[amazon2['Aggregation'] != 'Model name: Old Version']\n",
    "amazon_hp_combine = pd.merge(amazon_filter, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "amazon_hp_combine['Review Model'] = amazon_hp_combine['HP Model'] \n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "# amazon_hp_combine['Aggregation'] = amazon_hp_combine['Aggregation'].fillna('',inplace = True) \n",
    "amazon_hp_combine = amazon_hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "amazon_hp_combine.drop_duplicates(inplace = True)\n",
    "amazon_hp_combine\n",
    "\n",
    "\n",
    "# %%\n",
    "amazon_final = amazon_hp_combine \n",
    "amazon_final.drop_duplicates(inplace = True)\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'] .astype(str).apply(lambda x: re.sub(r'The media could not be loaded\\.', '', x).strip())\n",
    "amazon_final['Review Content'] = amazon_final['Review Content'].astype(str).apply(\n",
    "    lambda x: re.sub(\n",
    "        r'Video Player is loading\\.Play VideoPlayMuteCurrent Time[\\s\\S]*?This is a modal window\\.',\n",
    "        '',\n",
    "        x\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "amazon_final['Competitor_flag'] = amazon_final['Review Model'].astype(str).apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "amazon_final['Country'] = 'US'\n",
    "amazon_final.sort_values(by = ['Review date'],ascending = False)\n",
    "\n",
    "amazon_final_df= amazon_final.rename(columns={\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Retailer': 'Retailer',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'Review date': 'Review_Date',\n",
    "    'Review name': 'Review_Name',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'URL': 'URL',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Segment': 'Segment',\n",
    "    'Competitor_flag': 'Competitor_Flag',\n",
    "    'Aggregation':'Aggregation_Flag',\n",
    "    'Country': 'Country'\n",
    "})\n",
    "\n",
    "amazon_final_df['Review_Date'] = pd.to_datetime(amazon_final_df['Review_Date']).dt.date\n",
    "amazon_final_df['Review_Rating'] = amazon_final_df['Review_Rating'].astype('int64')\n",
    "amazon_final_df['People_Find_Helpful'] = amazon_final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "amazon_final_df['Scraping_Date'] =  pd.to_datetime(amazon_final_df['Scraping_Date']).dt.date\n",
    "amazon_final_df.reset_index(inplace = True,drop = True)\n",
    "amazon_final_df.sort_values(['Review_Date'],ascending = False) \n",
    "amazon_final_df['Review_Rating_Label'] = amazon_final_df['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "amazon_final_df\n",
    "\n",
    "\n",
    "# %%\n",
    "final_review = pd.concat([review_template, amazon_final_df])\n",
    "final_review\n",
    "\n",
    "# Save to CSV\n",
    "final_review.to_csv(r'us.csv', index=False)\n",
    "\n",
    "\n",
    "#marge amazon\n",
    "\n",
    "es_df = pd.read_csv('es.csv')\n",
    "header = es_df.columns\n",
    "\n",
    "# Read the data from es.csv, uk.csv, and us.csv without headers\n",
    "es_data = pd.read_csv('es.csv', header=0)  # Include header only for es.csv\n",
    "uk_data = pd.read_csv('uk.csv', header=0)  # Exclude header for uk.csv\n",
    "us_data = pd.read_csv('us.csv', header=0)  # Exclude header for us.csv\n",
    "\n",
    "# Combine the data\n",
    "combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "\n",
    "# Save the combined data with the header from es.csv\n",
    "combined_df.to_csv('amazon.csv', index=False, header=header)\n",
    "\n",
    "#marge amazon\n",
    "\n",
    "\n",
    "\n",
    "#marge amazon\n",
    "\n",
    "es_df = pd.read_csv('es.csv')\n",
    "header = es_df.columns\n",
    "\n",
    "# Read the data from es.csv, uk.csv, and us.csv without headers\n",
    "es_data = pd.read_csv('es.csv', header=0)  # Include header only for es.csv\n",
    "uk_data = pd.read_csv('uk.csv', header=0)  # Exclude header for uk.csv\n",
    "us_data = pd.read_csv('us.csv', header=0)  # Exclude header for us.csv\n",
    "\n",
    "# Combine the data\n",
    "combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "\n",
    "# Save the combined data with the header from es.csv\n",
    "combined_df.to_csv('amazon.csv', index=False, header=header)\n",
    "\n",
    "#marge amazon\n",
    "\n",
    "# Best buy hp\n",
    "\n",
    "\n",
    "def get_review_bestbuy(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "def bestbuy_review(soup, url):    \n",
    "    bestbuy = {}\n",
    "    bestbuy_reviews = []  \n",
    "    Model = soup.find(\"h1\", {'class':\"heading-5 v-fw-regular\"})\n",
    "    if not Model:\n",
    "        Model = soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"})\n",
    "    Model = Model.text if Model else None\n",
    "    \n",
    "        \n",
    "    npi = soup.find('span',{'class':'c-reviews order-2'} ).text\n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "    if review_session:\n",
    "        for item in review_session:    \n",
    "            bestbuy = {    \n",
    "                 'Model':Model,\n",
    "                # 'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "                'URL':url \n",
    "            }\n",
    "            try:    \n",
    "                bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review title']  = None\n",
    "\n",
    "            try:\n",
    "                bestbuy['Review_Name']  = item.find(\"div\", {\"class\": \"ugc-author v-fw-medium body-copy-lg\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review_Name']  = None\n",
    "                \n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review rating']  = None\n",
    "\n",
    "            review_date_element = item.find(\"time\", {\"class\": \"submission-date\"})\n",
    "            if review_date_element:\n",
    "                review_date_string = review_date_element['title']\n",
    "                review_date_datetime = datetime.strptime(review_date_string, '%b %d, %Y %I:%M %p')\n",
    "                formatted_review_date = review_date_datetime.strftime('%Y-%m-%d')\n",
    "                bestbuy['Review_Date'] = formatted_review_date\n",
    "            else:\n",
    "                bestbuy['Review_Date'] = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review promotion']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['Review aggregation']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Content']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "            except AttributeError:    \n",
    "                bestbuy['Review Recommendation']  = None\n",
    "\n",
    "            try:    \n",
    "                network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "                if network_badge:\n",
    "                    bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "                else:\n",
    "                    bestbuy['Seeding or not']  = \"\"\n",
    "            except AttributeError:    \n",
    "                bestbuy['Seeding or not']  = \"\"\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_helpful']  = None\n",
    "\n",
    "            try:    \n",
    "                bestbuy['People_find_unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "            except AttributeError:    \n",
    "                bestbuy['People_find_unhelpful']  = None\n",
    "\n",
    "\n",
    "            bestbuy_reviews.append(bestbuy)    \n",
    "        \n",
    "    \n",
    "  \n",
    "    return npi, bestbuy_reviews \n",
    "\n",
    "urls = ['https://www.bestbuy.com/site/reviews/hp-deskjet-2855e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6574145?variant=A']\n",
    "       # 'https://www.bestbuy.com/site/reviews/hp-deskjet-4255e-wireless-all-in-one-inkjet-printer-with-3-months-of-instant-ink-included-with-hp-white/6575024?variant=A']\n",
    "\n",
    "# %%\n",
    "max_attempts = 5\n",
    "bestbuy_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    should_continue = True\n",
    "    attempt_count = 0  # Counter for attempts\n",
    "    for x in range(1, 100):\n",
    "        if not should_continue:\n",
    "            break\n",
    "        while True:\n",
    "            url = f'{link}&page={x}'\n",
    "            try:\n",
    "                soup = get_review_bestbuy(url)\n",
    "                npi, reviews = bestbuy_review(soup, url)\n",
    "                if npi == 'Be the first to write a review':\n",
    "                    should_continue = False\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')\n",
    "                bestbuy_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-disabled\": \"true\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                if x > 1 and next_page_link and next_page_link.get(\"aria-disabled\") == \"true\":\n",
    "                    should_continue = False\n",
    "                    print('No more pages left')\n",
    "                    break\n",
    "\n",
    "                if len(reviews) < 20:\n",
    "                    should_continue = False\n",
    "                    print('Only 1 page')\n",
    "                    break\n",
    "                else:\n",
    "                    break \n",
    "            except Exception as e:\n",
    "                attempt_count += 1\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds... (Attempt {attempt_count}/{max_attempts})\")\n",
    "                if attempt_count >= max_attempts:\n",
    "                    print(\"Maximum number of attempts reached. Exiting loop.\")\n",
    "                    should_continue = False\n",
    "                    break\n",
    "                time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "review = pd.DataFrame(bestbuy_reviews)\n",
    "review['Retailer']=\"Best Buy\"\n",
    "review['scraping_date'] = pd.to_datetime(date.today())\n",
    "\n",
    "review['HP Model Number'] = review['Model'].str.extract(r'(\\d+e*)')\n",
    "\n",
    "hp_combine = pd.merge(review, df_amazon, on = \"HP Model Number\", how = \"left\" )\n",
    "\n",
    "hp_combine['Review Model'] = hp_combine['HP Model'] \n",
    "hp_combine['People_find_helpful'] = hp_combine['People_find_helpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "hp_combine['People_find_unhelpful'] = hp_combine['People_find_unhelpful'].fillna(0).astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "\n",
    "columns_to_drop = [  \n",
    "    'Model', 'HP Model Number', 'Comp Model number','HP Model'\n",
    "]  \n",
    "  \n",
    "hp_combine_bestbuy = hp_combine.drop(columns_to_drop, axis = 1) \n",
    "\n",
    "hp_combine_bestbuy = hp_combine_bestbuy.drop_duplicates()\n",
    "\n",
    "hp_combine_bestbuy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "final_review = pd.DataFrame()\n",
    "bestbuy_final = hp_combine_bestbuy\n",
    "bestbuy_final.drop_duplicates(inplace = True)\n",
    "\n",
    "bestbuy_final = bestbuy_final.sort_values(by = ['Review Model', 'Review title', 'Review Content', 'scraping_date'])\n",
    "\n",
    "bestbuy_final['Competitor_Flag'] = bestbuy_final['Review Model'].apply(lambda x: 'No' if 'HP' in x else 'Yes')\n",
    "bestbuy_final['Country'] = 'US'\n",
    "\n",
    "bestbuy_final_version = bestbuy_final.rename(columns={\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "bestbuy_final_version.drop(columns=['Review Recommendation', 'People_find_unhelpful'], inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "bestbuy_final_version.to_csv('Bestbuy_NPI_review.csv', index=False)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "Final_review = pd.concat([final_review, bestbuy_final_version], ignore_index=True)\n",
    "\n",
    "# Convert data types\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64')\n",
    "Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "# Create Review_Rating_Label column\n",
    "Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x < 4 else '4-5-star')\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "    'Orginal_Title', 'Orginal Title'\n",
    "]\n",
    "\n",
    "# Ensure all required columns are present\n",
    "for col in required_columns:\n",
    "    if col not in Final_review.columns:\n",
    "        Final_review[col] = None  # or use an appropriate default value\n",
    "\n",
    "# Select only the required columns\n",
    "Final_review = Final_review[required_columns]\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "Final_review.to_csv('bestbuy.csv', index=False)\n",
    "\n",
    "# Display the unique Scraping_Date values (optional)\n",
    "print(Final_review['Scraping_Date'].unique())\n",
    "\n",
    "# # Walmart\n",
    "\n",
    "\n",
    "#anam\n",
    "\n",
    "def get_soup_walmart(url):\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',  # Referer header might be required for some websites\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "    \n",
    "    api = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={url}\"\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page_number(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    # Load API key from Excel\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)  # Use global variable here\n",
    "    api_key = api['API'][0]\n",
    "    api_key = '671c7ee460f6e495bdec853c'\n",
    "    # Scrapingdog API with JavaScript rendering enabled\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    \n",
    "    # Ensure the response is successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')        \n",
    "        # Finding the page number elements with class or attribute related to pagination\n",
    "        page_number_elements = soup.find_all(\n",
    "            lambda tag: tag.name == 'a' and 'page-number' in tag.get('data-automation-id', '')\n",
    "        )\n",
    "        \n",
    "        # Extracting the page numbers\n",
    "        page_numbers = [int(element.text) for element in page_number_elements]\n",
    "\n",
    "        if page_numbers:\n",
    "            last_page_number = max(page_numbers)\n",
    "            return last_page_number\n",
    "        else:\n",
    "            print(\"No page numbers found. Assuming only one page.\")\n",
    "            return 1\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_review_walmart(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "    api_key = '671c7ee460f6e495bdec853c'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "            'api_key': api_key,\n",
    "            'url': url,\n",
    "            'dynamic': 'true',\n",
    "        })\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        li_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "        title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "        if title_all:\n",
    "            title = title_all.get('href')\n",
    "            pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "            model = re.findall(pattern, title)\n",
    "            li_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "            \n",
    "            for li_tag in li_elements:\n",
    "                product = {}\n",
    "                product['Model'] = title\n",
    "                # Extracting the review rating\n",
    "                review_rating_element = li_tag.select_one('.w_iUH7')\n",
    "                product['Review rating'] = review_rating_element.text if review_rating_element else None\n",
    "    \n",
    "                # Checking if it's a verified purchase\n",
    "                verified_purchase_element = li_tag.select_one('.pl2.green.b.f7.self-center')\n",
    "                product['Verified Purchase or not'] = verified_purchase_element.text if verified_purchase_element else None\n",
    "    \n",
    "                # Extracting the review date\n",
    "                review_date_element = li_tag.select_one('.f7.gray')\n",
    "                date = review_date_element.text if review_date_element else None\n",
    "                data = pd.to_datetime(date) - timedelta(days=1)\n",
    "                formatted_data = data.strftime('%m/%d/%Y')\n",
    "                product['Review date'] = formatted_data\n",
    "                \n",
    "                # Extracting the review title\n",
    "                review_title_element = li_tag.select_one('.w_kV33.w_Sl3f.w_mvVb.f5.b')\n",
    "                product['Review title'] = review_title_element.text if review_title_element else None\n",
    "    \n",
    "                # Extracting the review content\n",
    "                review_content_element = li_tag.select_one('span.tl-m.db-m')\n",
    "                product['Review Content'] = review_content_element.text.strip() if review_content_element else None\n",
    "    \n",
    "                # Extracting the reviewer's name\n",
    "                review_name_element = li_tag.select_one('.f7.b.mv0')\n",
    "                product['Review name'] = review_name_element.text if review_name_element else None\n",
    "    \n",
    "                # Extracting syndicated source,\n",
    "                syndication_element = li_tag.select_one('.flex.f7 span.gray')\n",
    "                if syndication_element and 'Review from' in syndication_element.text:\n",
    "                    product['Syndicated source'] = syndication_element.text.split('Review from ')[-1].strip()\n",
    "                else:\n",
    "                    product['Syndicated source'] = None  # Assign None if no syndicated source is found\n",
    "    \n",
    "                # Correctly specify the button's aria-label as it appears in your HTML snippet\n",
    "                helpful_element = soup.select_one('button[aria-label^=\"Upvote ndmomma review\"] span')\n",
    "                \n",
    "                # Extract the number of people who found the review helpful\n",
    "                people_find_helpful = int(helpful_element.text.strip('()')) if helpful_element else 0\n",
    "    \n",
    "                # Adding the URL of the review\n",
    "                product['URL'] = url\n",
    "    \n",
    "                # Append the extracted product information to the list of reviews\n",
    "                extracted_reviews.append(product)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "\n",
    "# %%\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5129928603'\n",
    "    # 'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "walmart_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    # initial value don't modify\n",
    "    retry_count = 0\n",
    "\n",
    "    # you can modify with your need\n",
    "    max_try = 5\n",
    "    retry_limit = max_try\n",
    "    print(link)\n",
    "    while retry_count < max_try:\n",
    "        try:\n",
    "            last_page_number = get_page_number(link)\n",
    "            if last_page_number is None:\n",
    "                retry_count += 1\n",
    "                if retry_count <= retry_limit:\n",
    "                    print(\"Failed to retrieve last page number. Retrying... Also Extract the data\")\n",
    "                    if retry_count == 1:\n",
    "                        for page_number in range(1, last_page_number + 1):\n",
    "                            retry_count = 0  # Reset retry count for each page\n",
    "                            while retry_count < max_try:\n",
    "                                try:\n",
    "                                    target_url = f'{link}?page={page_number}'\n",
    "                                    extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                                    if len(extracted_reviews) == 0:\n",
    "                                        print('No reviews found. Retrying in 5 seconds...')\n",
    "                                        retry_count += 1\n",
    "                                        time.sleep(5)\n",
    "                                    else:\n",
    "                                        walmart_reviews.extend(extracted_reviews)\n",
    "                                        print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                                        time.sleep(2)\n",
    "                                        break\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                                    retry_count += 1\n",
    "                                    time.sleep(3)\n",
    "                            else:\n",
    "                                print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Failed to retrieve last page number after multiple retries. Changing the link.\")\n",
    "                    # Change the link here\n",
    "                    link = new_link\n",
    "                    retry_count = 0  # Reset retry count\n",
    "                    continue\n",
    "            print('Total pages:', last_page_number)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "        retry_count += 1\n",
    "    else:\n",
    "        print(\"Max retries exceeded for this link. Moving to the next link.\")\n",
    "        continue  # Move to the next link if max retries exceeded\n",
    "\n",
    "    if last_page_number is None:\n",
    "        print(\"Skipping processing for this link due to inability to retrieve last page number.\")\n",
    "        continue  # Move to the next link if last_page_number is None\n",
    "\n",
    "    for page_number in range(1, last_page_number + 1):\n",
    "        retry_count = 0  # Reset retry count for each page\n",
    "        while retry_count < max_try:\n",
    "            try:\n",
    "                target_url = f'{link}?page={page_number}'\n",
    "                extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                if len(extracted_reviews) == 0:\n",
    "                    print('No reviews found. Retrying in 5 seconds...')\n",
    "                    retry_count += 1\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    walmart_reviews.extend(extracted_reviews)\n",
    "                    print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                retry_count += 1\n",
    "                time.sleep(3)\n",
    "        else:\n",
    "            print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "walmart = pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer'] = \"Walmart\"\n",
    "\n",
    "walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "\n",
    "walmart_hp_combine = pd.merge(walmart, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model']\n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis=1)\n",
    "\n",
    "walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['Review Model'].fillna(\"\")\n",
    "walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(\n",
    "    lambda x: 'No' if 'HP' in x else 'Yes'\n",
    ")\n",
    "\n",
    "walmart_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns in the original DataFrame\n",
    "walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "# Concatenate with an empty DataFrame\n",
    "Final_review = pd.concat([pd.DataFrame(), walmart_hp_combine], ignore_index=True)\n",
    "\n",
    "# Add default values for some columns\n",
    "Final_review['Country'] = 'US'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64', errors='ignore')\n",
    "Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in Final_review.columns:\n",
    "    Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    Final_review['People_Find_Helpful'] = 0\n",
    "\n",
    "Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "# Fill NaN values in string columns with empty string\n",
    "string_columns = Final_review.select_dtypes(include='object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "\n",
    "# Ensure all required columns are present\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "    'Orginal_Title', 'Orginal Title'\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in Final_review.columns:\n",
    "        Final_review[col] = None\n",
    "\n",
    "# Reorder columns to match the required_columns list\n",
    "Final_review = Final_review[required_columns]\n",
    "\n",
    "# change here\n",
    "previous = pd.read_csv(path +'Tassel_EMEA_Review_Raw.csv')\n",
    "# previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], format='mixed').dt.date\n",
    "# previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], format='mixed').dt.date\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], errors='coerce').dt.date\n",
    "previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], errors='coerce').dt.date\n",
    "previous['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_review(text):\n",
    "    text = str(text)\n",
    "\n",
    "    cleaned_text = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "Final_review['Review_Content'] = Final_review['Review_Content'].apply(clean_review)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    cleaned = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', cleaned_text)\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "\n",
    "# Print the total number of reviews\n",
    "print('Total walmart review:', len(Final_review))\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "Final_review.to_csv('walmart.csv', index=False)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('walmart.csv')\n",
    "\n",
    "# Drop rows where the 'Review_Content' column is blank\n",
    "df_cleaned = df.dropna(subset=['Review_Content'])\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "df_cleaned.to_csv('walmart.csv', index=False)\n",
    "\n",
    "# Read es.csv, uk.csv, and us.csv\n",
    "es_data = pd.read_csv('es.csv')\n",
    "uk_data = pd.read_csv('uk.csv')\n",
    "us_data = pd.read_csv('us.csv')\n",
    "\n",
    "# Add columns \"Orginal_Review\" and \"Orginal_Title\" to uk_data\n",
    "uk_data['Orginal_Review'] = \"\"\n",
    "uk_data['Orginal_Title'] = \"\"\n",
    "\n",
    "# Concatenate es_data, uk_data, and us_data\n",
    "combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "\n",
    "# Remove the column \"Orginal_Review\"\n",
    "combined_df.drop(columns=['Orginal_Review'], inplace=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('amazon.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Function to read CSV without header\n",
    "def read_csv_without_header(file_path, column_names):\n",
    "    df = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "    df.columns = column_names[:len(df.columns)]  # Assign column names based on the number of columns in the file\n",
    "    return df\n",
    "\n",
    "# List of files to merge\n",
    "file_paths = ['amazon.csv', 'bestbuy.csv', 'walmart.csv']\n",
    "\n",
    "# Read the CSV files without headers\n",
    "dfs = [read_csv_without_header(file_path, required_columns) for file_path in file_paths]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in final_df.columns:\n",
    "    final_df['People_Find_Helpful'] = final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    final_df['People_Find_Helpful'] = 0\n",
    "\n",
    "\n",
    "# Ensure all required columns are present and in the correct order\n",
    "for col in required_columns:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = None\n",
    "\n",
    "final_df = final_df[required_columns]\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('merged_reviews.csv', index=False)\n",
    "\n",
    "# Load the CSV files\n",
    "\n",
    "# change here\n",
    "df1 = pd.read_csv(path +'Tassel_EMEA_Review_Raw.csv')\n",
    "df2 = pd.read_csv('merged_reviews.csv')\n",
    "\n",
    "# Merge the files on the 'id' column\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "\n",
    "# Save the merged result to a new CSV file\n",
    "# change here\n",
    "merged_df.to_csv(path +'Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "\n",
    "# # # Remove the original CSV files\n",
    "# # for file_path in file_paths:\n",
    "# #     os.remove(file_path)\n",
    "\n",
    "\n",
    "# change here\n",
    "df = pd.read_csv(path +'Tassel_EMEA_Review_Raw.csv')\n",
    "\n",
    "# Function to get the first character of review content\n",
    "def first_character(content):\n",
    "    if pd.isna(content):  # Check if content is NaN\n",
    "        return content\n",
    "    return content[:10]\n",
    "\n",
    "# Apply the function to create a new column with the first character\n",
    "df['Review_content_first_char'] = df['Review_Content'].apply(first_character)\n",
    "\n",
    "# Identify duplicates based on 'Review_Name' and 'Review_content_first_char'\n",
    "duplicates = df.duplicated(subset=['Review_Name', 'Review_content_first_char'], keep='first')\n",
    "\n",
    "# Keep the first occurrence of duplicates and rows with blank 'Review_Content'\n",
    "df_no_duplicates = df[~(duplicates & ~df['Review_Content'].isnull())]\n",
    "\n",
    "# Drop the temporary column\n",
    "df_no_duplicates = df_no_duplicates.drop(columns=['Review_content_first_char'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "# change here\n",
    "df_no_duplicates.to_csv(path +'Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "\n",
    "print('Tassel_raw_data_scraping completed. Tassel_raw file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d167b-a822-4d22-b217-c6c8afe93dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
