{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890667b-09ee-430b-a610-a56d495a5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from openpyxl import load_workbook\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "#define system path start\n",
    "# change here\n",
    "path = r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\\\'\n",
    "excel_path = r'C:\\Users\\TaYu430\\anaconda3\\envs\\webscrap\\My Scripts\\\\'\n",
    "\n",
    "#define system path start\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the timestamp\n",
    "print(\"Current Timestamp:\", timestamp)\n",
    "\n",
    "print('Running Walmart Script')\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "excel_file_path = excel_path + \"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheet_name = \"data_new\"\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "df_amazon\n",
    "\n",
    "# %%\n",
    "excel_file_path = excel_path + \"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(excel_file_path, sheet_name=sheets, engine='openpyxl')\n",
    "review_template\n",
    "\n",
    "\n",
    "# Walmart\n",
    "def get_soup_walmart(url):\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',  # Referer header might be required for some websites\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "    \n",
    "    api = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={url}\"\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_page_number(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    # Load API key from Excel\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)  # Use global variable here\n",
    "    api_key = api['API'][0]\n",
    "    # Scrapingdog API with JavaScript rendering enabled\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    \n",
    "    # Ensure the response is successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')        \n",
    "        # Finding the page number elements with class or attribute related to pagination\n",
    "        page_number_elements = soup.find_all(\n",
    "            lambda tag: tag.name == 'a' and 'page-number' in tag.get('data-automation-id', '')\n",
    "        )\n",
    "        \n",
    "        # Extracting the page numbers\n",
    "        page_numbers = [int(element.text) for element in page_number_elements]\n",
    "\n",
    "        if page_numbers:\n",
    "            last_page_number = max(page_numbers)\n",
    "            return last_page_number\n",
    "        else:\n",
    "            print(\"No page numbers found. Assuming only one page.\")\n",
    "            return 1\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_review_walmart(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "            'api_key': api_key,\n",
    "            'url': url,\n",
    "            'dynamic': 'true',\n",
    "        })\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        li_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "        title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "        if title_all:\n",
    "            title = title_all.get('href')\n",
    "            pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "            model = re.findall(pattern, title)\n",
    "            li_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "            \n",
    "            for li_tag in li_elements:\n",
    "                product = {}\n",
    "                product['Model'] = title\n",
    "                # Extracting the review rating\n",
    "                review_rating_element = li_tag.select_one('.w_iUH7')\n",
    "                product['Review rating'] = review_rating_element.text if review_rating_element else None\n",
    "    \n",
    "                # Checking if it's a verified purchase\n",
    "                verified_purchase_element = li_tag.select_one('.pl2.green.b.f7.self-center')\n",
    "                product['Verified Purchase or not'] = verified_purchase_element.text if verified_purchase_element else None\n",
    "    \n",
    "                # Extracting the review date\n",
    "                review_date_element = li_tag.select_one('.f7.gray')\n",
    "                date = review_date_element.text if review_date_element else None\n",
    "                data = pd.to_datetime(date) - timedelta(days=1)\n",
    "                formatted_data = data.strftime('%m/%d/%Y')\n",
    "                product['Review date'] = formatted_data\n",
    "                \n",
    "                # Extracting the review title\n",
    "                review_title_element = li_tag.select_one('.w_kV33.w_Sl3f.w_mvVb.f5.b')\n",
    "                product['Review title'] = review_title_element.text if review_title_element else None\n",
    "    \n",
    "                # Extracting the review content\n",
    "                review_content_element = li_tag.select_one('span.tl-m.db-m')\n",
    "                product['Review Content'] = review_content_element.text.strip() if review_content_element else None\n",
    "    \n",
    "                # Extracting the reviewer's name\n",
    "                review_name_element = li_tag.select_one('.f7.b.mv0')\n",
    "                product['Review name'] = review_name_element.text if review_name_element else None\n",
    "    \n",
    "                # Extracting syndicated source,\n",
    "                syndication_element = li_tag.select_one('.flex.f7 span.gray')\n",
    "                if syndication_element and 'Review from' in syndication_element.text:\n",
    "                    product['Syndicated source'] = syndication_element.text.split('Review from ')[-1].strip()\n",
    "                else:\n",
    "                    product['Syndicated source'] = None  # Assign None if no syndicated source is found\n",
    "    \n",
    "                # Correctly specify the button's aria-label as it appears in your HTML snippet\n",
    "                helpful_element = soup.select_one('button[aria-label^=\"Upvote ndmomma review\"] span')\n",
    "                \n",
    "                # Extract the number of people who found the review helpful\n",
    "                people_find_helpful = int(helpful_element.text.strip('()')) if helpful_element else 0\n",
    "    \n",
    "                # Adding the URL of the review\n",
    "                product['URL'] = url\n",
    "    \n",
    "                # Append the extracted product information to the list of reviews\n",
    "                extracted_reviews.append(product)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "\n",
    "# %%\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5129928603'\n",
    "    # 'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# %%\n",
    "\n",
    "walmart_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    # initial value don't modify\n",
    "    retry_count = 0\n",
    "\n",
    "    # you can modify with your need\n",
    "    max_try = 5\n",
    "    retry_limit = max_try\n",
    "    print(link)\n",
    "    while retry_count < max_try:\n",
    "        try:\n",
    "            last_page_number = get_page_number(link)\n",
    "            if last_page_number is None:\n",
    "                retry_count += 1\n",
    "                if retry_count <= retry_limit:\n",
    "                    print(\"Failed to retrieve last page number. Retrying... Also Extract the data\")\n",
    "                    if retry_count == 1:\n",
    "                        for page_number in range(1, last_page_number + 1):\n",
    "                            retry_count = 0  # Reset retry count for each page\n",
    "                            while retry_count < max_try:\n",
    "                                try:\n",
    "                                    target_url = f'{link}?page={page_number}'\n",
    "                                    extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                                    if len(extracted_reviews) == 0:\n",
    "                                        print('No reviews found. Retrying in 5 seconds...')\n",
    "                                        retry_count += 1\n",
    "                                        time.sleep(5)\n",
    "                                    else:\n",
    "                                        walmart_reviews.extend(extracted_reviews)\n",
    "                                        print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                                        time.sleep(2)\n",
    "                                        break\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                                    retry_count += 1\n",
    "                                    time.sleep(3)\n",
    "                            else:\n",
    "                                print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Failed to retrieve last page number after multiple retries. Changing the link.\")\n",
    "                    # Change the link here\n",
    "                    link = new_link\n",
    "                    retry_count = 0  # Reset retry count\n",
    "                    continue\n",
    "            print('Total pages:', last_page_number)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "        retry_count += 1\n",
    "    else:\n",
    "        print(\"Max retries exceeded for this link. Moving to the next link.\")\n",
    "        continue  # Move to the next link if max retries exceeded\n",
    "\n",
    "    if last_page_number is None:\n",
    "        print(\"Skipping processing for this link due to inability to retrieve last page number.\")\n",
    "        continue  # Move to the next link if last_page_number is None\n",
    "\n",
    "    for page_number in range(1, last_page_number + 1):\n",
    "        retry_count = 0  # Reset retry count for each page\n",
    "        while retry_count < max_try:\n",
    "            try:\n",
    "                target_url = f'{link}?page={page_number}'\n",
    "                extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                if len(extracted_reviews) == 0:\n",
    "                    print('No reviews found. Retrying in 5 seconds...')\n",
    "                    retry_count += 1\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    walmart_reviews.extend(extracted_reviews)\n",
    "                    print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                retry_count += 1\n",
    "                time.sleep(3)\n",
    "        else:\n",
    "            print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "walmart = pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer'] = \"Walmart\"\n",
    "\n",
    "walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "\n",
    "walmart_hp_combine = pd.merge(walmart, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model']\n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis=1)\n",
    "\n",
    "walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['Review Model'].fillna(\"\")\n",
    "walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(\n",
    "    lambda x: 'No' if 'HP' in x else 'Yes'\n",
    ")\n",
    "\n",
    "walmart_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns in the original DataFrame\n",
    "walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "# Concatenate with an empty DataFrame\n",
    "Final_review = pd.concat([pd.DataFrame(), walmart_hp_combine], ignore_index=True)\n",
    "\n",
    "# Add default values for some columns\n",
    "Final_review['Country'] = 'US'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64', errors='ignore')\n",
    "Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in Final_review.columns:\n",
    "    Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    Final_review['People_Find_Helpful'] = 0\n",
    "\n",
    "Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "# Fill NaN values in string columns with empty string\n",
    "string_columns = Final_review.select_dtypes(include='object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "\n",
    "# Ensure all required columns are present\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "    'Orginal_Title', 'Orginal Title'\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in Final_review.columns:\n",
    "        Final_review[col] = None\n",
    "\n",
    "# Reorder columns to match the required_columns list\n",
    "Final_review = Final_review[required_columns]\n",
    "\n",
    "# change here\n",
    "previous = pd.read_csv(path +'Tassel_EMEA_Review_Raw.csv')\n",
    "# previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], format='mixed').dt.date\n",
    "# previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], format='mixed').dt.date\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], errors='coerce').dt.date\n",
    "previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], errors='coerce').dt.date\n",
    "previous['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "Final_review.to_csv('walmart.csv', index=False)\n",
    "print('Walmart Script running completed. Walmart.csv file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd01f0-e686-4b1e-a0ba-c2ea5cc06fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
