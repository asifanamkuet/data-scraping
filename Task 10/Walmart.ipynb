{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6c237d-cf1f-438f-a7d0-44a7345620e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Timestamp: 2024-10-30 20:53:59\n",
      "Running Tassel_raw_date_scraping.py\n",
      "https://www.walmart.com/reviews/product/5152138188\n",
      "Total pages: 35\n",
      "Error encountered: 'str' object has no attribute 'dt'. Retrying in 5 seconds...\n",
      "No reviews found. Retrying in 5 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 293\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo reviews found. Retrying in 5 seconds...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    292\u001b[0m     retry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 293\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     walmart_reviews\u001b[38;5;241m.\u001b[39mextend(extracted_reviews)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import openpyxl\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the timestamp\n",
    "print(\"Current Timestamp:\", timestamp)\n",
    "\n",
    "print('Running Tassel_raw_date_scraping.py')\n",
    "\n",
    "# %%\n",
    "excel_file_path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheet_name = \"data_new\"\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df_amazon = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "df_amazon['HP Model Number'] = df_amazon['HP Model Number'].astype(str)\n",
    "df_amazon['Comp Model number'] = df_amazon['Comp Model number'].fillna(0).round(0).astype(int).astype(str)\n",
    "df_amazon\n",
    "\n",
    "# %%\n",
    "path = r\"Star rating scrape URL and info - NPI.xlsx\"\n",
    "sheets = 'review_template'\n",
    "review_template = pd.read_excel(path, sheet_name = sheets, engine='openpyxl')\n",
    "review_template\n",
    "\n",
    "# # Walmart\n",
    "\n",
    "\n",
    "#anam\n",
    "\n",
    "def get_soup_walmart(url):\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',  # Referer header might be required for some websites\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "    \n",
    "    api = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={url}\"\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page_number(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.walmart.com/',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    # Load API key from Excel\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)  # Use global variable here\n",
    "    api_key = api['API'][0]\n",
    "    api_key = '671c7ee460f6e495bdec853c'\n",
    "    # Scrapingdog API with JavaScript rendering enabled\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'dynamic': 'true',\n",
    "    })\n",
    "    \n",
    "    # Ensure the response is successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')        \n",
    "        # Finding the page number elements with class or attribute related to pagination\n",
    "        page_number_elements = soup.find_all(\n",
    "            lambda tag: tag.name == 'a' and 'page-number' in tag.get('data-automation-id', '')\n",
    "        )\n",
    "        \n",
    "        # Extracting the page numbers\n",
    "        page_numbers = [int(element.text) for element in page_number_elements]\n",
    "\n",
    "        if page_numbers:\n",
    "            last_page_number = max(page_numbers)\n",
    "            return last_page_number\n",
    "        else:\n",
    "            print(\"No page numbers found. Assuming only one page.\")\n",
    "            return 1\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_review_walmart(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    sheets = \"api\"\n",
    "    api = pd.read_excel(excel_file_path, sheet_name=sheets)\n",
    "    api_key = api['API'][0]\n",
    "    api_key = '671c7ee460f6e495bdec853c'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "            'api_key': api_key,\n",
    "            'url': url,\n",
    "            'dynamic': 'true',\n",
    "        })\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "        if title_all:\n",
    "            title = title_all.get('href')\n",
    "            pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "            model = re.findall(pattern, title)\n",
    "        # Finding all review blocks (adjust class based on actual Walmart page structure)\n",
    "        review_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "        \n",
    "        if not review_elements:\n",
    "            return None  # No reviews found, return None to trigger retry\n",
    "    \n",
    "        for review in review_elements:\n",
    "            product = {}\n",
    "    \n",
    "            # Extract review details\n",
    "            product['Model'] = title\n",
    "            review_rating_element = review.select_one('.w_iUH7')\n",
    "            product['Review rating'] = review_rating_element.text if review_rating_element else None\n",
    "    \n",
    "            verified_purchase_element = review.select_one('.pl2.green.b.f7.self-center')\n",
    "            product['Verified Purchase or not'] = verified_purchase_element.text if verified_purchase_element else None\n",
    "    \n",
    "            review_date_element = review.select_one('.f7.gray')\n",
    "            product['Review date'] = review_date_element.text if review_date_element else None\n",
    "            print(product['Review date'].dt.date)\n",
    "            review_title_element = review.select_one('.w_kV33.w_Sl3f.w_mvVb.f5.b')\n",
    "            product['Review title'] = review_title_element.text if review_title_element else None\n",
    "    \n",
    "            review_content_element = review.select_one('span.tl-m.db-m')\n",
    "            product['Review Content'] = review_content_element.text.strip() if review_content_element else None\n",
    "    \n",
    "            review_name_element = review.select_one('.f7.b.mv0')\n",
    "            product['Review name'] = review_name_element.text if review_name_element else None\n",
    "    \n",
    "            # Adding the URL of the review\n",
    "            product['URL'] = url\n",
    "    \n",
    "            # Append the extracted product information to the list of reviews\n",
    "            extracted_reviews.append(product)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "\n",
    "# %%\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5152138188'\n",
    "    # 'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "walmart_reviews = []\n",
    "\n",
    "for link in urls:\n",
    "    # initial value don't modify\n",
    "    retry_count = 0\n",
    "\n",
    "    # you can modify with your need\n",
    "    max_try = 5\n",
    "    retry_limit = max_try\n",
    "    print(link)\n",
    "    while retry_count < max_try:\n",
    "        try:\n",
    "            last_page_number = get_page_number(link)\n",
    "            if last_page_number is None:\n",
    "                retry_count += 1\n",
    "                if retry_count <= retry_limit:\n",
    "                    print(\"Failed to retrieve last page number. Retrying... Also Extract the data\")\n",
    "                    if retry_count == 1:\n",
    "                        for page_number in range(1, last_page_number + 1):\n",
    "                            retry_count = 0  # Reset retry count for each page\n",
    "                            while retry_count < max_try:\n",
    "                                try:\n",
    "                                    target_url = f'{link}?page={page_number}'\n",
    "                                    extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                                    if len(extracted_reviews) == 0:\n",
    "                                        print('No reviews found. Retrying in 5 seconds...')\n",
    "                                        retry_count += 1\n",
    "                                        time.sleep(5)\n",
    "                                    else:\n",
    "                                        walmart_reviews.extend(extracted_reviews)\n",
    "                                        print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                                        time.sleep(2)\n",
    "                                        break\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                                    retry_count += 1\n",
    "                                    time.sleep(3)\n",
    "                            else:\n",
    "                                print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Failed to retrieve last page number after multiple retries. Changing the link.\")\n",
    "                    # Change the link here\n",
    "                    link = new_link\n",
    "                    retry_count = 0  # Reset retry count\n",
    "                    continue\n",
    "            print('Total pages:', last_page_number)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "        retry_count += 1\n",
    "    else:\n",
    "        print(\"Max retries exceeded for this link. Moving to the next link.\")\n",
    "        continue  # Move to the next link if max retries exceeded\n",
    "\n",
    "    if last_page_number is None:\n",
    "        print(\"Skipping processing for this link due to inability to retrieve last page number.\")\n",
    "        continue  # Move to the next link if last_page_number is None\n",
    "\n",
    "    for page_number in range(1, last_page_number + 1):\n",
    "        retry_count = 0  # Reset retry count for each page\n",
    "        while retry_count < max_try:\n",
    "            try:\n",
    "                target_url = f'{link}?page={page_number}'\n",
    "                extracted_reviews = get_review_walmart(target_url)\n",
    "\n",
    "                if len(extracted_reviews) == 0:\n",
    "                    print('No reviews found. Retrying in 5 seconds...')\n",
    "                    retry_count += 1\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    walmart_reviews.extend(extracted_reviews)\n",
    "                    print(f'Review count in page {page_number}:', len(extracted_reviews))\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}. Retrying in 3 seconds...\")\n",
    "                retry_count += 1\n",
    "                time.sleep(3)\n",
    "        else:\n",
    "            print(f\"Max retries exceeded for page {page_number}. Skipping to the next page.\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "walmart = pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer'] = \"Walmart\"\n",
    "\n",
    "walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "\n",
    "walmart_hp_combine = pd.merge(walmart, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model']\n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis=1)\n",
    "\n",
    "walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['Review Model'].fillna(\"\")\n",
    "walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(\n",
    "    lambda x: 'No' if 'HP' in x else 'Yes'\n",
    ")\n",
    "\n",
    "walmart_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns in the original DataFrame\n",
    "walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "# Concatenate with an empty DataFrame\n",
    "Final_review = pd.concat([pd.DataFrame(), walmart_hp_combine], ignore_index=True)\n",
    "\n",
    "# Add default values for some columns\n",
    "Final_review['Country'] = 'US'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64', errors='ignore')\n",
    "Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in Final_review.columns:\n",
    "    Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    Final_review['People_Find_Helpful'] = 0\n",
    "\n",
    "Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "# Fill NaN values in string columns with empty string\n",
    "string_columns = Final_review.select_dtypes(include='object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "\n",
    "# Ensure all required columns are present\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "    'Orginal_Title', 'Orginal Title'\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in Final_review.columns:\n",
    "        Final_review[col] = None\n",
    "\n",
    "# Reorder columns to match the required_columns list\n",
    "Final_review = Final_review[required_columns]\n",
    "\n",
    "previous = pd.read_csv(r'Tassel_EMEA_Review_Raw.csv')\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], format='mixed').dt.date\n",
    "previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], format='mixed').dt.date\n",
    "previous['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_review(text):\n",
    "    text = str(text)\n",
    "\n",
    "    cleaned_text = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "Final_review['Review_Content'] = Final_review['Review_Content'].apply(clean_review)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    cleaned = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', cleaned_text)\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "\n",
    "# Print the total number of reviews\n",
    "print('Total walmart review:', len(Final_review))\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "Final_review.to_csv('walmart.csv', index=False)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('walmart.csv')\n",
    "\n",
    "# Drop rows where the 'Review_Content' column is blank\n",
    "df_cleaned = df.dropna(subset=['Review_Content'])\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "df_cleaned.to_csv('walmart.csv', index=False)\n",
    "\n",
    "# Read es.csv, uk.csv, and us.csv\n",
    "es_data = pd.read_csv('es.csv')\n",
    "uk_data = pd.read_csv('uk.csv')\n",
    "us_data = pd.read_csv('us.csv')\n",
    "\n",
    "# Add columns \"Orginal_Review\" and \"Orginal_Title\" to uk_data\n",
    "uk_data['Orginal_Review'] = \"\"\n",
    "uk_data['Orginal_Title'] = \"\"\n",
    "\n",
    "# Concatenate es_data, uk_data, and us_data\n",
    "combined_df = pd.concat([es_data, uk_data, us_data], ignore_index=True)\n",
    "\n",
    "# Remove the column \"Orginal_Review\"\n",
    "combined_df.drop(columns=['Orginal_Review'], inplace=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('amazon.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Function to read CSV without header\n",
    "def read_csv_without_header(file_path, column_names):\n",
    "    df = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "    df.columns = column_names[:len(df.columns)]  # Assign column names based on the number of columns in the file\n",
    "    return df\n",
    "\n",
    "# List of files to merge\n",
    "file_paths = ['amazon.csv', 'bestbuy.csv', 'walmart.csv']\n",
    "\n",
    "# Read the CSV files without headers\n",
    "dfs = [read_csv_without_header(file_path, required_columns) for file_path in file_paths]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in final_df.columns:\n",
    "    final_df['People_Find_Helpful'] = final_df['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    final_df['People_Find_Helpful'] = 0\n",
    "\n",
    "\n",
    "# Ensure all required columns are present and in the correct order\n",
    "for col in required_columns:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = None\n",
    "\n",
    "final_df = final_df[required_columns]\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('merged_reviews.csv', index=False)\n",
    "\n",
    "# Load the two CSV files\n",
    "merged_reviews = pd.read_csv('merged_reviews.csv')\n",
    "tassel_emea_review_raw = pd.read_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv')\n",
    "\n",
    "# Combine the data, keeping the header of Tassel_EMEA_Review_Raw.csv\n",
    "combined_data = pd.concat([tassel_emea_review_raw, merged_reviews], ignore_index=True)\n",
    "\n",
    "# Save the combined data to Tassel_EMEA_Review_Raw.csv\n",
    "combined_data.to_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "\n",
    "# # # Remove the original CSV files\n",
    "# # for file_path in file_paths:\n",
    "# #     os.remove(file_path)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv')\n",
    "\n",
    "# Function to get the first character of review content\n",
    "def first_character(content):\n",
    "    if pd.isna(content):  # Check if content is NaN\n",
    "        return content\n",
    "    return content[:10]\n",
    "\n",
    "# Apply the function to create a new column with the first characterheader\n",
    "df['Review_content_first_char'] = df['Review_Content'].apply(first_character)\n",
    "\n",
    "# Identify duplicates based on 'Review_Name' and 'Review_content_first_char'\n",
    "duplicates = df.duplicated(subset=['Review_Name', 'Review_content_first_char'], keep='first')\n",
    "\n",
    "# Keep the first occurrence of duplicates and rows with blank 'Review_Content'\n",
    "df_no_duplicates = df[~(duplicates & ~df['Review_Content'].isnull())]\n",
    "\n",
    "# Drop the temporary column\n",
    "df_no_duplicates = df_no_duplicates.drop(columns=['Review_content_first_char'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_no_duplicates.to_csv(r'C:\\Users\\TaYu430\\OneDrive - HP Inc\\General - Core Team Laser & Ink\\For Lip Kiat and Choon Chong\\Web review\\14_Text_mining\\Tassel\\Tassel_EMEA_Review_Raw.csv', index=False)\n",
    "\n",
    "print('Tassel_raw_data_scraping completed. Tassel_raw file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaabe9e3-8e31-4b8c-bdcc-dc69c0648633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review rating</th>\n",
       "      <th>Verified Purchase or not</th>\n",
       "      <th>Review date</th>\n",
       "      <th>Review title</th>\n",
       "      <th>Review Content</th>\n",
       "      <th>Review name</th>\n",
       "      <th>URL</th>\n",
       "      <th>Retailer</th>\n",
       "      <th>scraping_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>Worth it.</td>\n",
       "      <td>I really like this printer.  What I like even ...</td>\n",
       "      <td>Mac in MT</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-24</td>\n",
       "      <td>Deskjet 4252e printer</td>\n",
       "      <td>Deskject 4242e printer, prints, copies, and sc...</td>\n",
       "      <td>martiron</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-29</td>\n",
       "      <td>None</td>\n",
       "      <td>i ordered this after my previous printer died....</td>\n",
       "      <td>scanney</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>Printer review</td>\n",
       "      <td>Great product and have no problems. I've read ...</td>\n",
       "      <td>Casey</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>Good for a simple office - limited in features.</td>\n",
       "      <td>I didn't realize that automatic 2 sided printi...</td>\n",
       "      <td>MsLink</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>None</td>\n",
       "      <td>When trying to print; turns everything into pd...</td>\n",
       "      <td>roro</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-07-30</td>\n",
       "      <td>ink highly disappointing !</td>\n",
       "      <td>4 prints of color is all you'll get than it pr...</td>\n",
       "      <td>Vakeicha</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>Bad Deal</td>\n",
       "      <td>This printer is a  ink using monster.</td>\n",
       "      <td>Barbara</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>Setup</td>\n",
       "      <td>The setup process was buggy and frustrated. I ...</td>\n",
       "      <td>Patrick</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>None</td>\n",
       "      <td>Doesn't work</td>\n",
       "      <td>Esthere</td>\n",
       "      <td>https://www.walmart.com/reviews/product/512992...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>2024-10-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Review rating Verified Purchase or not Review date  \\\n",
       "0                5                     None  2024-08-08   \n",
       "1                5                     None  2024-09-24   \n",
       "2                5                     None  2024-09-29   \n",
       "3                4                     None  2024-09-02   \n",
       "4                3                     None  2024-08-08   \n",
       "..             ...                      ...         ...   \n",
       "391              3                     None  2024-09-17   \n",
       "392              1                     None  2024-07-30   \n",
       "393              3                     None  2024-10-07   \n",
       "394              3                     None  2024-04-28   \n",
       "395              1                     None  2024-09-19   \n",
       "\n",
       "                                        Review title  \\\n",
       "0                                          Worth it.   \n",
       "1                              Deskjet 4252e printer   \n",
       "2                                               None   \n",
       "3                                     Printer review   \n",
       "4    Good for a simple office - limited in features.   \n",
       "..                                               ...   \n",
       "391                                             None   \n",
       "392                       ink highly disappointing !   \n",
       "393                                         Bad Deal   \n",
       "394                                            Setup   \n",
       "395                                             None   \n",
       "\n",
       "                                        Review Content Review name  \\\n",
       "0    I really like this printer.  What I like even ...   Mac in MT   \n",
       "1    Deskject 4242e printer, prints, copies, and sc...    martiron   \n",
       "2    i ordered this after my previous printer died....     scanney   \n",
       "3    Great product and have no problems. I've read ...       Casey   \n",
       "4    I didn't realize that automatic 2 sided printi...      MsLink   \n",
       "..                                                 ...         ...   \n",
       "391  When trying to print; turns everything into pd...        roro   \n",
       "392  4 prints of color is all you'll get than it pr...    Vakeicha   \n",
       "393              This printer is a  ink using monster.     Barbara   \n",
       "394  The setup process was buggy and frustrated. I ...     Patrick   \n",
       "395                                       Doesn't work     Esthere   \n",
       "\n",
       "                                                   URL Retailer scraping_date  \n",
       "0    https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "1    https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "2    https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "3    https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "4    https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "..                                                 ...      ...           ...  \n",
       "391  https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "392  https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "393  https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "394  https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "395  https://www.walmart.com/reviews/product/512992...  Walmart    2024-10-26  \n",
       "\n",
       "[396 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb139c1-4a79-4c27-b181-521bf0400da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total walmart review: 348\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "walmart = pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer'] = \"Walmart\"\n",
    "\n",
    "walmart['scraping_date'] = date.today().strftime('%Y/%m/%d')\n",
    "walmart['scraping_date'] = pd.to_datetime(walmart['scraping_date']).dt.date\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date']).dt.date\n",
    "walmart['Review rating'] = walmart['Review rating'].astype(str).str.replace(' out of 5 stars review', '').astype(int)\n",
    "walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'])\n",
    "\n",
    "walmart_hp_combine = pd.merge(walmart, df_amazon, on=\"HP Model Number\", how=\"left\")\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['HP Model']\n",
    "\n",
    "columns_to_drop = ['Model', 'HP Model Number', 'Comp Model number', 'HP Model']\n",
    "walmart_hp_combine = walmart_hp_combine.drop(columns_to_drop, axis=1)\n",
    "\n",
    "walmart_hp_combine = walmart_hp_combine.drop_duplicates()\n",
    "walmart_hp_combine['Review Model'] = walmart_hp_combine['Review Model'].fillna(\"\")\n",
    "walmart_hp_combine['Competitor_Flag'] = walmart_hp_combine['Review Model'].apply(\n",
    "    lambda x: 'No' if 'HP' in x else 'Yes'\n",
    ")\n",
    "\n",
    "walmart_hp_combine['Country'] = 'US'\n",
    "\n",
    "column_mapping = {\n",
    "    'Review date': 'Review_Date',\n",
    "    'review_text': 'Review_Content',\n",
    "    'Review rating': 'Review_Rating',\n",
    "    'url': 'URL',\n",
    "    'review_title': 'Review_Title',\n",
    "    'Verified Purchase or not': 'Verified_Purchase_Flag',\n",
    "    'reviewer_name': 'Review_Name',\n",
    "    'syndication': 'Syndicated_Source',\n",
    "    'stars': 'Review_Rating',\n",
    "    'Retailer': 'Retailer',\n",
    "    'scraping_date': 'Scraping_Date',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Review title': 'Review_Title',\n",
    "    'Review Content': 'Review_Content',\n",
    "    'Review date': 'Review_Date',\n",
    "    'URL': 'URL',\n",
    "    'Seeding or not': 'Seeding_Flag',\n",
    "    'Review name': 'Review_Name',\n",
    "    'People_find_helpful': 'People_Find_Helpful',\n",
    "    'Syndicated source': 'Syndicated_Source',\n",
    "    'Comp Model': 'Comp_Model',\n",
    "    'HP Class': 'HP_Class',\n",
    "    'Review Model': 'Review_Model',\n",
    "    'Competitor_Flag': 'Competitor_Flag'\n",
    "}\n",
    "\n",
    "# Rename columns in the original DataFrame\n",
    "walmart_hp_combine = walmart_hp_combine.rename(columns=column_mapping)\n",
    "\n",
    "# Concatenate with an empty DataFrame\n",
    "Final_review = pd.concat([pd.DataFrame(), walmart_hp_combine], ignore_index=True)\n",
    "\n",
    "# Add default values for some columns\n",
    "Final_review['Country'] = 'US'\n",
    "Final_review['Review_Date'] = pd.to_datetime(Final_review['Review_Date']).dt.date\n",
    "Final_review['Review_Rating'] = Final_review['Review_Rating'].astype('int64', errors='ignore')\n",
    "Final_review['Review_Rating_Label'] = Final_review['Review_Rating'].apply(lambda x: '1-2-3-star' if x <4 else '4-5-star') \n",
    "# Handle missing 'People_Find_Helpful' column\n",
    "if 'People_Find_Helpful' in Final_review.columns:\n",
    "    Final_review['People_Find_Helpful'] = Final_review['People_Find_Helpful'].fillna(0).astype('int64')\n",
    "else:\n",
    "    Final_review['People_Find_Helpful'] = 0\n",
    "\n",
    "Final_review['Scraping_Date'] = pd.to_datetime(Final_review['Scraping_Date']).dt.date\n",
    "\n",
    "# Fill NaN values in string columns with empty string\n",
    "string_columns = Final_review.select_dtypes(include='object').columns\n",
    "Final_review[string_columns] = Final_review[string_columns].fillna('')\n",
    "\n",
    "# Ensure all required columns are present\n",
    "required_columns = [\n",
    "    'Review_Model', 'Competitor_Flag', 'HP_Class', 'Segment', 'Retailer',\n",
    "    'Comp_Model', 'Review_Date', 'Review_Name', 'Review_Rating',\n",
    "    'Review_Rating_Label', 'Review_Title', 'Review_Content', 'Seeding_Flag',\n",
    "    'Verified_Purchase_Flag', 'Promotion_Flag', 'Aggregation_Flag',\n",
    "    'People_Find_Helpful', 'Syndicated_Source', 'Response_Date',\n",
    "    'Response_Text', 'Response_Name', 'URL', 'Scraping_Date', 'Country',\n",
    "    'Orginal_Title', 'Orginal Title'\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in Final_review.columns:\n",
    "        Final_review[col] = None\n",
    "\n",
    "# Reorder columns to match the required_columns list\n",
    "Final_review = Final_review[required_columns]\n",
    "\n",
    "previous = pd.read_csv(r'Tassel_EMEA_Review_Raw.csv')\n",
    "previous['Review_Date'] = pd.to_datetime(previous['Review_Date'], format='mixed').dt.date\n",
    "previous['Scraping_Date'] = pd.to_datetime(previous['Scraping_Date'], format='mixed').dt.date\n",
    "previous['Review_Rating'] = previous['Review_Rating'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_review(text):\n",
    "    text = str(text)\n",
    "\n",
    "    cleaned_text = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "Final_review['Review_Content'] = Final_review['Review_Content'].apply(clean_review)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-English characters and punctuations\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove extra whitespaces and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    cleaned = re.sub(r'Media(?: content)? could not be loaded\\.?', ' ', cleaned_text)\n",
    "    english_words = re.findall(r'\\b[a-z]+\\b', cleaned)\n",
    "    first_ten_words = ''.join(english_words[:10])\n",
    "    return first_ten_words\n",
    "\n",
    "\n",
    "# Print the total number of reviews\n",
    "print('Total walmart review:', len(Final_review))\n",
    "\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "Final_review.to_csv('walmart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3c2c5d-06d2-49c1-9093-fdbd86e17c9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAug 29, 2024\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "data = pd.to_datetime('Aug 29, 2024').strftime('%m/%d/%Y')\n",
    "data-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43f35b-08b6-4846-8ff6-e995fa64c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
