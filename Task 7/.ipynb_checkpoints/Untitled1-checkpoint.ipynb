{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0d038-1e28-4920-959d-bb9b213d3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL and headers\n",
    "url = \"https://www.walmart.com/reviews/product/5129928603\"\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the script tag with id=\"__NEXT_DATA__\"\n",
    "next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "\n",
    "# Get the content of the script\n",
    "next_data_content = next_data_script.string if next_data_script else \"No script tag found with id='__NEXT_DATA__'.\"\n",
    "next_data_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c1e01-1d91-47e7-85bf-9b85a20a4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_data_until_end():\n",
    "    page_number = 1\n",
    "    all_data = []\n",
    "    \n",
    "    while True:\n",
    "        # Update the URL with the current page number\n",
    "        url = f\"https://www.walmart.com/reviews/product/5129928603?page={page_number}\"\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"}\n",
    "        \n",
    "        # Fetch the page content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the script tag with id=\"__NEXT_DATA__\"\n",
    "        next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "        \n",
    "        # If script tag is not found, stop the loop\n",
    "        if not next_data_script:\n",
    "            print(f\"No script tag found on page {page_number}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Get the content of the script and store it\n",
    "        next_data_content = next_data_script.string\n",
    "        all_data.append(next_data_content)\n",
    "        \n",
    "        print(f\"Fetched data from page {page_number}\")\n",
    "        \n",
    "        # Increment page number\n",
    "        page_number += 1\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Example: Fetch all the data\n",
    "data = fetch_data_until_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0050d58c-daa4-4a14-9f0c-f804dea63c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddc279-3607-4335-8fbd-5d792a5818e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "def fetch_data_with_bypass(page_number):\n",
    "    url = f\"{url}?page={page_number}\"\n",
    "    response = requests.get(\"https://api.scrapingdog.com/scrape\", params={\n",
    "    'api_key': '66ee9bb9db63104e98957be2',\n",
    "    'url': url,\n",
    "    })\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    if not next_data_script:\n",
    "        return None\n",
    "    return next_data_script.string\n",
    "\n",
    "# Main function to loop through pages and fetch data\n",
    "def fetch_all_pages():\n",
    "    session = requests.Session()\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        data = fetch_data_with_bypass(page)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Stopping at page {page}, no more data found.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched data from page {page}\")\n",
    "        page += 1\n",
    "\n",
    "# Call the main function to start fetching data\n",
    "fetch_all_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829492aa-f7a5-420d-93b2-6713af3cfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def fetch_data_with_bypass(page_number, url):\n",
    "    # Define the base API URL\n",
    "    api_url = f\"https://api.scrapingdog.com/scrape\"\n",
    "    params = {\n",
    "        'api_key': '66ee9bb9db63104e98957be2',\n",
    "        'url': f'{url}?page={page_number}'\n",
    "    }\n",
    "    \n",
    "    # Make the request to the API\n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    # Parse the response content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the JSON script that contains the reviews data\n",
    "    next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    if not next_data_script:\n",
    "        return None, None\n",
    "    \n",
    "    # Load the data from the JSON string\n",
    "    json_data = json.loads(next_data_script.string)\n",
    "    \n",
    "    # Navigate to the part of the JSON where the reviews and pagination data are stored\n",
    "    reviews_data = json_data.get('props', {}).get('pageProps', {}).get('initialData', {}).get('data', {}).get('reviews', {})\n",
    "    \n",
    "    # Check all necessary fields to ensure we stop when any key field is null or empty\n",
    "    pagination = reviews_data.get('pagination', {})\n",
    "    current_span = pagination.get('currentSpan', None)\n",
    "    next_page = pagination.get('next', None)\n",
    "    total_reviews = pagination.get('total', None)\n",
    "    pages = pagination.get('pages', [])\n",
    "    \n",
    "    # If any of these are null or indicate no more pages, stop\n",
    "    if not current_span or not next_page or not total_reviews or len(pages) == 0:\n",
    "        return reviews_data, None\n",
    "    \n",
    "    # Return the relevant data and the next page information\n",
    "    return reviews_data, next_page\n",
    "\n",
    "# Main function to loop through pages and fetch data\n",
    "def fetch_all_pages(url):\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        data, next_page = fetch_data_with_bypass(page, url)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Stopping at page {page}, no more data found.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched data from page {page}\")\n",
    "\n",
    "        # If no next page is found, stop\n",
    "        if next_page is None:\n",
    "            print(f\"No next page found, stopping at page {page}.\")\n",
    "            break\n",
    "\n",
    "        # Increment the page counter\n",
    "        page += 1\n",
    "\n",
    "# Call the main function to start fetching data\n",
    "fetch_all_pages('https://www.walmart.com/reviews/product/5129928603')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f514e24-f408-4ebb-b452-76b4763acdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "def is_valid_date(date_string):\n",
    "    \"\"\"Utility function to check if the date string is a valid date or string.\"\"\"\n",
    "    try:\n",
    "        # Try to parse the date using dateutil's parse function\n",
    "        parse(date_string)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def fetch_data_with_bypass(page_number, url):\n",
    "    \"\"\"Fetch data for a specific page number and return the review data and next page flag.\"\"\"\n",
    "    # Define the base API URL\n",
    "    api_url = f\"https://api.scrapingdog.com/scrape\"\n",
    "    params = {\n",
    "        'api_key': '66ee9bb9db63104e98957be2',\n",
    "        'url': f'{url}?page={page_number}'\n",
    "    }\n",
    "    \n",
    "    # Make the request to the API\n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    # Parse the response content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the JSON script that contains the reviews data\n",
    "    next_data_script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    if not next_data_script:\n",
    "        return None, None\n",
    "    \n",
    "    # Load the data from the JSON string\n",
    "    json_data = json.loads(next_data_script.string)\n",
    "    \n",
    "    # Navigate to the part of the JSON where the reviews and pagination data are stored\n",
    "    reviews_data = json_data.get('props', {}).get('pageProps', {}).get('initialData', {}).get('data', {}).get('reviews', {})\n",
    "    \n",
    "    # Check all necessary fields to ensure we stop when any key field is null or empty\n",
    "    pagination = reviews_data.get('pagination', {})\n",
    "    next_page = pagination.get('next', None)\n",
    "    \n",
    "    # Extract the customer reviews to validate their submission times\n",
    "    customer_reviews = reviews_data.get('customerReviews', [])\n",
    "    \n",
    "    # Check if all reviews have valid reviewSubmissionTime\n",
    "    for review in customer_reviews:\n",
    "        review_submission_time = review.get('reviewSubmissionTime', \"\")\n",
    "        if not review_submission_time or not is_valid_date(review_submission_time):\n",
    "            # If any review has invalid date, stop increasing pages\n",
    "            return reviews_data, None\n",
    "    \n",
    "    # Return the relevant data and the next page information\n",
    "    return reviews_data, next_page\n",
    "\n",
    "# Main function to loop through pages and fetch data\n",
    "def fetch_all_pages(url):\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        data, next_page = fetch_data_with_bypass(page, url)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Stopping at page {page}, no more data found or invalid reviewSubmissionTime.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched data from page {page}\")\n",
    "\n",
    "        # If no next page is found or reviewSubmissionTime is invalid, stop\n",
    "        if next_page is None:\n",
    "            print(f\"No next page found or invalid reviewSubmissionTime, stopping at page {page}.\")\n",
    "            break\n",
    "\n",
    "        # Increment the page counter\n",
    "        page += 1\n",
    "\n",
    "# Call the main function to start fetching data\n",
    "fetch_all_pages('https://www.walmart.com/reviews/product/514088591')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e9a2f-6cac-48a4-8646-41fa71c5eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Set up undetected Chrome WebDriver with stealth techniques.\"\"\"\n",
    "    # Configure undetected Chrome with options\n",
    "    options = uc.ChromeOptions()\n",
    "    \n",
    "    # Set a random user agent to mimic different browsers/devices\n",
    "    user_agent = UserAgent().random\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    # Disable other browser detection flags\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Maximize window for human-like behavior\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    \n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "    \n",
    "    # Additional stealth techniques\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def random_sleep():\n",
    "    \"\"\"Random sleep to mimic human interaction.\"\"\"\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "def is_valid_submission_time(submission_time):\n",
    "    \"\"\"Check if the submission time is a valid date or empty/invalid.\"\"\"\n",
    "    if submission_time == \"…\" or submission_time.strip() == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        parse(submission_time)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def fetch_next_data_script(page_number, driver, url):\n",
    "    \"\"\"Fetch and parse the __NEXT_DATA__ script content.\"\"\"\n",
    "    full_url = f\"{url}?page={page_number}\"\n",
    "    driver.get(full_url)\n",
    "    \n",
    "    # Random sleep to avoid detection\n",
    "    random_sleep()\n",
    "\n",
    "    try:\n",
    "        # Extract the __NEXT_DATA__ script content\n",
    "        next_data_script = driver.find_element(By.ID, \"__NEXT_DATA__\").get_attribute(\"innerHTML\")\n",
    "        \n",
    "        # Parse the JSON data\n",
    "        json_data = json.loads(next_data_script)\n",
    "        \n",
    "        # Extract reviews data\n",
    "        reviews = json_data.get('props', {}).get('pageProps', {}).get('initialData', {}).get('data', {}).get('reviews', {}).get('customerReviews', [])\n",
    "        \n",
    "        # Check if any review has an invalid submission time\n",
    "        for review in reviews:\n",
    "            review_submission_time = review.get('reviewSubmissionTime', \"\")\n",
    "            if not is_valid_submission_time(review_submission_time):\n",
    "                print(f\"Invalid reviewSubmissionTime found: {review_submission_time}\")\n",
    "                return False  # Stop pagination if invalid date is found\n",
    "        \n",
    "        return True  # Continue pagination if all dates are valid\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return False  # Stop pagination if there's any issue\n",
    "\n",
    "# Main function to loop through pages and fetch data\n",
    "def fetch_all_pages(url):\n",
    "    # Set up the undetected Chrome WebDriver with stealth techniques\n",
    "    driver = setup_selenium()\n",
    "    \n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        valid_page = fetch_next_data_script(page, driver, url)\n",
    "        \n",
    "        if not valid_page:\n",
    "            print(f\"Stopping at page {page}. Invalid data or issue encountered.\")\n",
    "            break\n",
    "\n",
    "        # Increment the page counter for next request\n",
    "        page += 1\n",
    "\n",
    "    # Close the browser once done\n",
    "    driver.quit()\n",
    "\n",
    "# Call the main function to start fetching data\n",
    "fetch_all_pages('https://www.walmart.com/reviews/product/514088591')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bb864-0dd8-4dcd-b641-3ee79c18f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Set up undetected Chrome WebDriver with additional techniques.\"\"\"\n",
    "    # Configure undetected Chrome with options\n",
    "    options = uc.ChromeOptions()\n",
    "    \n",
    "    # Use a stable, real desktop browser user agent (static)\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36\"\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    # Disable headless mode to ensure full browser rendering\n",
    "    # Headless mode off ensures rendering is as close to real as possible\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Enable incognito mode (optional)\n",
    "    options.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Maximize window or set window size (common desktop resolution)\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    \n",
    "    # Set proxy (optional, useful for region-specific page changes)\n",
    "    # options.add_argument('--proxy-server=http://your_proxy:port')\n",
    "\n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "    \n",
    "    # Remove WebDriver detection (automationControlled) property\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def open_url(driver, url):\n",
    "    \"\"\"Open a URL in the browser and wait for user input to close.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Add some wait time to let the page fully load\n",
    "    input(\"Press Enter to close the browser...\")\n",
    "    driver.quit()\n",
    "\n",
    "# Main function to open a URL\n",
    "def main(url):\n",
    "    driver = setup_selenium()\n",
    "    open_url(driver, url)\n",
    "\n",
    "# Call the main function with the desired URL\n",
    "main('https://www.walmart.com/reviews/product/514088591')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83074284-4c90-4cdd-914c-e602a68ee074",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install undetected-chromedriver fake-useragent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3de30-8812-49af-80a5-c3c7cf959b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "def setup_selenium():\n",
    "    \"\"\"Set up undetected Chrome WebDriver with options.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    \n",
    "    # Set a stable user agent\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0\"\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    # Maximize window size\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    \n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "    \n",
    "    # Execute script to disable WebDriver detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def random_sleep():\n",
    "    \"\"\"Introduce random sleep times to simulate human behavior.\"\"\"\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "def fetch_reviews(driver, url, last_review_id=None):\n",
    "    \"\"\"Fetch reviews data and handle pagination.\"\"\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    return True, current_review_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching reviews: {e}\")\n",
    "    \n",
    "    return False, last_review_id\n",
    "    \n",
    "fetch_all_reviews('https://www.walmart.com/reviews/product/514088591')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71b254-de14-4e68-a77a-33a5b9e0bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Step 1: Set up undetected ChromeDriver with stealth techniques\n",
    "def setup_selenium():\n",
    "    \"\"\"Set up undetected Chrome WebDriver with stealth techniques.\"\"\"\n",
    "    # Configure undetected Chrome with options\n",
    "    options = uc.ChromeOptions()\n",
    "    \n",
    "    # Set a specific user agent to mimic Firefox on Linux\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0\"\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    # Disable other browser detection flags\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        # Enable incognito mode (optional)\n",
    "    options.add_argument(\"--incognito\")\n",
    "    \n",
    "    # Maximize window or set window size (common desktop resolution)\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    \n",
    "    # Maximize window for human-like behavior\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    \n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "    \n",
    "    # Additional stealth techniques\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "# Random sleep time for human-like behavior\n",
    "def random_sleep():\n",
    "    \"\"\"Introduce random sleep times to simulate human behavior.\"\"\"\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Step 2: Extract Reviews from Page\n",
    "def extract_reviews(page_html, url):\n",
    "    \"\"\"Extract review data using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    li_elements = soup.find_all('div', class_=re.compile(r'overflow-visible b--none mt\\d-l ma0 dark-gray'))\n",
    "    extracted_reviews = []\n",
    "\n",
    "    title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "    if title_all:\n",
    "        title = title_all.get('href')\n",
    "        pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "        model = re.findall(pattern, title)\n",
    "\n",
    "        for li_tag in li_elements:\n",
    "            product = {}\n",
    "            product['Model'] = model[0] if model else None\n",
    "\n",
    "            # Extracting the review rating\n",
    "            review_rating_element = li_tag.select_one('.w_iUH7')\n",
    "            product['Review rating'] = review_rating_element.text if review_rating_element else None\n",
    "\n",
    "            # Checking if it's a verified purchase\n",
    "            verified_purchase_element = li_tag.select_one('.pl2.green.b.f7.self-center')\n",
    "            product['Verified Purchase or not'] = verified_purchase_element.text if verified_purchase_element else None\n",
    "\n",
    "            # Extracting the review date\n",
    "            review_date_element = li_tag.select_one('.f7.gray')\n",
    "            product['Review date'] = review_date_element.text if review_date_element else None\n",
    "\n",
    "            # Extracting the review title\n",
    "            review_title_element = li_tag.select_one('.w_kV33.w_Sl3f.w_mvVb.f5.b')\n",
    "            product['Review title'] = review_title_element.text if review_title_element else None\n",
    "\n",
    "            # Extracting the review content\n",
    "            review_content_element = li_tag.select_one('span.tl-m.db-m')\n",
    "            product['Review Content'] = review_content_element.text.strip() if review_content_element else None\n",
    "\n",
    "            # Extracting the reviewer's name\n",
    "            review_name_element = li_tag.select_one('.f7.b.mv0')\n",
    "            product['Review name'] = review_name_element.text if review_name_element else None\n",
    "\n",
    "            # Extracting syndicated source\n",
    "            syndication_element = li_tag.select_one('.flex.f7 span.gray')\n",
    "            if syndication_element and 'Review from' in syndication_element.text:\n",
    "                product['Syndicated source'] = syndication_element.text.split('Review from ')[-1].strip()\n",
    "            else:\n",
    "                product['Syndicated source'] = None  # Assign None if no syndicated source is found\n",
    "\n",
    "            # Extract the number of people who found the review helpful\n",
    "            helpful_element = soup.select_one('button[aria-label^=\"Upvote ndmomma review\"] span')\n",
    "            people_find_helpful = int(helpful_element.text.strip('()')) if helpful_element else 0\n",
    "            product['People find helpful'] = people_find_helpful\n",
    "\n",
    "            # Adding the URL of the review\n",
    "            product['URL'] = url\n",
    "\n",
    "            # Append the extracted product information to the list of reviews\n",
    "            extracted_reviews.append(product)\n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "# Step 3: Fetch Reviews with Selenium\n",
    "def fetch_reviews(driver, url):\n",
    "    \"\"\"Fetch reviews from a page and handle dynamic loading.\"\"\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    random_sleep()\n",
    "\n",
    "    # Get the page source after it has fully loaded\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    # Extract reviews using BeautifulSoup\n",
    "    reviews = extract_reviews(page_html, url)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Step 4: Fetch All Reviews\n",
    "def fetch_all_reviews(url):\n",
    "    \"\"\"Main function to scrape reviews from all pages.\"\"\"\n",
    "    driver = setup_selenium()\n",
    "    page = 1\n",
    "    all_reviews = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Fetching page {page}...\")\n",
    "\n",
    "        # Construct the URL for the current page\n",
    "        page_url = f\"{url}?page={page}\"\n",
    "\n",
    "        # Fetch reviews for the current page\n",
    "        reviews = fetch_reviews(driver, page_url)\n",
    "\n",
    "        # If no reviews were found, stop the loop\n",
    "        if not reviews:\n",
    "            print(f\"Stopping at page {page}. No more reviews.\")\n",
    "            break\n",
    "\n",
    "        # Add reviews to the total list\n",
    "        all_reviews.extend(reviews)\n",
    "        print(f\"Reviews extracted from page {page}: {len(reviews)}\")\n",
    "\n",
    "        # Increment the page counter\n",
    "        page += 1\n",
    "\n",
    "        # Random sleep to avoid detection\n",
    "        random_sleep()\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    driver.quit()\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Step 5: Define Walmart URLs\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# Step 6: Scrape Reviews from All URLs\n",
    "walmart_reviews = []\n",
    "\n",
    "for url in urls:\n",
    "    walmart_reviews.extend(fetch_all_reviews(url))\n",
    "\n",
    "# Step 7: Convert Reviews to DataFrame and Save to CSV\n",
    "df_walmart = pd.DataFrame(walmart_reviews)\n",
    "\n",
    "# Post-processing and cleaning the DataFrame\n",
    "df_walmart['Retailer'] = \"Walmart\"\n",
    "df_walmart['scraping_date'] = pd.to_datetime('today').date()\n",
    "df_walmart['Review date'] = pd.to_datetime(df_walmart['Review date']).dt.date\n",
    "df_walmart['Review rating'] = df_walmart['Review rating'].str.replace(' out of 5 stars review', '').astype(float)\n",
    "df_walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_walmart.to_csv('walmart_reviews.csv', index=False)\n",
    "\n",
    "print(\"Reviews scraped and saved to 'walmart_reviews.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed42826c-d9a4-4b2a-943a-ab889736ca06",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2611097462.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 44\u001b[0;36m\u001b[0m\n\u001b[0;31m    title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Step 1: Set up undetected ChromeDriver with stealth techniques\n",
    "def setup_selenium(user_agent):\n",
    "    \"\"\"Set up undetected Chrome WebDriver with stealth techniques.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    # Set the user agent\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "    # Disable browser detection flags\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Enable incognito mode (optional)\n",
    "    options.add_argument(\"--incognito\")\n",
    "\n",
    "    # Maximize window or set window size (common desktop resolution)\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "\n",
    "    # Maximize window for human-like behavior\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "\n",
    "    # Additional stealth techniques\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "# Step 2: Extract Reviews from Page\n",
    "def extract_reviews(page_html, url):\n",
    "    \"\"\"Extract review data using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "        title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "        if title_all:\n",
    "            title = title_all.get('href')\n",
    "            pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "            model = re.findall(pattern, title)\n",
    "        #         model = re.search(r'\\b(\\d{4}e)\\b', title_all).group(1)\n",
    "        li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "\n",
    "        if li_elements:\n",
    "            for li_tag in li_elements:\n",
    "                product = {}\n",
    "                product['Model'] = title\n",
    "                product['Review rating'] = li_tag.select_one('.w_iUH7').text\n",
    "                product['Verified Purchase or not'] = li_tag.select_one(\n",
    "                    '.pl2.green.b.f7.self-center').text if li_tag.select_one('.pl2.green.b.f7.self-center') else None\n",
    "                product['Review date'] = li_tag.select_one('.f7.gray').text if li_tag.select_one('.f7.gray') else None\n",
    "\n",
    "                review_title_element = li_tag.select_one('h3.b')\n",
    "                product['Review title'] = review_title_element.text if review_title_element else None\n",
    "\n",
    "                product['Review Content'] = li_tag.find('span', class_='tl-m mb3 db-m').text if li_tag.find('span',\n",
    "                                                                                                            class_='tl-m mb3 db-m') else None\n",
    "                product['Review name'] = li_tag.select_one('.f6.gray').text if li_tag.select_one('.f6.gray') else None\n",
    "\n",
    "                syndication_element = li_tag.select_one('.b.ph1.dark.gray')\n",
    "                product['Syndicated source'] = syndication_element.text if syndication_element else None\n",
    "                product['URL'] = url\n",
    "\n",
    "                extracted_reviews.append(product)\n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "# Step 3: Fetch Reviews with Selenium\n",
    "def fetch_reviews(driver, url):\n",
    "    \"\"\"Fetch reviews from a page and handle dynamic loading.\"\"\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    random_sleep()\n",
    "\n",
    "    # Get the page source after it has fully loaded\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    # Extract reviews using BeautifulSoup\n",
    "    reviews = extract_reviews(page_html, url)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Random sleep time for human-like behavior\n",
    "def random_sleep():\n",
    "    \"\"\"Introduce random sleep times to simulate human behavior.\"\"\"\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Step 4: Navigate to specific star ratings\n",
    "def click_star_rating(driver, rating):\n",
    "    \"\"\"Click on a specific star rating filter (5 stars, 4 stars, etc.).\"\"\"\n",
    "    try:\n",
    "        # Locate the button for the specific star rating\n",
    "        star_button = driver.find_element_by_xpath(f'//button[contains(text(), \"{rating} stars\")]')\n",
    "        star_button.click()\n",
    "        random_sleep()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to click on {rating}-star rating: {e}\")\n",
    "\n",
    "# Step 5: Click the \"Next Page\" button\n",
    "def click_next_page(driver):\n",
    "    \"\"\"Click the 'Next Page' button if it exists.\"\"\"\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//button[contains(text(), \"Next\")]')\n",
    "        next_button.click()\n",
    "        random_sleep()\n",
    "    except Exception as e:\n",
    "        print(f\"Next page button not found: {e}\")\n",
    "\n",
    "# Step 6: Retry mechanism and user agent switching\n",
    "def fetch_all_reviews(url, max_retries=20):\n",
    "    \"\"\"Main function to scrape reviews from all pages with retry mechanism.\"\"\"\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        # Add more user agents as needed\n",
    "    ]\n",
    "\n",
    "    retry_count = 0\n",
    "    all_reviews = []\n",
    "    page = 1\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        print(f\"Attempt {retry_count + 1}/{max_retries} - Fetching page {page}...\")\n",
    "\n",
    "        # Select a random user agent\n",
    "        user_agent = random.choice(user_agents)\n",
    "\n",
    "        # Set up the browser with the chosen user agent\n",
    "        driver = setup_selenium(user_agent)\n",
    "\n",
    "        # Construct the URL for the current page\n",
    "        driver.get(url)\n",
    "        random_sleep()\n",
    "\n",
    "        # Loop through star ratings (5-star, 4-star, etc.)\n",
    "        for star_rating in range(5, 3, -1):\n",
    "            print(f\"Fetching {star_rating}-star reviews...\")\n",
    "            click_star_rating(driver, star_rating)\n",
    "\n",
    "            while True:\n",
    "                reviews = fetch_reviews(driver, url)\n",
    "\n",
    "                if reviews:\n",
    "                    # Add reviews to the total list if found\n",
    "                    all_reviews.extend(reviews)\n",
    "                    print(f\"Reviews extracted from page {page}: {len(reviews)}\")\n",
    "                else:\n",
    "                    break  # Break out if no reviews are found on this page\n",
    "\n",
    "                # Try to click on the next page\n",
    "                try:\n",
    "                    click_next_page(driver)\n",
    "                    page += 1\n",
    "                except Exception as e:\n",
    "                    print(\"No more pages available.\")\n",
    "                    break  # Stop pagination when no more pages are available\n",
    "\n",
    "        # Close the driver after each attempt\n",
    "        driver.quit()\n",
    "\n",
    "        # Stop if no more reviews are found after retries\n",
    "        if retry_count == max_retries:\n",
    "            print(\"Max retries reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Step 7: Define Walmart URLs\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# Step 8: Scrape Reviews from All URLs\n",
    "walmart_reviews = []\n",
    "\n",
    "for url in urls:\n",
    "    walmart_reviews.extend(fetch_all_reviews(url))\n",
    "\n",
    "# Step 9: Convert Reviews to DataFrame and Save to CSV\n",
    "df_walmart = pd.DataFrame(walmart_reviews)\n",
    "\n",
    "# Post-processing and cleaning the DataFrame\n",
    "df_walmart['Retailer'] = \"Walmart\"\n",
    "df_walmart['scraping_date'] = pd.to_datetime('today').date()\n",
    "df_walmart['Review date'] = pd.to_datetime(df_walmart['Review date']).dt.date\n",
    "df_walmart['Review rating'] = df_walmart['Review rating'].str.replace(' out of 5 stars review', '').astype(float)\n",
    "df_walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_walmart.to_csv('walmart_reviews.csv', index=False)\n",
    "\n",
    "print(\"Reviews scraped and saved to 'walmart_reviews.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78711da0-26dd-4615-b6fc-a8ae18589154",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3180292650.py, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 74\u001b[0;36m\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Step 1: Set up undetected ChromeDriver with stealth techniques\n",
    "def setup_selenium(user_agent):\n",
    "    \"\"\"Set up undetected Chrome WebDriver with stealth techniques.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    # Set the user agent\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "    # Disable browser detection flags\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Enable incognito mode (optional)\n",
    "    options.add_argument(\"--incognito\")\n",
    "\n",
    "    # Maximize window or set window size (common desktop resolution)\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "\n",
    "    # Maximize window for human-like behavior\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    # Start the Chrome WebDriver using undetected-chromedriver\n",
    "    driver = uc.Chrome(options=options)\n",
    "\n",
    "    # Additional stealth techniques\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "# Step 2: Extract Reviews from Page\n",
    "def extract_reviews(page_html, url):\n",
    "    \"\"\"Extract review data using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "    title_all = soup.find('a', class_='w_x7ug f6 dark-gray')\n",
    "    if title_all:\n",
    "        title = title_all.get('href')\n",
    "        pattern = r'(\\d{4}[a-zA-Z]?)-'\n",
    "        model = re.findall(pattern, title)\n",
    "    #         model = re.search(r'\\b(\\d{4}e)\\b', title_all).group(1)\n",
    "    li_elements = soup.find_all('li', class_='dib w-100 mb3')\n",
    "\n",
    "    if li_elements:\n",
    "        for li_tag in li_elements:\n",
    "            product = {}\n",
    "            product['Model'] = title\n",
    "            product['Review rating'] = li_tag.select_one('.w_iUH7').text\n",
    "            product['Verified Purchase or not'] = li_tag.select_one(\n",
    "                '.pl2.green.b.f7.self-center').text if li_tag.select_one('.pl2.green.b.f7.self-center') else None\n",
    "            product['Review date'] = li_tag.select_one('.f7.gray').text if li_tag.select_one('.f7.gray') else None\n",
    "\n",
    "            review_title_element = li_tag.select_one('h3.b')\n",
    "            product['Review title'] = review_title_element.text if review_title_element else None\n",
    "\n",
    "            product['Review Content'] = li_tag.find('span', class_='tl-m mb3 db-m').text if li_tag.find('span',\n",
    "                                                                                                        class_='tl-m mb3 db-m') else None\n",
    "            product['Review name'] = li_tag.select_one('.f6.gray').text if li_tag.select_one('.f6.gray') else None\n",
    "\n",
    "            syndication_element = li_tag.select_one('.b.ph1.dark.gray')\n",
    "            product['Syndicated source'] = syndication_element.text if syndication_element else None\n",
    "            product['URL'] = url\n",
    "\n",
    "            extracted_reviews.append(product) \n",
    "\n",
    "    return extracted_reviews\n",
    "\n",
    "# Step 3: Fetch Reviews with Selenium\n",
    "def fetch_reviews(driver, url):\n",
    "    \"\"\"Fetch reviews from a page and handle dynamic loading.\"\"\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    random_sleep()\n",
    "\n",
    "    # Get the page source after it has fully loaded\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    # Extract reviews using BeautifulSoup\n",
    "    reviews = extract_reviews(page_html, url)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Random sleep time for human-like behavior\n",
    "def random_sleep():\n",
    "    \"\"\"Introduce random sleep times to simulate human behavior.\"\"\"\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Step 4: Navigate to specific star ratings using aria-label (5 stars to 1 star)\n",
    "def click_star_rating(driver, stars):\n",
    "    \"\"\"Click on a specific star rating filter using aria-label (5 stars, 4 stars, etc.).\"\"\"\n",
    "    try:\n",
    "        # Find the button that matches the aria-label containing the star rating\n",
    "        pattern = f\"{stars} stars\"\n",
    "        star_button = driver.find_element_by_xpath(f'//button[contains(@aria-label, \"{pattern}\")]')\n",
    "        star_button.click()\n",
    "        random_sleep()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to click on {stars}-star rating: {e}\")\n",
    "\n",
    "# Step 5: Click the \"Next Page\" button using aria-label\n",
    "def click_next_page(driver, current_page):\n",
    "    \"\"\"Click the 'Next Page' button based on aria-label for page navigation.\"\"\"\n",
    "    try:\n",
    "        next_page_label = f\"Go to Page {current_page + 1}\"\n",
    "        next_button = driver.find_element_by_xpath(f'//a[@aria-label=\"{next_page_label}\"]')\n",
    "        next_button.click()\n",
    "        random_sleep()\n",
    "    except Exception as e:\n",
    "        print(f\"No next page button found for page {current_page}: {e}\")\n",
    "\n",
    "# Step 6: Retry mechanism and user agent switching\n",
    "def fetch_all_reviews(url, max_retries=20):\n",
    "    \"\"\"Main function to scrape reviews from all pages with retry mechanism.\"\"\"\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        # Add more user agents as needed\n",
    "    ]\n",
    "\n",
    "    retry_count = 0\n",
    "    all_reviews = []\n",
    "    page = 1\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        print(f\"Attempt {retry_count + 1}/{max_retries} - Fetching page {page}...\")\n",
    "\n",
    "        # Select a random user agent\n",
    "        user_agent = random.choice(user_agents)\n",
    "\n",
    "        # Set up the browser with the chosen user agent\n",
    "        driver = setup_selenium(user_agent)\n",
    "\n",
    "        # Construct the URL for the current page\n",
    "        driver.get(url)\n",
    "        random_sleep()\n",
    "\n",
    "        # Loop through star ratings (5-star to 1-star)\n",
    "        for star_rating in range(5, 0, -1):\n",
    "            print(f\"Fetching {star_rating}-star reviews...\")\n",
    "            click_star_rating(driver, star_rating)\n",
    "\n",
    "            while True:\n",
    "                reviews = fetch_reviews(driver, url)\n",
    "\n",
    "                if reviews:\n",
    "                    # Add reviews to the total list if found\n",
    "                    all_reviews.extend(reviews)\n",
    "                    print(f\"Reviews extracted from page {page}: {len(reviews)}\")\n",
    "                else:\n",
    "                    break  # Break out if no reviews are found on this page\n",
    "\n",
    "                # Try to click on the next page using aria-label\n",
    "                try:\n",
    "                    click_next_page(driver, page)\n",
    "                    page += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"No more pages available for {star_rating}-star reviews.\")\n",
    "                    break  # Stop pagination when no more pages are available\n",
    "\n",
    "        # Close the driver after each attempt\n",
    "        driver.quit()\n",
    "\n",
    "        # Stop if no more reviews are found after retries\n",
    "        if retry_count == max_retries:\n",
    "            print(\"Max retries reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Step 7: Define Walmart URLs\n",
    "urls = [\n",
    "    'https://www.walmart.com/reviews/product/5129928603'\n",
    "]\n",
    "\n",
    "# Step 8: Scrape Reviews from All URLs\n",
    "walmart_reviews = []\n",
    "\n",
    "for url in urls:\n",
    "    walmart_reviews.extend(fetch_all_reviews(url))\n",
    "\n",
    "# Step 9: Convert Reviews to DataFrame and Save to CSV\n",
    "df_walmart = pd.DataFrame(walmart_reviews)\n",
    "\n",
    "# Post-processing and cleaning the DataFrame\n",
    "df_walmart['Retailer'] = \"Walmart\"\n",
    "df_walmart['scraping_date'] = pd.to_datetime('today').date()\n",
    "df_walmart['Review date'] = pd.to_datetime(df_walmart['Review date']).dt.date\n",
    "df_walmart['Review rating'] = df_walmart['Review rating'].str.replace(' out of 5 stars review', '').astype(float)\n",
    "df_walmart.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_walmart.to_csv('walmart_reviews.csv', index=False)\n",
    "\n",
    "print(\"Reviews scraped and saved to 'walmart_reviews.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea866a8-ced6-40a5-a850-dddd2cd995e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
